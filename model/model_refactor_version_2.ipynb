{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/HDD/RyanFolder/projects/Weather-Predition-Project/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATv2Conv, GraphConv\n",
    "from torch_geometric.nn.norm import BatchNorm\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">**ZE EPIC DATA**</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([72790024141, 72785524114, 72789094197, 72793024233, 72785794129, 72788594266, 72797624217, 72785024157, 72797094240, 72798594276, 72792424223, 72792894263, 72781024243, 72781524237, 72788324220, 72698824219, 72793894274, 74206024207, 72782724110, 72793724222, 72792594227, 72782594239, 72794504205, 72792394225, 72784524163, 72792024227, 72785694176])\n"
     ]
    }
   ],
   "source": [
    "# Directory containing the CSV files\n",
    "directory = '../processed-final-data'\n",
    "\n",
    "# Dictionary to store the dataframes\n",
    "dataframes = {}\n",
    "\n",
    "# Iterate over all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        # Extract the file name without extension and convert it to int\n",
    "        key = int(os.path.splitext(filename)[0])\n",
    "        \n",
    "        # Read the CSV file into a dataframe\n",
    "        df = pd.read_csv(os.path.join(directory, filename))\n",
    "        dataframes[key] = df\n",
    "\n",
    "\n",
    "# Print the dictionary keys to verify\n",
    "print(dataframes.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72790024141: Sequential split verified.\n",
      "72785524114: Sequential split verified.\n",
      "72789094197: Sequential split verified.\n",
      "72793024233: Sequential split verified.\n",
      "72785794129: Sequential split verified.\n",
      "72788594266: Sequential split verified.\n",
      "72797624217: Sequential split verified.\n",
      "72785024157: Sequential split verified.\n",
      "72797094240: Sequential split verified.\n",
      "72798594276: Sequential split verified.\n",
      "72792424223: Sequential split verified.\n",
      "72792894263: Sequential split verified.\n",
      "72781024243: Sequential split verified.\n",
      "72781524237: Sequential split verified.\n",
      "72788324220: Sequential split verified.\n",
      "72698824219: Sequential split verified.\n",
      "72793894274: Sequential split verified.\n",
      "74206024207: Sequential split verified.\n",
      "72782724110: Sequential split verified.\n",
      "72793724222: Sequential split verified.\n",
      "72792594227: Sequential split verified.\n",
      "72782594239: Sequential split verified.\n",
      "72794504205: Sequential split verified.\n",
      "72792394225: Sequential split verified.\n",
      "72784524163: Sequential split verified.\n",
      "72792024227: Sequential split verified.\n",
      "72785694176: Sequential split verified.\n",
      "dict_keys([72790024141, 72785524114, 72789094197, 72793024233, 72785794129, 72788594266, 72797624217, 72785024157, 72797094240, 72798594276, 72792424223, 72792894263, 72781024243, 72781524237, 72788324220, 72698824219, 72793894274, 74206024207, 72782724110, 72793724222, 72792594227, 72782594239, 72794504205, 72792394225, 72784524163, 72792024227, 72785694176])\n",
      "dict_keys([72790024141, 72785524114, 72789094197, 72793024233, 72785794129, 72788594266, 72797624217, 72785024157, 72797094240, 72798594276, 72792424223, 72792894263, 72781024243, 72781524237, 72788324220, 72698824219, 72793894274, 74206024207, 72782724110, 72793724222, 72792594227, 72782594239, 72794504205, 72792394225, 72784524163, 72792024227, 72785694176])\n"
     ]
    }
   ],
   "source": [
    "# Dictionaries to store the training and testing dataframes\n",
    "train_dataframes = {}\n",
    "test_dataframes = {}\n",
    "\n",
    "# Split each dataframe into training and testing sets\n",
    "for key, df in dataframes.items():\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, shuffle=False)\n",
    "    train_dataframes[key] = train_df\n",
    "    test_dataframes[key] = test_df\n",
    "    # Check if the maximum index of the training set is less than the minimum index of the testing set\n",
    "    if train_df.index.max() < test_df.index.min():\n",
    "        print(f\"{key}: Sequential split verified.\")\n",
    "    else:\n",
    "        print(f\"{key}: Sequential split NOT verified.\")\n",
    "\n",
    "# Print the keys of the training and testing dictionaries to verify\n",
    "print(train_dataframes.keys())\n",
    "print(test_dataframes.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1408, 27, 69])\n",
      "torch.Size([352, 27, 67])\n",
      "tensor([[    0.0172,     0.9999,    34.5600,  ...,     0.0000,     0.0000,\n",
      "             0.0000],\n",
      "        [    0.0172,     0.9999,    37.9706,  ...,     0.0000,     0.0000,\n",
      "             0.0000],\n",
      "        [    0.0172,     0.9999,    31.8846,  ...,     0.0000,     0.0000,\n",
      "             0.0000],\n",
      "        ...,\n",
      "        [    0.0172,     0.9999,    37.8750,  ...,     0.0000,     0.0000,\n",
      "             0.0000],\n",
      "        [    0.0172,     0.9999,    44.3333,  ...,     0.0000,     0.0000,\n",
      "             0.0000],\n",
      "        [    0.0172,     0.9999,    34.9310,  ...,     0.0000,     0.0000,\n",
      "             0.0000]])\n",
      "tensor([[30.0000, 37.8485, 76.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [29.4688, 34.1875, 84.7500,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [26.5385, 38.6923, 63.0769,  ...,  0.0000,  0.0000,  1.0000],\n",
      "        ...,\n",
      "        [36.7083, 46.0833, 61.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [41.2553, 43.7660, 90.9362,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [28.3333, 36.7778, 92.0000,  ...,  0.0000,  0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "def create_node_features_sequences(dataframes):\n",
    "    # Create a list to store the node features for each time step for input and desired output\n",
    "    node_features_sequence_input = []\n",
    "    node_features_sequence_output = []\n",
    "\n",
    "    # Iterate over the rows of the dataframes (assuming all dataframes have the same number of rows)\n",
    "    for i in range(len(next(iter(dataframes.values())))):\n",
    "        if i == len(next(iter(dataframes.values()))) - 1:\n",
    "            break\n",
    "        # Create a list to store the features of all nodes at the current time step for input\n",
    "        node_features_input = []\n",
    "        # Create a list to store the features of all nodes at the next time step for output\n",
    "        node_features_output = []\n",
    "\n",
    "        # Iterate over each dataframe and extract the features at the current row for input\n",
    "        # and the next row for output\n",
    "        for key, df in dataframes.items():\n",
    "            node_features_input.append(df.iloc[i].values)\n",
    "            node_features_output.append(df.iloc[i + 1].drop(['DayOfYear_sin', 'DayOfYear_cos']).values)\n",
    "\n",
    "        # Stack the features of all nodes to create a 2D array (num_nodes, num_features)\n",
    "        node_features_sequence_input.append(np.stack(node_features_input))\n",
    "        node_features_sequence_output.append(np.stack(node_features_output))\n",
    "\n",
    "    # Convert the lists to numpy arrays (time_steps, num_nodes, num_features)\n",
    "    node_features_sequence_input = np.array(node_features_sequence_input)\n",
    "    node_features_sequence_output = np.array(node_features_sequence_output)\n",
    "\n",
    "    # Convert the numpy arrays to torch tensors\n",
    "    node_features_sequence_input = torch.tensor(node_features_sequence_input, dtype=torch.float)\n",
    "    node_features_sequence_output = torch.tensor(node_features_sequence_output, dtype=torch.float)\n",
    "\n",
    "    return node_features_sequence_input, node_features_sequence_output\n",
    "\n",
    "# Call the function and print the shapes of the resulting tensors\n",
    "node_features_sequence_input_train, node_features_sequence_output_train = create_node_features_sequences(train_dataframes)\n",
    "node_features_sequence_input_test, node_features_sequence_output_test = create_node_features_sequences(test_dataframes)\n",
    "np.set_printoptions(suppress=True, precision=4)\n",
    "torch.set_printoptions(sci_mode=False, precision=4)\n",
    "\n",
    "print(node_features_sequence_input_train.shape)\n",
    "print(node_features_sequence_output_test.shape)\n",
    "print(node_features_sequence_input_train[0])\n",
    "print(node_features_sequence_output_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">**ZE EPIC EDGE DATA**</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        STATION  LONGITUDE  LATITUDE  ELEVATION\n",
      "0  7.279002e+10 -119.51551  47.30777      382.1\n",
      "1  7.278552e+10 -117.65000  47.63333      750.1\n",
      "2  7.278909e+10 -119.52091  48.46113      397.4\n",
      "3  7.279302e+10 -122.31442  47.44467      112.5\n",
      "4  7.278579e+10 -117.11581  46.74376      775.7\n"
     ]
    }
   ],
   "source": [
    "# Import the location-datamap.csv file as a dataframe\n",
    "location_datamap_df = pd.read_csv('../location-datamap.csv')\n",
    "\n",
    "# Print the first few rows of the dataframe to verify\n",
    "print(location_datamap_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_distance(lat1, lon1, lat2, lon2, el1=0, el2=0):\n",
    "    # Radius of the Earth in kilometers\n",
    "    R = 6371.0\n",
    "\n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "\n",
    "    # Difference in coordinates\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "\n",
    "    # Haversine formula\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "\n",
    "    # Distance in kilometers\n",
    "    distance = R * c\n",
    "\n",
    "    # Elevation difference\n",
    "    height = el2 - el1\n",
    "\n",
    "    # Calculate the total distance considering elevation\n",
    "    total_distance = sqrt(distance**2 + height**2)\n",
    "\n",
    "    return total_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[72790024141, 72785524114, 0, 1],\n",
       " [72785524114, 72790024141, 1, 0],\n",
       " [72790024141, 72789094197, 0, 2],\n",
       " [72789094197, 72790024141, 2, 0],\n",
       " [72790024141, 72793024233, 0, 3],\n",
       " [72793024233, 72790024141, 3, 0],\n",
       " [72790024141, 72785794129, 0, 4],\n",
       " [72785794129, 72790024141, 4, 0],\n",
       " [72790024141, 72788594266, 0, 5],\n",
       " [72788594266, 72790024141, 5, 0],\n",
       " [72790024141, 72797624217, 0, 6],\n",
       " [72797624217, 72790024141, 6, 0],\n",
       " [72790024141, 72785024157, 0, 7],\n",
       " [72785024157, 72790024141, 7, 0],\n",
       " [72790024141, 72797094240, 0, 8],\n",
       " [72797094240, 72790024141, 8, 0],\n",
       " [72790024141, 72798594276, 0, 9],\n",
       " [72798594276, 72790024141, 9, 0],\n",
       " [72790024141, 72792424223, 0, 10],\n",
       " [72792424223, 72790024141, 10, 0],\n",
       " [72790024141, 72792894263, 0, 11],\n",
       " [72792894263, 72790024141, 11, 0],\n",
       " [72790024141, 72781024243, 0, 12],\n",
       " [72781024243, 72790024141, 12, 0],\n",
       " [72790024141, 72781524237, 0, 13],\n",
       " [72781524237, 72790024141, 13, 0],\n",
       " [72790024141, 72788324220, 0, 14],\n",
       " [72788324220, 72790024141, 14, 0],\n",
       " [72790024141, 72698824219, 0, 15],\n",
       " [72698824219, 72790024141, 15, 0],\n",
       " [72790024141, 72793894274, 0, 16],\n",
       " [72793894274, 72790024141, 16, 0],\n",
       " [72790024141, 74206024207, 0, 17],\n",
       " [74206024207, 72790024141, 17, 0],\n",
       " [72790024141, 72782724110, 0, 18],\n",
       " [72782724110, 72790024141, 18, 0],\n",
       " [72790024141, 72793724222, 0, 19],\n",
       " [72793724222, 72790024141, 19, 0],\n",
       " [72790024141, 72792594227, 0, 20],\n",
       " [72792594227, 72790024141, 20, 0],\n",
       " [72790024141, 72782594239, 0, 21],\n",
       " [72782594239, 72790024141, 21, 0],\n",
       " [72790024141, 72794504205, 0, 22],\n",
       " [72794504205, 72790024141, 22, 0],\n",
       " [72790024141, 72792394225, 0, 23],\n",
       " [72792394225, 72790024141, 23, 0],\n",
       " [72790024141, 72784524163, 0, 24],\n",
       " [72784524163, 72790024141, 24, 0],\n",
       " [72790024141, 72792024227, 0, 25],\n",
       " [72792024227, 72790024141, 25, 0],\n",
       " [72790024141, 72785694176, 0, 26],\n",
       " [72785694176, 72790024141, 26, 0],\n",
       " [72785524114, 72789094197, 1, 2],\n",
       " [72789094197, 72785524114, 2, 1],\n",
       " [72785524114, 72793024233, 1, 3],\n",
       " [72793024233, 72785524114, 3, 1],\n",
       " [72785524114, 72785794129, 1, 4],\n",
       " [72785794129, 72785524114, 4, 1],\n",
       " [72785524114, 72788594266, 1, 5],\n",
       " [72788594266, 72785524114, 5, 1],\n",
       " [72785524114, 72797624217, 1, 6],\n",
       " [72797624217, 72785524114, 6, 1],\n",
       " [72785524114, 72785024157, 1, 7],\n",
       " [72785024157, 72785524114, 7, 1],\n",
       " [72785524114, 72797094240, 1, 8],\n",
       " [72797094240, 72785524114, 8, 1],\n",
       " [72785524114, 72798594276, 1, 9],\n",
       " [72798594276, 72785524114, 9, 1],\n",
       " [72785524114, 72792424223, 1, 10],\n",
       " [72792424223, 72785524114, 10, 1],\n",
       " [72785524114, 72792894263, 1, 11],\n",
       " [72792894263, 72785524114, 11, 1],\n",
       " [72785524114, 72781024243, 1, 12],\n",
       " [72781024243, 72785524114, 12, 1],\n",
       " [72785524114, 72781524237, 1, 13],\n",
       " [72781524237, 72785524114, 13, 1],\n",
       " [72785524114, 72788324220, 1, 14],\n",
       " [72788324220, 72785524114, 14, 1],\n",
       " [72785524114, 72698824219, 1, 15],\n",
       " [72698824219, 72785524114, 15, 1],\n",
       " [72785524114, 72793894274, 1, 16],\n",
       " [72793894274, 72785524114, 16, 1],\n",
       " [72785524114, 74206024207, 1, 17],\n",
       " [74206024207, 72785524114, 17, 1],\n",
       " [72785524114, 72782724110, 1, 18],\n",
       " [72782724110, 72785524114, 18, 1],\n",
       " [72785524114, 72793724222, 1, 19],\n",
       " [72793724222, 72785524114, 19, 1],\n",
       " [72785524114, 72792594227, 1, 20],\n",
       " [72792594227, 72785524114, 20, 1],\n",
       " [72785524114, 72782594239, 1, 21],\n",
       " [72782594239, 72785524114, 21, 1],\n",
       " [72785524114, 72794504205, 1, 22],\n",
       " [72794504205, 72785524114, 22, 1],\n",
       " [72785524114, 72792394225, 1, 23],\n",
       " [72792394225, 72785524114, 23, 1],\n",
       " [72785524114, 72784524163, 1, 24],\n",
       " [72784524163, 72785524114, 24, 1],\n",
       " [72785524114, 72792024227, 1, 25],\n",
       " [72792024227, 72785524114, 25, 1],\n",
       " [72785524114, 72785694176, 1, 26],\n",
       " [72785694176, 72785524114, 26, 1],\n",
       " [72789094197, 72793024233, 2, 3],\n",
       " [72793024233, 72789094197, 3, 2],\n",
       " [72789094197, 72785794129, 2, 4],\n",
       " [72785794129, 72789094197, 4, 2],\n",
       " [72789094197, 72788594266, 2, 5],\n",
       " [72788594266, 72789094197, 5, 2],\n",
       " [72789094197, 72797624217, 2, 6],\n",
       " [72797624217, 72789094197, 6, 2],\n",
       " [72789094197, 72785024157, 2, 7],\n",
       " [72785024157, 72789094197, 7, 2],\n",
       " [72789094197, 72797094240, 2, 8],\n",
       " [72797094240, 72789094197, 8, 2],\n",
       " [72789094197, 72798594276, 2, 9],\n",
       " [72798594276, 72789094197, 9, 2],\n",
       " [72789094197, 72792424223, 2, 10],\n",
       " [72792424223, 72789094197, 10, 2],\n",
       " [72789094197, 72792894263, 2, 11],\n",
       " [72792894263, 72789094197, 11, 2],\n",
       " [72789094197, 72781024243, 2, 12],\n",
       " [72781024243, 72789094197, 12, 2],\n",
       " [72789094197, 72781524237, 2, 13],\n",
       " [72781524237, 72789094197, 13, 2],\n",
       " [72789094197, 72788324220, 2, 14],\n",
       " [72788324220, 72789094197, 14, 2],\n",
       " [72789094197, 72698824219, 2, 15],\n",
       " [72698824219, 72789094197, 15, 2],\n",
       " [72789094197, 72793894274, 2, 16],\n",
       " [72793894274, 72789094197, 16, 2],\n",
       " [72789094197, 74206024207, 2, 17],\n",
       " [74206024207, 72789094197, 17, 2],\n",
       " [72789094197, 72782724110, 2, 18],\n",
       " [72782724110, 72789094197, 18, 2],\n",
       " [72789094197, 72793724222, 2, 19],\n",
       " [72793724222, 72789094197, 19, 2],\n",
       " [72789094197, 72792594227, 2, 20],\n",
       " [72792594227, 72789094197, 20, 2],\n",
       " [72789094197, 72782594239, 2, 21],\n",
       " [72782594239, 72789094197, 21, 2],\n",
       " [72789094197, 72794504205, 2, 22],\n",
       " [72794504205, 72789094197, 22, 2],\n",
       " [72789094197, 72792394225, 2, 23],\n",
       " [72792394225, 72789094197, 23, 2],\n",
       " [72789094197, 72784524163, 2, 24],\n",
       " [72784524163, 72789094197, 24, 2],\n",
       " [72789094197, 72792024227, 2, 25],\n",
       " [72792024227, 72789094197, 25, 2],\n",
       " [72789094197, 72785694176, 2, 26],\n",
       " [72785694176, 72789094197, 26, 2],\n",
       " [72793024233, 72785794129, 3, 4],\n",
       " [72785794129, 72793024233, 4, 3],\n",
       " [72793024233, 72788594266, 3, 5],\n",
       " [72788594266, 72793024233, 5, 3],\n",
       " [72793024233, 72797624217, 3, 6],\n",
       " [72797624217, 72793024233, 6, 3],\n",
       " [72793024233, 72785024157, 3, 7],\n",
       " [72785024157, 72793024233, 7, 3],\n",
       " [72793024233, 72797094240, 3, 8],\n",
       " [72797094240, 72793024233, 8, 3],\n",
       " [72793024233, 72798594276, 3, 9],\n",
       " [72798594276, 72793024233, 9, 3],\n",
       " [72793024233, 72792424223, 3, 10],\n",
       " [72792424223, 72793024233, 10, 3],\n",
       " [72793024233, 72792894263, 3, 11],\n",
       " [72792894263, 72793024233, 11, 3],\n",
       " [72793024233, 72781024243, 3, 12],\n",
       " [72781024243, 72793024233, 12, 3],\n",
       " [72793024233, 72781524237, 3, 13],\n",
       " [72781524237, 72793024233, 13, 3],\n",
       " [72793024233, 72788324220, 3, 14],\n",
       " [72788324220, 72793024233, 14, 3],\n",
       " [72793024233, 72698824219, 3, 15],\n",
       " [72698824219, 72793024233, 15, 3],\n",
       " [72793024233, 72793894274, 3, 16],\n",
       " [72793894274, 72793024233, 16, 3],\n",
       " [72793024233, 74206024207, 3, 17],\n",
       " [74206024207, 72793024233, 17, 3],\n",
       " [72793024233, 72782724110, 3, 18],\n",
       " [72782724110, 72793024233, 18, 3],\n",
       " [72793024233, 72793724222, 3, 19],\n",
       " [72793724222, 72793024233, 19, 3],\n",
       " [72793024233, 72792594227, 3, 20],\n",
       " [72792594227, 72793024233, 20, 3],\n",
       " [72793024233, 72782594239, 3, 21],\n",
       " [72782594239, 72793024233, 21, 3],\n",
       " [72793024233, 72794504205, 3, 22],\n",
       " [72794504205, 72793024233, 22, 3],\n",
       " [72793024233, 72792394225, 3, 23],\n",
       " [72792394225, 72793024233, 23, 3],\n",
       " [72793024233, 72784524163, 3, 24],\n",
       " [72784524163, 72793024233, 24, 3],\n",
       " [72793024233, 72792024227, 3, 25],\n",
       " [72792024227, 72793024233, 25, 3],\n",
       " [72793024233, 72785694176, 3, 26],\n",
       " [72785694176, 72793024233, 26, 3],\n",
       " [72785794129, 72788594266, 4, 5],\n",
       " [72788594266, 72785794129, 5, 4],\n",
       " [72785794129, 72797624217, 4, 6],\n",
       " [72797624217, 72785794129, 6, 4],\n",
       " [72785794129, 72785024157, 4, 7],\n",
       " [72785024157, 72785794129, 7, 4],\n",
       " [72785794129, 72797094240, 4, 8],\n",
       " [72797094240, 72785794129, 8, 4],\n",
       " [72785794129, 72798594276, 4, 9],\n",
       " [72798594276, 72785794129, 9, 4],\n",
       " [72785794129, 72792424223, 4, 10],\n",
       " [72792424223, 72785794129, 10, 4],\n",
       " [72785794129, 72792894263, 4, 11],\n",
       " [72792894263, 72785794129, 11, 4],\n",
       " [72785794129, 72781024243, 4, 12],\n",
       " [72781024243, 72785794129, 12, 4],\n",
       " [72785794129, 72781524237, 4, 13],\n",
       " [72781524237, 72785794129, 13, 4],\n",
       " [72785794129, 72788324220, 4, 14],\n",
       " [72788324220, 72785794129, 14, 4],\n",
       " [72785794129, 72698824219, 4, 15],\n",
       " [72698824219, 72785794129, 15, 4],\n",
       " [72785794129, 72793894274, 4, 16],\n",
       " [72793894274, 72785794129, 16, 4],\n",
       " [72785794129, 74206024207, 4, 17],\n",
       " [74206024207, 72785794129, 17, 4],\n",
       " [72785794129, 72782724110, 4, 18],\n",
       " [72782724110, 72785794129, 18, 4],\n",
       " [72785794129, 72793724222, 4, 19],\n",
       " [72793724222, 72785794129, 19, 4],\n",
       " [72785794129, 72792594227, 4, 20],\n",
       " [72792594227, 72785794129, 20, 4],\n",
       " [72785794129, 72782594239, 4, 21],\n",
       " [72782594239, 72785794129, 21, 4],\n",
       " [72785794129, 72794504205, 4, 22],\n",
       " [72794504205, 72785794129, 22, 4],\n",
       " [72785794129, 72792394225, 4, 23],\n",
       " [72792394225, 72785794129, 23, 4],\n",
       " [72785794129, 72784524163, 4, 24],\n",
       " [72784524163, 72785794129, 24, 4],\n",
       " [72785794129, 72792024227, 4, 25],\n",
       " [72792024227, 72785794129, 25, 4],\n",
       " [72785794129, 72785694176, 4, 26],\n",
       " [72785694176, 72785794129, 26, 4],\n",
       " [72788594266, 72797624217, 5, 6],\n",
       " [72797624217, 72788594266, 6, 5],\n",
       " [72788594266, 72785024157, 5, 7],\n",
       " [72785024157, 72788594266, 7, 5],\n",
       " [72788594266, 72797094240, 5, 8],\n",
       " [72797094240, 72788594266, 8, 5],\n",
       " [72788594266, 72798594276, 5, 9],\n",
       " [72798594276, 72788594266, 9, 5],\n",
       " [72788594266, 72792424223, 5, 10],\n",
       " [72792424223, 72788594266, 10, 5],\n",
       " [72788594266, 72792894263, 5, 11],\n",
       " [72792894263, 72788594266, 11, 5],\n",
       " [72788594266, 72781024243, 5, 12],\n",
       " [72781024243, 72788594266, 12, 5],\n",
       " [72788594266, 72781524237, 5, 13],\n",
       " [72781524237, 72788594266, 13, 5],\n",
       " [72788594266, 72788324220, 5, 14],\n",
       " [72788324220, 72788594266, 14, 5],\n",
       " [72788594266, 72698824219, 5, 15],\n",
       " [72698824219, 72788594266, 15, 5],\n",
       " [72788594266, 72793894274, 5, 16],\n",
       " [72793894274, 72788594266, 16, 5],\n",
       " [72788594266, 74206024207, 5, 17],\n",
       " [74206024207, 72788594266, 17, 5],\n",
       " [72788594266, 72782724110, 5, 18],\n",
       " [72782724110, 72788594266, 18, 5],\n",
       " [72788594266, 72793724222, 5, 19],\n",
       " [72793724222, 72788594266, 19, 5],\n",
       " [72788594266, 72792594227, 5, 20],\n",
       " [72792594227, 72788594266, 20, 5],\n",
       " [72788594266, 72782594239, 5, 21],\n",
       " [72782594239, 72788594266, 21, 5],\n",
       " [72788594266, 72794504205, 5, 22],\n",
       " [72794504205, 72788594266, 22, 5],\n",
       " [72788594266, 72792394225, 5, 23],\n",
       " [72792394225, 72788594266, 23, 5],\n",
       " [72788594266, 72784524163, 5, 24],\n",
       " [72784524163, 72788594266, 24, 5],\n",
       " [72788594266, 72792024227, 5, 25],\n",
       " [72792024227, 72788594266, 25, 5],\n",
       " [72788594266, 72785694176, 5, 26],\n",
       " [72785694176, 72788594266, 26, 5],\n",
       " [72797624217, 72785024157, 6, 7],\n",
       " [72785024157, 72797624217, 7, 6],\n",
       " [72797624217, 72797094240, 6, 8],\n",
       " [72797094240, 72797624217, 8, 6],\n",
       " [72797624217, 72798594276, 6, 9],\n",
       " [72798594276, 72797624217, 9, 6],\n",
       " [72797624217, 72792424223, 6, 10],\n",
       " [72792424223, 72797624217, 10, 6],\n",
       " [72797624217, 72792894263, 6, 11],\n",
       " [72792894263, 72797624217, 11, 6],\n",
       " [72797624217, 72781024243, 6, 12],\n",
       " [72781024243, 72797624217, 12, 6],\n",
       " [72797624217, 72781524237, 6, 13],\n",
       " [72781524237, 72797624217, 13, 6],\n",
       " [72797624217, 72788324220, 6, 14],\n",
       " [72788324220, 72797624217, 14, 6],\n",
       " [72797624217, 72698824219, 6, 15],\n",
       " [72698824219, 72797624217, 15, 6],\n",
       " [72797624217, 72793894274, 6, 16],\n",
       " [72793894274, 72797624217, 16, 6],\n",
       " [72797624217, 74206024207, 6, 17],\n",
       " [74206024207, 72797624217, 17, 6],\n",
       " [72797624217, 72782724110, 6, 18],\n",
       " [72782724110, 72797624217, 18, 6],\n",
       " [72797624217, 72793724222, 6, 19],\n",
       " [72793724222, 72797624217, 19, 6],\n",
       " [72797624217, 72792594227, 6, 20],\n",
       " [72792594227, 72797624217, 20, 6],\n",
       " [72797624217, 72782594239, 6, 21],\n",
       " [72782594239, 72797624217, 21, 6],\n",
       " [72797624217, 72794504205, 6, 22],\n",
       " [72794504205, 72797624217, 22, 6],\n",
       " [72797624217, 72792394225, 6, 23],\n",
       " [72792394225, 72797624217, 23, 6],\n",
       " [72797624217, 72784524163, 6, 24],\n",
       " [72784524163, 72797624217, 24, 6],\n",
       " [72797624217, 72792024227, 6, 25],\n",
       " [72792024227, 72797624217, 25, 6],\n",
       " [72797624217, 72785694176, 6, 26],\n",
       " [72785694176, 72797624217, 26, 6],\n",
       " [72785024157, 72797094240, 7, 8],\n",
       " [72797094240, 72785024157, 8, 7],\n",
       " [72785024157, 72798594276, 7, 9],\n",
       " [72798594276, 72785024157, 9, 7],\n",
       " [72785024157, 72792424223, 7, 10],\n",
       " [72792424223, 72785024157, 10, 7],\n",
       " [72785024157, 72792894263, 7, 11],\n",
       " [72792894263, 72785024157, 11, 7],\n",
       " [72785024157, 72781024243, 7, 12],\n",
       " [72781024243, 72785024157, 12, 7],\n",
       " [72785024157, 72781524237, 7, 13],\n",
       " [72781524237, 72785024157, 13, 7],\n",
       " [72785024157, 72788324220, 7, 14],\n",
       " [72788324220, 72785024157, 14, 7],\n",
       " [72785024157, 72698824219, 7, 15],\n",
       " [72698824219, 72785024157, 15, 7],\n",
       " [72785024157, 72793894274, 7, 16],\n",
       " [72793894274, 72785024157, 16, 7],\n",
       " [72785024157, 74206024207, 7, 17],\n",
       " [74206024207, 72785024157, 17, 7],\n",
       " [72785024157, 72782724110, 7, 18],\n",
       " [72782724110, 72785024157, 18, 7],\n",
       " [72785024157, 72793724222, 7, 19],\n",
       " [72793724222, 72785024157, 19, 7],\n",
       " [72785024157, 72792594227, 7, 20],\n",
       " [72792594227, 72785024157, 20, 7],\n",
       " [72785024157, 72782594239, 7, 21],\n",
       " [72782594239, 72785024157, 21, 7],\n",
       " [72785024157, 72794504205, 7, 22],\n",
       " [72794504205, 72785024157, 22, 7],\n",
       " [72785024157, 72792394225, 7, 23],\n",
       " [72792394225, 72785024157, 23, 7],\n",
       " [72785024157, 72784524163, 7, 24],\n",
       " [72784524163, 72785024157, 24, 7],\n",
       " [72785024157, 72792024227, 7, 25],\n",
       " [72792024227, 72785024157, 25, 7],\n",
       " [72785024157, 72785694176, 7, 26],\n",
       " [72785694176, 72785024157, 26, 7],\n",
       " [72797094240, 72798594276, 8, 9],\n",
       " [72798594276, 72797094240, 9, 8],\n",
       " [72797094240, 72792424223, 8, 10],\n",
       " [72792424223, 72797094240, 10, 8],\n",
       " [72797094240, 72792894263, 8, 11],\n",
       " [72792894263, 72797094240, 11, 8],\n",
       " [72797094240, 72781024243, 8, 12],\n",
       " [72781024243, 72797094240, 12, 8],\n",
       " [72797094240, 72781524237, 8, 13],\n",
       " [72781524237, 72797094240, 13, 8],\n",
       " [72797094240, 72788324220, 8, 14],\n",
       " [72788324220, 72797094240, 14, 8],\n",
       " [72797094240, 72698824219, 8, 15],\n",
       " [72698824219, 72797094240, 15, 8],\n",
       " [72797094240, 72793894274, 8, 16],\n",
       " [72793894274, 72797094240, 16, 8],\n",
       " [72797094240, 74206024207, 8, 17],\n",
       " [74206024207, 72797094240, 17, 8],\n",
       " [72797094240, 72782724110, 8, 18],\n",
       " [72782724110, 72797094240, 18, 8],\n",
       " [72797094240, 72793724222, 8, 19],\n",
       " [72793724222, 72797094240, 19, 8],\n",
       " [72797094240, 72792594227, 8, 20],\n",
       " [72792594227, 72797094240, 20, 8],\n",
       " [72797094240, 72782594239, 8, 21],\n",
       " [72782594239, 72797094240, 21, 8],\n",
       " [72797094240, 72794504205, 8, 22],\n",
       " [72794504205, 72797094240, 22, 8],\n",
       " [72797094240, 72792394225, 8, 23],\n",
       " [72792394225, 72797094240, 23, 8],\n",
       " [72797094240, 72784524163, 8, 24],\n",
       " [72784524163, 72797094240, 24, 8],\n",
       " [72797094240, 72792024227, 8, 25],\n",
       " [72792024227, 72797094240, 25, 8],\n",
       " [72797094240, 72785694176, 8, 26],\n",
       " [72785694176, 72797094240, 26, 8],\n",
       " [72798594276, 72792424223, 9, 10],\n",
       " [72792424223, 72798594276, 10, 9],\n",
       " [72798594276, 72792894263, 9, 11],\n",
       " [72792894263, 72798594276, 11, 9],\n",
       " [72798594276, 72781024243, 9, 12],\n",
       " [72781024243, 72798594276, 12, 9],\n",
       " [72798594276, 72781524237, 9, 13],\n",
       " [72781524237, 72798594276, 13, 9],\n",
       " [72798594276, 72788324220, 9, 14],\n",
       " [72788324220, 72798594276, 14, 9],\n",
       " [72798594276, 72698824219, 9, 15],\n",
       " [72698824219, 72798594276, 15, 9],\n",
       " [72798594276, 72793894274, 9, 16],\n",
       " [72793894274, 72798594276, 16, 9],\n",
       " [72798594276, 74206024207, 9, 17],\n",
       " [74206024207, 72798594276, 17, 9],\n",
       " [72798594276, 72782724110, 9, 18],\n",
       " [72782724110, 72798594276, 18, 9],\n",
       " [72798594276, 72793724222, 9, 19],\n",
       " [72793724222, 72798594276, 19, 9],\n",
       " [72798594276, 72792594227, 9, 20],\n",
       " [72792594227, 72798594276, 20, 9],\n",
       " [72798594276, 72782594239, 9, 21],\n",
       " [72782594239, 72798594276, 21, 9],\n",
       " [72798594276, 72794504205, 9, 22],\n",
       " [72794504205, 72798594276, 22, 9],\n",
       " [72798594276, 72792394225, 9, 23],\n",
       " [72792394225, 72798594276, 23, 9],\n",
       " [72798594276, 72784524163, 9, 24],\n",
       " [72784524163, 72798594276, 24, 9],\n",
       " [72798594276, 72792024227, 9, 25],\n",
       " [72792024227, 72798594276, 25, 9],\n",
       " [72798594276, 72785694176, 9, 26],\n",
       " [72785694176, 72798594276, 26, 9],\n",
       " [72792424223, 72792894263, 10, 11],\n",
       " [72792894263, 72792424223, 11, 10],\n",
       " [72792424223, 72781024243, 10, 12],\n",
       " [72781024243, 72792424223, 12, 10],\n",
       " [72792424223, 72781524237, 10, 13],\n",
       " [72781524237, 72792424223, 13, 10],\n",
       " [72792424223, 72788324220, 10, 14],\n",
       " [72788324220, 72792424223, 14, 10],\n",
       " [72792424223, 72698824219, 10, 15],\n",
       " [72698824219, 72792424223, 15, 10],\n",
       " [72792424223, 72793894274, 10, 16],\n",
       " [72793894274, 72792424223, 16, 10],\n",
       " [72792424223, 74206024207, 10, 17],\n",
       " [74206024207, 72792424223, 17, 10],\n",
       " [72792424223, 72782724110, 10, 18],\n",
       " [72782724110, 72792424223, 18, 10],\n",
       " [72792424223, 72793724222, 10, 19],\n",
       " [72793724222, 72792424223, 19, 10],\n",
       " [72792424223, 72792594227, 10, 20],\n",
       " [72792594227, 72792424223, 20, 10],\n",
       " [72792424223, 72782594239, 10, 21],\n",
       " [72782594239, 72792424223, 21, 10],\n",
       " [72792424223, 72794504205, 10, 22],\n",
       " [72794504205, 72792424223, 22, 10],\n",
       " [72792424223, 72792394225, 10, 23],\n",
       " [72792394225, 72792424223, 23, 10],\n",
       " [72792424223, 72784524163, 10, 24],\n",
       " [72784524163, 72792424223, 24, 10],\n",
       " [72792424223, 72792024227, 10, 25],\n",
       " [72792024227, 72792424223, 25, 10],\n",
       " [72792424223, 72785694176, 10, 26],\n",
       " [72785694176, 72792424223, 26, 10],\n",
       " [72792894263, 72781024243, 11, 12],\n",
       " [72781024243, 72792894263, 12, 11],\n",
       " [72792894263, 72781524237, 11, 13],\n",
       " [72781524237, 72792894263, 13, 11],\n",
       " [72792894263, 72788324220, 11, 14],\n",
       " [72788324220, 72792894263, 14, 11],\n",
       " [72792894263, 72698824219, 11, 15],\n",
       " [72698824219, 72792894263, 15, 11],\n",
       " [72792894263, 72793894274, 11, 16],\n",
       " [72793894274, 72792894263, 16, 11],\n",
       " [72792894263, 74206024207, 11, 17],\n",
       " [74206024207, 72792894263, 17, 11],\n",
       " [72792894263, 72782724110, 11, 18],\n",
       " [72782724110, 72792894263, 18, 11],\n",
       " [72792894263, 72793724222, 11, 19],\n",
       " [72793724222, 72792894263, 19, 11],\n",
       " [72792894263, 72792594227, 11, 20],\n",
       " [72792594227, 72792894263, 20, 11],\n",
       " [72792894263, 72782594239, 11, 21],\n",
       " [72782594239, 72792894263, 21, 11],\n",
       " [72792894263, 72794504205, 11, 22],\n",
       " [72794504205, 72792894263, 22, 11],\n",
       " [72792894263, 72792394225, 11, 23],\n",
       " [72792394225, 72792894263, 23, 11],\n",
       " [72792894263, 72784524163, 11, 24],\n",
       " [72784524163, 72792894263, 24, 11],\n",
       " [72792894263, 72792024227, 11, 25],\n",
       " [72792024227, 72792894263, 25, 11],\n",
       " [72792894263, 72785694176, 11, 26],\n",
       " [72785694176, 72792894263, 26, 11],\n",
       " [72781024243, 72781524237, 12, 13],\n",
       " [72781524237, 72781024243, 13, 12],\n",
       " [72781024243, 72788324220, 12, 14],\n",
       " [72788324220, 72781024243, 14, 12],\n",
       " [72781024243, 72698824219, 12, 15],\n",
       " [72698824219, 72781024243, 15, 12],\n",
       " [72781024243, 72793894274, 12, 16],\n",
       " [72793894274, 72781024243, 16, 12],\n",
       " [72781024243, 74206024207, 12, 17],\n",
       " [74206024207, 72781024243, 17, 12],\n",
       " [72781024243, 72782724110, 12, 18],\n",
       " [72782724110, 72781024243, 18, 12],\n",
       " [72781024243, 72793724222, 12, 19],\n",
       " [72793724222, 72781024243, 19, 12],\n",
       " [72781024243, 72792594227, 12, 20],\n",
       " [72792594227, 72781024243, 20, 12],\n",
       " [72781024243, 72782594239, 12, 21],\n",
       " [72782594239, 72781024243, 21, 12],\n",
       " [72781024243, 72794504205, 12, 22],\n",
       " [72794504205, 72781024243, 22, 12],\n",
       " [72781024243, 72792394225, 12, 23],\n",
       " [72792394225, 72781024243, 23, 12],\n",
       " [72781024243, 72784524163, 12, 24],\n",
       " [72784524163, 72781024243, 24, 12],\n",
       " [72781024243, 72792024227, 12, 25],\n",
       " [72792024227, 72781024243, 25, 12],\n",
       " [72781024243, 72785694176, 12, 26],\n",
       " [72785694176, 72781024243, 26, 12],\n",
       " [72781524237, 72788324220, 13, 14],\n",
       " [72788324220, 72781524237, 14, 13],\n",
       " [72781524237, 72698824219, 13, 15],\n",
       " [72698824219, 72781524237, 15, 13],\n",
       " [72781524237, 72793894274, 13, 16],\n",
       " [72793894274, 72781524237, 16, 13],\n",
       " [72781524237, 74206024207, 13, 17],\n",
       " [74206024207, 72781524237, 17, 13],\n",
       " [72781524237, 72782724110, 13, 18],\n",
       " [72782724110, 72781524237, 18, 13],\n",
       " [72781524237, 72793724222, 13, 19],\n",
       " [72793724222, 72781524237, 19, 13],\n",
       " [72781524237, 72792594227, 13, 20],\n",
       " [72792594227, 72781524237, 20, 13],\n",
       " [72781524237, 72782594239, 13, 21],\n",
       " [72782594239, 72781524237, 21, 13],\n",
       " [72781524237, 72794504205, 13, 22],\n",
       " [72794504205, 72781524237, 22, 13],\n",
       " [72781524237, 72792394225, 13, 23],\n",
       " [72792394225, 72781524237, 23, 13],\n",
       " [72781524237, 72784524163, 13, 24],\n",
       " [72784524163, 72781524237, 24, 13],\n",
       " [72781524237, 72792024227, 13, 25],\n",
       " [72792024227, 72781524237, 25, 13],\n",
       " [72781524237, 72785694176, 13, 26],\n",
       " [72785694176, 72781524237, 26, 13],\n",
       " [72788324220, 72698824219, 14, 15],\n",
       " [72698824219, 72788324220, 15, 14],\n",
       " [72788324220, 72793894274, 14, 16],\n",
       " [72793894274, 72788324220, 16, 14],\n",
       " [72788324220, 74206024207, 14, 17],\n",
       " [74206024207, 72788324220, 17, 14],\n",
       " [72788324220, 72782724110, 14, 18],\n",
       " [72782724110, 72788324220, 18, 14],\n",
       " [72788324220, 72793724222, 14, 19],\n",
       " [72793724222, 72788324220, 19, 14],\n",
       " [72788324220, 72792594227, 14, 20],\n",
       " [72792594227, 72788324220, 20, 14],\n",
       " [72788324220, 72782594239, 14, 21],\n",
       " [72782594239, 72788324220, 21, 14],\n",
       " [72788324220, 72794504205, 14, 22],\n",
       " [72794504205, 72788324220, 22, 14],\n",
       " [72788324220, 72792394225, 14, 23],\n",
       " [72792394225, 72788324220, 23, 14],\n",
       " [72788324220, 72784524163, 14, 24],\n",
       " [72784524163, 72788324220, 24, 14],\n",
       " [72788324220, 72792024227, 14, 25],\n",
       " [72792024227, 72788324220, 25, 14],\n",
       " [72788324220, 72785694176, 14, 26],\n",
       " [72785694176, 72788324220, 26, 14],\n",
       " [72698824219, 72793894274, 15, 16],\n",
       " [72793894274, 72698824219, 16, 15],\n",
       " [72698824219, 74206024207, 15, 17],\n",
       " [74206024207, 72698824219, 17, 15],\n",
       " [72698824219, 72782724110, 15, 18],\n",
       " [72782724110, 72698824219, 18, 15],\n",
       " [72698824219, 72793724222, 15, 19],\n",
       " [72793724222, 72698824219, 19, 15],\n",
       " [72698824219, 72792594227, 15, 20],\n",
       " [72792594227, 72698824219, 20, 15],\n",
       " [72698824219, 72782594239, 15, 21],\n",
       " [72782594239, 72698824219, 21, 15],\n",
       " [72698824219, 72794504205, 15, 22],\n",
       " [72794504205, 72698824219, 22, 15],\n",
       " [72698824219, 72792394225, 15, 23],\n",
       " [72792394225, 72698824219, 23, 15],\n",
       " [72698824219, 72784524163, 15, 24],\n",
       " [72784524163, 72698824219, 24, 15],\n",
       " [72698824219, 72792024227, 15, 25],\n",
       " [72792024227, 72698824219, 25, 15],\n",
       " [72698824219, 72785694176, 15, 26],\n",
       " [72785694176, 72698824219, 26, 15],\n",
       " [72793894274, 74206024207, 16, 17],\n",
       " [74206024207, 72793894274, 17, 16],\n",
       " [72793894274, 72782724110, 16, 18],\n",
       " [72782724110, 72793894274, 18, 16],\n",
       " [72793894274, 72793724222, 16, 19],\n",
       " [72793724222, 72793894274, 19, 16],\n",
       " [72793894274, 72792594227, 16, 20],\n",
       " [72792594227, 72793894274, 20, 16],\n",
       " [72793894274, 72782594239, 16, 21],\n",
       " [72782594239, 72793894274, 21, 16],\n",
       " [72793894274, 72794504205, 16, 22],\n",
       " [72794504205, 72793894274, 22, 16],\n",
       " [72793894274, 72792394225, 16, 23],\n",
       " [72792394225, 72793894274, 23, 16],\n",
       " [72793894274, 72784524163, 16, 24],\n",
       " [72784524163, 72793894274, 24, 16],\n",
       " [72793894274, 72792024227, 16, 25],\n",
       " [72792024227, 72793894274, 25, 16],\n",
       " [72793894274, 72785694176, 16, 26],\n",
       " [72785694176, 72793894274, 26, 16],\n",
       " [74206024207, 72782724110, 17, 18],\n",
       " [72782724110, 74206024207, 18, 17],\n",
       " [74206024207, 72793724222, 17, 19],\n",
       " [72793724222, 74206024207, 19, 17],\n",
       " [74206024207, 72792594227, 17, 20],\n",
       " [72792594227, 74206024207, 20, 17],\n",
       " [74206024207, 72782594239, 17, 21],\n",
       " [72782594239, 74206024207, 21, 17],\n",
       " [74206024207, 72794504205, 17, 22],\n",
       " [72794504205, 74206024207, 22, 17],\n",
       " [74206024207, 72792394225, 17, 23],\n",
       " [72792394225, 74206024207, 23, 17],\n",
       " [74206024207, 72784524163, 17, 24],\n",
       " [72784524163, 74206024207, 24, 17],\n",
       " [74206024207, 72792024227, 17, 25],\n",
       " [72792024227, 74206024207, 25, 17],\n",
       " [74206024207, 72785694176, 17, 26],\n",
       " [72785694176, 74206024207, 26, 17],\n",
       " [72782724110, 72793724222, 18, 19],\n",
       " [72793724222, 72782724110, 19, 18],\n",
       " [72782724110, 72792594227, 18, 20],\n",
       " [72792594227, 72782724110, 20, 18],\n",
       " [72782724110, 72782594239, 18, 21],\n",
       " [72782594239, 72782724110, 21, 18],\n",
       " [72782724110, 72794504205, 18, 22],\n",
       " [72794504205, 72782724110, 22, 18],\n",
       " [72782724110, 72792394225, 18, 23],\n",
       " [72792394225, 72782724110, 23, 18],\n",
       " [72782724110, 72784524163, 18, 24],\n",
       " [72784524163, 72782724110, 24, 18],\n",
       " [72782724110, 72792024227, 18, 25],\n",
       " [72792024227, 72782724110, 25, 18],\n",
       " [72782724110, 72785694176, 18, 26],\n",
       " [72785694176, 72782724110, 26, 18],\n",
       " [72793724222, 72792594227, 19, 20],\n",
       " [72792594227, 72793724222, 20, 19],\n",
       " [72793724222, 72782594239, 19, 21],\n",
       " [72782594239, 72793724222, 21, 19],\n",
       " [72793724222, 72794504205, 19, 22],\n",
       " [72794504205, 72793724222, 22, 19],\n",
       " [72793724222, 72792394225, 19, 23],\n",
       " [72792394225, 72793724222, 23, 19],\n",
       " [72793724222, 72784524163, 19, 24],\n",
       " [72784524163, 72793724222, 24, 19],\n",
       " [72793724222, 72792024227, 19, 25],\n",
       " [72792024227, 72793724222, 25, 19],\n",
       " [72793724222, 72785694176, 19, 26],\n",
       " [72785694176, 72793724222, 26, 19],\n",
       " [72792594227, 72782594239, 20, 21],\n",
       " [72782594239, 72792594227, 21, 20],\n",
       " [72792594227, 72794504205, 20, 22],\n",
       " [72794504205, 72792594227, 22, 20],\n",
       " [72792594227, 72792394225, 20, 23],\n",
       " [72792394225, 72792594227, 23, 20],\n",
       " [72792594227, 72784524163, 20, 24],\n",
       " [72784524163, 72792594227, 24, 20],\n",
       " [72792594227, 72792024227, 20, 25],\n",
       " [72792024227, 72792594227, 25, 20],\n",
       " [72792594227, 72785694176, 20, 26],\n",
       " [72785694176, 72792594227, 26, 20],\n",
       " [72782594239, 72794504205, 21, 22],\n",
       " [72794504205, 72782594239, 22, 21],\n",
       " [72782594239, 72792394225, 21, 23],\n",
       " [72792394225, 72782594239, 23, 21],\n",
       " [72782594239, 72784524163, 21, 24],\n",
       " [72784524163, 72782594239, 24, 21],\n",
       " [72782594239, 72792024227, 21, 25],\n",
       " [72792024227, 72782594239, 25, 21],\n",
       " [72782594239, 72785694176, 21, 26],\n",
       " [72785694176, 72782594239, 26, 21],\n",
       " [72794504205, 72792394225, 22, 23],\n",
       " [72792394225, 72794504205, 23, 22],\n",
       " [72794504205, 72784524163, 22, 24],\n",
       " [72784524163, 72794504205, 24, 22],\n",
       " [72794504205, 72792024227, 22, 25],\n",
       " [72792024227, 72794504205, 25, 22],\n",
       " [72794504205, 72785694176, 22, 26],\n",
       " [72785694176, 72794504205, 26, 22],\n",
       " [72792394225, 72784524163, 23, 24],\n",
       " [72784524163, 72792394225, 24, 23],\n",
       " [72792394225, 72792024227, 23, 25],\n",
       " [72792024227, 72792394225, 25, 23],\n",
       " [72792394225, 72785694176, 23, 26],\n",
       " [72785694176, 72792394225, 26, 23],\n",
       " [72784524163, 72792024227, 24, 25],\n",
       " [72792024227, 72784524163, 25, 24],\n",
       " [72784524163, 72785694176, 24, 26],\n",
       " [72785694176, 72784524163, 26, 24],\n",
       " [72792024227, 72785694176, 25, 26],\n",
       " [72785694176, 72792024227, 26, 25]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[[0, 1],\n",
       " [1, 0],\n",
       " [0, 2],\n",
       " [2, 0],\n",
       " [0, 3],\n",
       " [3, 0],\n",
       " [0, 4],\n",
       " [4, 0],\n",
       " [0, 5],\n",
       " [5, 0],\n",
       " [0, 6],\n",
       " [6, 0],\n",
       " [0, 7],\n",
       " [7, 0],\n",
       " [0, 8],\n",
       " [8, 0],\n",
       " [0, 9],\n",
       " [9, 0],\n",
       " [0, 10],\n",
       " [10, 0],\n",
       " [0, 11],\n",
       " [11, 0],\n",
       " [0, 12],\n",
       " [12, 0],\n",
       " [0, 13],\n",
       " [13, 0],\n",
       " [0, 14],\n",
       " [14, 0],\n",
       " [0, 15],\n",
       " [15, 0],\n",
       " [0, 16],\n",
       " [16, 0],\n",
       " [0, 17],\n",
       " [17, 0],\n",
       " [0, 18],\n",
       " [18, 0],\n",
       " [0, 19],\n",
       " [19, 0],\n",
       " [0, 20],\n",
       " [20, 0],\n",
       " [0, 21],\n",
       " [21, 0],\n",
       " [0, 22],\n",
       " [22, 0],\n",
       " [0, 23],\n",
       " [23, 0],\n",
       " [0, 24],\n",
       " [24, 0],\n",
       " [0, 25],\n",
       " [25, 0],\n",
       " [0, 26],\n",
       " [26, 0],\n",
       " [1, 2],\n",
       " [2, 1],\n",
       " [1, 3],\n",
       " [3, 1],\n",
       " [1, 4],\n",
       " [4, 1],\n",
       " [1, 5],\n",
       " [5, 1],\n",
       " [1, 6],\n",
       " [6, 1],\n",
       " [1, 7],\n",
       " [7, 1],\n",
       " [1, 8],\n",
       " [8, 1],\n",
       " [1, 9],\n",
       " [9, 1],\n",
       " [1, 10],\n",
       " [10, 1],\n",
       " [1, 11],\n",
       " [11, 1],\n",
       " [1, 12],\n",
       " [12, 1],\n",
       " [1, 13],\n",
       " [13, 1],\n",
       " [1, 14],\n",
       " [14, 1],\n",
       " [1, 15],\n",
       " [15, 1],\n",
       " [1, 16],\n",
       " [16, 1],\n",
       " [1, 17],\n",
       " [17, 1],\n",
       " [1, 18],\n",
       " [18, 1],\n",
       " [1, 19],\n",
       " [19, 1],\n",
       " [1, 20],\n",
       " [20, 1],\n",
       " [1, 21],\n",
       " [21, 1],\n",
       " [1, 22],\n",
       " [22, 1],\n",
       " [1, 23],\n",
       " [23, 1],\n",
       " [1, 24],\n",
       " [24, 1],\n",
       " [1, 25],\n",
       " [25, 1],\n",
       " [1, 26],\n",
       " [26, 1],\n",
       " [2, 3],\n",
       " [3, 2],\n",
       " [2, 4],\n",
       " [4, 2],\n",
       " [2, 5],\n",
       " [5, 2],\n",
       " [2, 6],\n",
       " [6, 2],\n",
       " [2, 7],\n",
       " [7, 2],\n",
       " [2, 8],\n",
       " [8, 2],\n",
       " [2, 9],\n",
       " [9, 2],\n",
       " [2, 10],\n",
       " [10, 2],\n",
       " [2, 11],\n",
       " [11, 2],\n",
       " [2, 12],\n",
       " [12, 2],\n",
       " [2, 13],\n",
       " [13, 2],\n",
       " [2, 14],\n",
       " [14, 2],\n",
       " [2, 15],\n",
       " [15, 2],\n",
       " [2, 16],\n",
       " [16, 2],\n",
       " [2, 17],\n",
       " [17, 2],\n",
       " [2, 18],\n",
       " [18, 2],\n",
       " [2, 19],\n",
       " [19, 2],\n",
       " [2, 20],\n",
       " [20, 2],\n",
       " [2, 21],\n",
       " [21, 2],\n",
       " [2, 22],\n",
       " [22, 2],\n",
       " [2, 23],\n",
       " [23, 2],\n",
       " [2, 24],\n",
       " [24, 2],\n",
       " [2, 25],\n",
       " [25, 2],\n",
       " [2, 26],\n",
       " [26, 2],\n",
       " [3, 4],\n",
       " [4, 3],\n",
       " [3, 5],\n",
       " [5, 3],\n",
       " [3, 6],\n",
       " [6, 3],\n",
       " [3, 7],\n",
       " [7, 3],\n",
       " [3, 8],\n",
       " [8, 3],\n",
       " [3, 9],\n",
       " [9, 3],\n",
       " [3, 10],\n",
       " [10, 3],\n",
       " [3, 11],\n",
       " [11, 3],\n",
       " [3, 12],\n",
       " [12, 3],\n",
       " [3, 13],\n",
       " [13, 3],\n",
       " [3, 14],\n",
       " [14, 3],\n",
       " [3, 15],\n",
       " [15, 3],\n",
       " [3, 16],\n",
       " [16, 3],\n",
       " [3, 17],\n",
       " [17, 3],\n",
       " [3, 18],\n",
       " [18, 3],\n",
       " [3, 19],\n",
       " [19, 3],\n",
       " [3, 20],\n",
       " [20, 3],\n",
       " [3, 21],\n",
       " [21, 3],\n",
       " [3, 22],\n",
       " [22, 3],\n",
       " [3, 23],\n",
       " [23, 3],\n",
       " [3, 24],\n",
       " [24, 3],\n",
       " [3, 25],\n",
       " [25, 3],\n",
       " [3, 26],\n",
       " [26, 3],\n",
       " [4, 5],\n",
       " [5, 4],\n",
       " [4, 6],\n",
       " [6, 4],\n",
       " [4, 7],\n",
       " [7, 4],\n",
       " [4, 8],\n",
       " [8, 4],\n",
       " [4, 9],\n",
       " [9, 4],\n",
       " [4, 10],\n",
       " [10, 4],\n",
       " [4, 11],\n",
       " [11, 4],\n",
       " [4, 12],\n",
       " [12, 4],\n",
       " [4, 13],\n",
       " [13, 4],\n",
       " [4, 14],\n",
       " [14, 4],\n",
       " [4, 15],\n",
       " [15, 4],\n",
       " [4, 16],\n",
       " [16, 4],\n",
       " [4, 17],\n",
       " [17, 4],\n",
       " [4, 18],\n",
       " [18, 4],\n",
       " [4, 19],\n",
       " [19, 4],\n",
       " [4, 20],\n",
       " [20, 4],\n",
       " [4, 21],\n",
       " [21, 4],\n",
       " [4, 22],\n",
       " [22, 4],\n",
       " [4, 23],\n",
       " [23, 4],\n",
       " [4, 24],\n",
       " [24, 4],\n",
       " [4, 25],\n",
       " [25, 4],\n",
       " [4, 26],\n",
       " [26, 4],\n",
       " [5, 6],\n",
       " [6, 5],\n",
       " [5, 7],\n",
       " [7, 5],\n",
       " [5, 8],\n",
       " [8, 5],\n",
       " [5, 9],\n",
       " [9, 5],\n",
       " [5, 10],\n",
       " [10, 5],\n",
       " [5, 11],\n",
       " [11, 5],\n",
       " [5, 12],\n",
       " [12, 5],\n",
       " [5, 13],\n",
       " [13, 5],\n",
       " [5, 14],\n",
       " [14, 5],\n",
       " [5, 15],\n",
       " [15, 5],\n",
       " [5, 16],\n",
       " [16, 5],\n",
       " [5, 17],\n",
       " [17, 5],\n",
       " [5, 18],\n",
       " [18, 5],\n",
       " [5, 19],\n",
       " [19, 5],\n",
       " [5, 20],\n",
       " [20, 5],\n",
       " [5, 21],\n",
       " [21, 5],\n",
       " [5, 22],\n",
       " [22, 5],\n",
       " [5, 23],\n",
       " [23, 5],\n",
       " [5, 24],\n",
       " [24, 5],\n",
       " [5, 25],\n",
       " [25, 5],\n",
       " [5, 26],\n",
       " [26, 5],\n",
       " [6, 7],\n",
       " [7, 6],\n",
       " [6, 8],\n",
       " [8, 6],\n",
       " [6, 9],\n",
       " [9, 6],\n",
       " [6, 10],\n",
       " [10, 6],\n",
       " [6, 11],\n",
       " [11, 6],\n",
       " [6, 12],\n",
       " [12, 6],\n",
       " [6, 13],\n",
       " [13, 6],\n",
       " [6, 14],\n",
       " [14, 6],\n",
       " [6, 15],\n",
       " [15, 6],\n",
       " [6, 16],\n",
       " [16, 6],\n",
       " [6, 17],\n",
       " [17, 6],\n",
       " [6, 18],\n",
       " [18, 6],\n",
       " [6, 19],\n",
       " [19, 6],\n",
       " [6, 20],\n",
       " [20, 6],\n",
       " [6, 21],\n",
       " [21, 6],\n",
       " [6, 22],\n",
       " [22, 6],\n",
       " [6, 23],\n",
       " [23, 6],\n",
       " [6, 24],\n",
       " [24, 6],\n",
       " [6, 25],\n",
       " [25, 6],\n",
       " [6, 26],\n",
       " [26, 6],\n",
       " [7, 8],\n",
       " [8, 7],\n",
       " [7, 9],\n",
       " [9, 7],\n",
       " [7, 10],\n",
       " [10, 7],\n",
       " [7, 11],\n",
       " [11, 7],\n",
       " [7, 12],\n",
       " [12, 7],\n",
       " [7, 13],\n",
       " [13, 7],\n",
       " [7, 14],\n",
       " [14, 7],\n",
       " [7, 15],\n",
       " [15, 7],\n",
       " [7, 16],\n",
       " [16, 7],\n",
       " [7, 17],\n",
       " [17, 7],\n",
       " [7, 18],\n",
       " [18, 7],\n",
       " [7, 19],\n",
       " [19, 7],\n",
       " [7, 20],\n",
       " [20, 7],\n",
       " [7, 21],\n",
       " [21, 7],\n",
       " [7, 22],\n",
       " [22, 7],\n",
       " [7, 23],\n",
       " [23, 7],\n",
       " [7, 24],\n",
       " [24, 7],\n",
       " [7, 25],\n",
       " [25, 7],\n",
       " [7, 26],\n",
       " [26, 7],\n",
       " [8, 9],\n",
       " [9, 8],\n",
       " [8, 10],\n",
       " [10, 8],\n",
       " [8, 11],\n",
       " [11, 8],\n",
       " [8, 12],\n",
       " [12, 8],\n",
       " [8, 13],\n",
       " [13, 8],\n",
       " [8, 14],\n",
       " [14, 8],\n",
       " [8, 15],\n",
       " [15, 8],\n",
       " [8, 16],\n",
       " [16, 8],\n",
       " [8, 17],\n",
       " [17, 8],\n",
       " [8, 18],\n",
       " [18, 8],\n",
       " [8, 19],\n",
       " [19, 8],\n",
       " [8, 20],\n",
       " [20, 8],\n",
       " [8, 21],\n",
       " [21, 8],\n",
       " [8, 22],\n",
       " [22, 8],\n",
       " [8, 23],\n",
       " [23, 8],\n",
       " [8, 24],\n",
       " [24, 8],\n",
       " [8, 25],\n",
       " [25, 8],\n",
       " [8, 26],\n",
       " [26, 8],\n",
       " [9, 10],\n",
       " [10, 9],\n",
       " [9, 11],\n",
       " [11, 9],\n",
       " [9, 12],\n",
       " [12, 9],\n",
       " [9, 13],\n",
       " [13, 9],\n",
       " [9, 14],\n",
       " [14, 9],\n",
       " [9, 15],\n",
       " [15, 9],\n",
       " [9, 16],\n",
       " [16, 9],\n",
       " [9, 17],\n",
       " [17, 9],\n",
       " [9, 18],\n",
       " [18, 9],\n",
       " [9, 19],\n",
       " [19, 9],\n",
       " [9, 20],\n",
       " [20, 9],\n",
       " [9, 21],\n",
       " [21, 9],\n",
       " [9, 22],\n",
       " [22, 9],\n",
       " [9, 23],\n",
       " [23, 9],\n",
       " [9, 24],\n",
       " [24, 9],\n",
       " [9, 25],\n",
       " [25, 9],\n",
       " [9, 26],\n",
       " [26, 9],\n",
       " [10, 11],\n",
       " [11, 10],\n",
       " [10, 12],\n",
       " [12, 10],\n",
       " [10, 13],\n",
       " [13, 10],\n",
       " [10, 14],\n",
       " [14, 10],\n",
       " [10, 15],\n",
       " [15, 10],\n",
       " [10, 16],\n",
       " [16, 10],\n",
       " [10, 17],\n",
       " [17, 10],\n",
       " [10, 18],\n",
       " [18, 10],\n",
       " [10, 19],\n",
       " [19, 10],\n",
       " [10, 20],\n",
       " [20, 10],\n",
       " [10, 21],\n",
       " [21, 10],\n",
       " [10, 22],\n",
       " [22, 10],\n",
       " [10, 23],\n",
       " [23, 10],\n",
       " [10, 24],\n",
       " [24, 10],\n",
       " [10, 25],\n",
       " [25, 10],\n",
       " [10, 26],\n",
       " [26, 10],\n",
       " [11, 12],\n",
       " [12, 11],\n",
       " [11, 13],\n",
       " [13, 11],\n",
       " [11, 14],\n",
       " [14, 11],\n",
       " [11, 15],\n",
       " [15, 11],\n",
       " [11, 16],\n",
       " [16, 11],\n",
       " [11, 17],\n",
       " [17, 11],\n",
       " [11, 18],\n",
       " [18, 11],\n",
       " [11, 19],\n",
       " [19, 11],\n",
       " [11, 20],\n",
       " [20, 11],\n",
       " [11, 21],\n",
       " [21, 11],\n",
       " [11, 22],\n",
       " [22, 11],\n",
       " [11, 23],\n",
       " [23, 11],\n",
       " [11, 24],\n",
       " [24, 11],\n",
       " [11, 25],\n",
       " [25, 11],\n",
       " [11, 26],\n",
       " [26, 11],\n",
       " [12, 13],\n",
       " [13, 12],\n",
       " [12, 14],\n",
       " [14, 12],\n",
       " [12, 15],\n",
       " [15, 12],\n",
       " [12, 16],\n",
       " [16, 12],\n",
       " [12, 17],\n",
       " [17, 12],\n",
       " [12, 18],\n",
       " [18, 12],\n",
       " [12, 19],\n",
       " [19, 12],\n",
       " [12, 20],\n",
       " [20, 12],\n",
       " [12, 21],\n",
       " [21, 12],\n",
       " [12, 22],\n",
       " [22, 12],\n",
       " [12, 23],\n",
       " [23, 12],\n",
       " [12, 24],\n",
       " [24, 12],\n",
       " [12, 25],\n",
       " [25, 12],\n",
       " [12, 26],\n",
       " [26, 12],\n",
       " [13, 14],\n",
       " [14, 13],\n",
       " [13, 15],\n",
       " [15, 13],\n",
       " [13, 16],\n",
       " [16, 13],\n",
       " [13, 17],\n",
       " [17, 13],\n",
       " [13, 18],\n",
       " [18, 13],\n",
       " [13, 19],\n",
       " [19, 13],\n",
       " [13, 20],\n",
       " [20, 13],\n",
       " [13, 21],\n",
       " [21, 13],\n",
       " [13, 22],\n",
       " [22, 13],\n",
       " [13, 23],\n",
       " [23, 13],\n",
       " [13, 24],\n",
       " [24, 13],\n",
       " [13, 25],\n",
       " [25, 13],\n",
       " [13, 26],\n",
       " [26, 13],\n",
       " [14, 15],\n",
       " [15, 14],\n",
       " [14, 16],\n",
       " [16, 14],\n",
       " [14, 17],\n",
       " [17, 14],\n",
       " [14, 18],\n",
       " [18, 14],\n",
       " [14, 19],\n",
       " [19, 14],\n",
       " [14, 20],\n",
       " [20, 14],\n",
       " [14, 21],\n",
       " [21, 14],\n",
       " [14, 22],\n",
       " [22, 14],\n",
       " [14, 23],\n",
       " [23, 14],\n",
       " [14, 24],\n",
       " [24, 14],\n",
       " [14, 25],\n",
       " [25, 14],\n",
       " [14, 26],\n",
       " [26, 14],\n",
       " [15, 16],\n",
       " [16, 15],\n",
       " [15, 17],\n",
       " [17, 15],\n",
       " [15, 18],\n",
       " [18, 15],\n",
       " [15, 19],\n",
       " [19, 15],\n",
       " [15, 20],\n",
       " [20, 15],\n",
       " [15, 21],\n",
       " [21, 15],\n",
       " [15, 22],\n",
       " [22, 15],\n",
       " [15, 23],\n",
       " [23, 15],\n",
       " [15, 24],\n",
       " [24, 15],\n",
       " [15, 25],\n",
       " [25, 15],\n",
       " [15, 26],\n",
       " [26, 15],\n",
       " [16, 17],\n",
       " [17, 16],\n",
       " [16, 18],\n",
       " [18, 16],\n",
       " [16, 19],\n",
       " [19, 16],\n",
       " [16, 20],\n",
       " [20, 16],\n",
       " [16, 21],\n",
       " [21, 16],\n",
       " [16, 22],\n",
       " [22, 16],\n",
       " [16, 23],\n",
       " [23, 16],\n",
       " [16, 24],\n",
       " [24, 16],\n",
       " [16, 25],\n",
       " [25, 16],\n",
       " [16, 26],\n",
       " [26, 16],\n",
       " [17, 18],\n",
       " [18, 17],\n",
       " [17, 19],\n",
       " [19, 17],\n",
       " [17, 20],\n",
       " [20, 17],\n",
       " [17, 21],\n",
       " [21, 17],\n",
       " [17, 22],\n",
       " [22, 17],\n",
       " [17, 23],\n",
       " [23, 17],\n",
       " [17, 24],\n",
       " [24, 17],\n",
       " [17, 25],\n",
       " [25, 17],\n",
       " [17, 26],\n",
       " [26, 17],\n",
       " [18, 19],\n",
       " [19, 18],\n",
       " [18, 20],\n",
       " [20, 18],\n",
       " [18, 21],\n",
       " [21, 18],\n",
       " [18, 22],\n",
       " [22, 18],\n",
       " [18, 23],\n",
       " [23, 18],\n",
       " [18, 24],\n",
       " [24, 18],\n",
       " [18, 25],\n",
       " [25, 18],\n",
       " [18, 26],\n",
       " [26, 18],\n",
       " [19, 20],\n",
       " [20, 19],\n",
       " [19, 21],\n",
       " [21, 19],\n",
       " [19, 22],\n",
       " [22, 19],\n",
       " [19, 23],\n",
       " [23, 19],\n",
       " [19, 24],\n",
       " [24, 19],\n",
       " [19, 25],\n",
       " [25, 19],\n",
       " [19, 26],\n",
       " [26, 19],\n",
       " [20, 21],\n",
       " [21, 20],\n",
       " [20, 22],\n",
       " [22, 20],\n",
       " [20, 23],\n",
       " [23, 20],\n",
       " [20, 24],\n",
       " [24, 20],\n",
       " [20, 25],\n",
       " [25, 20],\n",
       " [20, 26],\n",
       " [26, 20],\n",
       " [21, 22],\n",
       " [22, 21],\n",
       " [21, 23],\n",
       " [23, 21],\n",
       " [21, 24],\n",
       " [24, 21],\n",
       " [21, 25],\n",
       " [25, 21],\n",
       " [21, 26],\n",
       " [26, 21],\n",
       " [22, 23],\n",
       " [23, 22],\n",
       " [22, 24],\n",
       " [24, 22],\n",
       " [22, 25],\n",
       " [25, 22],\n",
       " [22, 26],\n",
       " [26, 22],\n",
       " [23, 24],\n",
       " [24, 23],\n",
       " [23, 25],\n",
       " [25, 23],\n",
       " [23, 26],\n",
       " [26, 23],\n",
       " [24, 25],\n",
       " [25, 24],\n",
       " [24, 26],\n",
       " [26, 24],\n",
       " [25, 26],\n",
       " [26, 25]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_edge_index(dataframes):\n",
    "    edges = []\n",
    "    edges_verifications = []\n",
    "    keys = list(dataframes.keys())\n",
    "    for i in range(len(keys)):\n",
    "        for j in range(i + 1, len(keys)):\n",
    "            if i != j:\n",
    "                edges.append(([i, j]))\n",
    "                edges.append(([j, i]))\n",
    "                edges_verifications.append(([keys[i],keys[j],i, j]))\n",
    "                edges_verifications.append(([keys[j],keys[i],j, i]))\n",
    "    display(edges_verifications)\n",
    "    return edges\n",
    "\n",
    "edge_index = create_edge_index(dataframes)\n",
    "display(edge_index)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[72790024141, 72785524114, 0, 1, 395.4679457314314],\n",
       " [72785524114, 72790024141, 1, 0, 395.4679457314314],\n",
       " [72790024141, 72789094197, 0, 2, 129.15783117310457],\n",
       " [72789094197, 72790024141, 2, 0, 129.15783117310457],\n",
       " [72790024141, 72793024233, 0, 3, 342.5330553210191],\n",
       " [72793024233, 72790024141, 3, 0, 342.5330553210191],\n",
       " [72790024141, 72785794129, 0, 4, 438.10430559873856],\n",
       " [72785794129, 72790024141, 4, 0, 438.10430559873856],\n",
       " [72790024141, 72788594266, 0, 5, 432.0504921333856],\n",
       " [72788594266, 72790024141, 5, 0, 432.0504921333856],\n",
       " [72790024141, 72797624217, 0, 6, 437.11321302131137],\n",
       " [72797624217, 72790024141, 6, 0, 437.11321302131137],\n",
       " [72790024141, 72785024157, 0, 7, 369.0087247919059],\n",
       " [72785024157, 72790024141, 7, 0, 369.0087247919059],\n",
       " [72790024141, 72797094240, 0, 8, 503.7031148557305],\n",
       " [72797094240, 72790024141, 8, 0, 503.7031148557305],\n",
       " [72790024141, 72798594276, 0, 9, 455.6811328123654],\n",
       " [72798594276, 72790024141, 9, 0, 455.6811328123654],\n",
       " [72790024141, 72792424223, 0, 10, 474.59654599988056],\n",
       " [72792424223, 72790024141, 10, 0, 474.59654599988056],\n",
       " [72790024141, 72792894263, 0, 11, 348.0950737502581],\n",
       " [72792894263, 72790024141, 11, 0, 348.0950737502581],\n",
       " [72790024141, 72781024243, 0, 12, 128.6980383337319],\n",
       " [72781024243, 72790024141, 12, 0, 128.6980383337319],\n",
       " [72790024141, 72781524237, 0, 13, 835.9738346074198],\n",
       " [72781524237, 72790024141, 13, 0, 835.9738346074198],\n",
       " [72790024141, 72788324220, 0, 14, 171.59537540199554],\n",
       " [72788324220, 72790024141, 14, 0, 171.59537540199554],\n",
       " [72790024141, 72698824219, 0, 15, 384.56540064951065],\n",
       " [72698824219, 72790024141, 15, 0, 384.56540064951065],\n",
       " [72790024141, 72793894274, 0, 16, 373.58764051749137],\n",
       " [72793894274, 72790024141, 16, 0, 373.58764051749137],\n",
       " [72790024141, 74206024207, 0, 17, 362.1843030623904],\n",
       " [74206024207, 72790024141, 17, 0, 362.1843030623904],\n",
       " [72790024141, 72782724110, 0, 18, 32.53657446575969],\n",
       " [72782724110, 72790024141, 18, 0, 32.53657446575969],\n",
       " [72790024141, 72793724222, 0, 19, 306.4908073667999],\n",
       " [72793724222, 72790024141, 19, 0, 306.4908073667999],\n",
       " [72790024141, 72792594227, 0, 20, 404.50991220772727],\n",
       " [72792594227, 72790024141, 20, 0, 404.50991220772727],\n",
       " [72790024141, 72782594239, 0, 21, 52.81240627430448],\n",
       " [72782594239, 72790024141, 21, 0, 52.81240627430448],\n",
       " [72790024141, 72794504205, 0, 22, 404.8085057270694],\n",
       " [72794504205, 72790024141, 22, 0, 404.8085057270694],\n",
       " [72790024141, 72792394225, 0, 23, 505.4288197147213],\n",
       " [72792394225, 72790024141, 23, 0, 505.4288197147213],\n",
       " [72790024141, 72784524163, 0, 24, 286.0836462058196],\n",
       " [72784524163, 72790024141, 24, 0, 286.0836462058196],\n",
       " [72790024141, 72792024227, 0, 25, 412.6227567328569],\n",
       " [72792024227, 72790024141, 25, 0, 412.6227567328569],\n",
       " [72790024141, 72785694176, 0, 26, 272.19719043646785],\n",
       " [72785694176, 72790024141, 26, 0, 272.19719043646785],\n",
       " [72785524114, 72789094197, 1, 2, 390.14029117618804],\n",
       " [72789094197, 72785524114, 2, 1, 390.14029117618804],\n",
       " [72785524114, 72793024233, 1, 3, 727.6917541369755],\n",
       " [72793024233, 72785524114, 3, 1, 727.6917541369755],\n",
       " [72785524114, 72785794129, 1, 4, 109.85904921372176],\n",
       " [72785794129, 72785524114, 4, 1, 109.85904921372176],\n",
       " [72785524114, 72788594266, 1, 5, 798.995937782049],\n",
       " [72788594266, 72785524114, 5, 1, 798.995937782049],\n",
       " [72785524114, 72797624217, 1, 6, 802.4581375032445],\n",
       " [72797624217, 72785524114, 6, 1, 802.4581375032445],\n",
       " [72785524114, 72785024157, 1, 7, 33.690956835057186],\n",
       " [72785024157, 72785524114, 7, 1, 33.690956835057186],\n",
       " [72785524114, 72797094240, 1, 8, 865.1737430163511],\n",
       " [72797094240, 72785524114, 8, 1, 865.1737430163511],\n",
       " [72785524114, 72798594276, 1, 9, 825.6499844120128],\n",
       " [72798594276, 72785524114, 9, 1, 825.6499844120128],\n",
       " [72785524114, 72792424223, 1, 10, 860.6728795779842],\n",
       " [72792424223, 72785524114, 10, 1, 860.6728795779842],\n",
       " [72785524114, 72792894263, 1, 11, 724.9963923066945],\n",
       " [72792894263, 72785524114, 11, 1, 724.9963923066945],\n",
       " [72785524114, 72781024243, 1, 12, 495.92270395996894],\n",
       " [72781024243, 72785524114, 12, 1, 495.92270395996894],\n",
       " [72785524114, 72781524237, 1, 13, 535.6266022869553],\n",
       " [72781524237, 72785524114, 13, 1, 535.6266022869553],\n",
       " [72785524114, 72788324220, 1, 14, 314.54161595295784],\n",
       " [72788324220, 72785524114, 14, 1, 314.54161595295784],\n",
       " [72785524114, 72698824219, 1, 15, 763.6141112163369],\n",
       " [72698824219, 72785524114, 15, 1, 763.6141112163369],\n",
       " [72785524114, 72793894274, 1, 16, 759.3841400612968],\n",
       " [72793894274, 72785524114, 16, 1, 759.3841400612968],\n",
       " [72785524114, 74206024207, 1, 17, 748.5486591666476],\n",
       " [74206024207, 72785524114, 17, 1, 748.5486591666476],\n",
       " [72785524114, 72782724110, 1, 18, 416.12981588895354],\n",
       " [72782724110, 72785524114, 18, 1, 416.12981588895354],\n",
       " [72785524114, 72793724222, 1, 19, 678.7815012654146],\n",
       " [72793724222, 72785524114, 19, 1, 678.7815012654146],\n",
       " [72785524114, 72792594227, 1, 20, 784.7770299622911],\n",
       " [72792594227, 72785524114, 20, 1, 784.7770299622911],\n",
       " [72785524114, 72782594239, 1, 21, 419.79175595055284],\n",
       " [72782594239, 72785524114, 21, 1, 419.79175595055284],\n",
       " [72785524114, 72794504205, 1, 22, 786.18443118649],\n",
       " [72794504205, 72785524114, 22, 1, 786.18443118649],\n",
       " [72785524114, 72792394225, 1, 23, 886.2658783050533],\n",
       " [72792394225, 72785524114, 23, 1, 886.2658783050533],\n",
       " [72785524114, 72784524163, 1, 24, 655.5895444605051],\n",
       " [72784524163, 72785524114, 24, 1, 655.5895444605051],\n",
       " [72785524114, 72792024227, 1, 25, 798.318691116012],\n",
       " [72792024227, 72785524114, 25, 1, 798.318691116012],\n",
       " [72785524114, 72785694176, 1, 26, 157.5431429141368],\n",
       " [72785694176, 72785524114, 26, 1, 157.5431429141368],\n",
       " [72789094197, 72793024233, 2, 3, 370.42190175405364],\n",
       " [72793024233, 72789094197, 3, 2, 370.42190175405364],\n",
       " [72789094197, 72785794129, 2, 4, 460.5219705003162],\n",
       " [72785794129, 72789094197, 4, 2, 460.5219705003162],\n",
       " [72789094197, 72788594266, 2, 5, 432.51979230044606],\n",
       " [72788594266, 72789094197, 5, 2, 432.51979230044606],\n",
       " [72789094197, 72797624217, 2, 6, 417.3828379309705],\n",
       " [72797624217, 72789094197, 6, 2, 417.3828379309705],\n",
       " [72789094197, 72785024157, 2, 7, 365.04041085309774],\n",
       " [72785024157, 72789094197, 7, 2, 365.04041085309774],\n",
       " [72789094197, 72797094240, 2, 8, 508.89691489063597],\n",
       " [72797094240, 72789094197, 8, 2, 508.89691489063597],\n",
       " [72789094197, 72798594276, 2, 9, 445.736559884783],\n",
       " [72798594276, 72789094197, 9, 2, 445.736559884783],\n",
       " [72789094197, 72792424223, 2, 10, 534.5520542530354],\n",
       " [72792424223, 72789094197, 10, 2, 534.5520542530354],\n",
       " [72789094197, 72792894263, 2, 11, 372.6745336577991],\n",
       " [72792894263, 72789094197, 11, 2, 372.6745336577991],\n",
       " [72789094197, 72781024243, 2, 12, 236.9285970548754],\n",
       " [72781024243, 72789094197, 12, 2, 236.9285970548754],\n",
       " [72789094197, 72781524237, 2, 13, 831.0617187024466],\n",
       " [72781524237, 72789094197, 13, 2, 831.0617187024466],\n",
       " [72789094197, 72788324220, 2, 14, 221.69557651346355],\n",
       " [72788324220, 72789094197, 14, 2, 221.69557651346355],\n",
       " [72789094197, 72698824219, 2, 15, 470.9577296531585],\n",
       " [72698824219, 72789094197, 15, 2, 470.9577296531585],\n",
       " [72789094197, 72793894274, 2, 16, 406.24280451088293],\n",
       " [72793894274, 72789094197, 16, 2, 406.24280451088293],\n",
       " [72789094197, 74206024207, 2, 17, 399.71114545506987],\n",
       " [74206024207, 72789094197, 17, 2, 399.71114545506987],\n",
       " [72789094197, 72782724110, 2, 18, 147.68778122386612],\n",
       " [72782724110, 72789094197, 18, 2, 147.68778122386612],\n",
       " [72789094197, 72793724222, 2, 19, 313.8982068438734],\n",
       " [72793724222, 72789094197, 19, 2, 313.8982068438734],\n",
       " [72789094197, 72792594227, 2, 20, 435.3656912848407],\n",
       " [72792594227, 72789094197, 20, 2, 435.3656912848407],\n",
       " [72789094197, 72782594239, 2, 21, 130.20708355028395],\n",
       " [72782594239, 72789094197, 21, 2, 130.20708355028395],\n",
       " [72789094197, 72794504205, 2, 22, 406.9699486305622],\n",
       " [72794504205, 72789094197, 22, 2, 406.9699486305622],\n",
       " [72789094197, 72792394225, 2, 23, 539.0132078425393],\n",
       " [72792394225, 72789094197, 23, 2, 539.0132078425393],\n",
       " [72789094197, 72784524163, 2, 24, 368.91523621650714],\n",
       " [72784524163, 72789094197, 24, 2, 368.91523621650714],\n",
       " [72789094197, 72792024227, 2, 25, 452.3832910640423],\n",
       " [72792024227, 72789094197, 25, 2, 452.3832910640423],\n",
       " [72789094197, 72785694176, 2, 26, 270.2281521689557],\n",
       " [72785694176, 72789094197, 26, 2, 270.2281521689557],\n",
       " [72793024233, 72785794129, 3, 4, 775.0588941479449],\n",
       " [72785794129, 72793024233, 4, 3, 775.0588941479449],\n",
       " [72793024233, 72788594266, 3, 5, 120.26611512741678],\n",
       " [72788594266, 72793024233, 5, 3, 120.26611512741678],\n",
       " [72793024233, 72797624217, 3, 6, 165.52848353942377],\n",
       " [72797624217, 72793024233, 6, 3, 165.52848353942377],\n",
       " [72793024233, 72785024157, 3, 7, 704.0879836514051],\n",
       " [72785024157, 72793024233, 7, 3, 704.0879836514051],\n",
       " [72793024233, 72797094240, 3, 8, 185.28412975619443],\n",
       " [72797094240, 72793024233, 8, 3, 185.28412975619443],\n",
       " [72793024233, 72798594276, 3, 9, 152.43166309928537],\n",
       " [72798594276, 72793024233, 9, 3, 152.43166309928537],\n",
       " [72793024233, 72792424223, 3, 10, 187.27787071696568],\n",
       " [72792424223, 72793024233, 10, 3, 187.27787071696568],\n",
       " [72793024233, 72792894263, 3, 11, 41.159846544112625],\n",
       " [72792894263, 72793024233, 11, 3, 41.159846544112625],\n",
       " [72793024233, 72781024243, 3, 12, 266.9618608270944],\n",
       " [72781024243, 72793024233, 12, 3, 266.9618608270944],\n",
       " [72793024233, 72781524237, 3, 13, 1096.8318771247875],\n",
       " [72781524237, 72793024233, 13, 3, 1096.8318771247875],\n",
       " [72793024233, 72788324220, 3, 14, 443.4038293955239],\n",
       " [72788324220, 72793024233, 14, 3, 443.4038293955239],\n",
       " [72793024233, 72698824219, 3, 15, 225.02216925780087],\n",
       " [72698824219, 72793024233, 15, 3, 225.02216925780087],\n",
       " [72793024233, 72793894274, 3, 16, 36.85266364881154],\n",
       " [72793894274, 72793024233, 16, 3, 36.85266364881154],\n",
       " [72793024233, 74206024207, 3, 17, 37.9895765777111],\n",
       " [74206024207, 72793024233, 17, 3, 37.9895765777111],\n",
       " [72793024233, 72782724110, 3, 18, 333.6917238582668],\n",
       " [72782724110, 72793024233, 18, 3, 333.6917238582668],\n",
       " [72793024233, 72793724222, 3, 19, 76.27723241526142],\n",
       " [72793724222, 72793024233, 19, 3, 76.27723241526142],\n",
       " [72793024233, 72792594227, 3, 20, 72.18185980856553],\n",
       " [72792594227, 72793024233, 20, 3, 72.18185980856553],\n",
       " [72793024233, 72782594239, 3, 21, 309.0749255571238],\n",
       " [72782594239, 72793024233, 21, 3, 309.0749255571238],\n",
       " [72793024233, 72794504205, 3, 22, 107.09860018045799],\n",
       " [72794504205, 72793024233, 22, 3, 107.09860018045799],\n",
       " [72793024233, 72792394225, 3, 23, 171.23674671831446],\n",
       " [72792394225, 72793024233, 23, 3, 171.23674671831446],\n",
       " [72793024233, 72784524163, 3, 24, 276.0731537992283],\n",
       " [72784524163, 72793024233, 24, 3, 276.0731537992283],\n",
       " [72793024233, 72792024227, 3, 25, 85.99186556469607],\n",
       " [72792024227, 72793024233, 25, 3, 85.99186556469607],\n",
       " [72793024233, 72785694176, 3, 26, 611.0889227645558],\n",
       " [72785694176, 72793024233, 26, 3, 611.0889227645558],\n",
       " [72785794129, 72788594266, 4, 5, 856.701667766572],\n",
       " [72788594266, 72785794129, 5, 4, 856.701667766572],\n",
       " [72785794129, 72797624217, 4, 6, 865.4792367557994],\n",
       " [72797624217, 72785794129, 6, 4, 865.4792367557994],\n",
       " [72785794129, 72785024157, 4, 7, 117.74480456011335],\n",
       " [72785024157, 72785794129, 7, 4, 117.74480456011335],\n",
       " [72785794129, 72797094240, 4, 8, 921.4842980890041],\n",
       " [72797094240, 72785794129, 8, 4, 921.4842980890041],\n",
       " [72785794129, 72798594276, 4, 9, 885.9031028312606],\n",
       " [72798594276, 72785794129, 9, 4, 885.9031028312606],\n",
       " [72785794129, 72792424223, 4, 10, 890.5893542480043],\n",
       " [72792424223, 72785794129, 10, 4, 890.5893542480043],\n",
       " [72785794129, 72792894263, 4, 11, 774.366157782777],\n",
       " [72792894263, 72785794129, 11, 4, 774.366157782777],\n",
       " [72785794129, 72781024243, 4, 12, 524.6337306289196],\n",
       " [72781024243, 72785794129, 12, 4, 524.6337306289196],\n",
       " [72785794129, 72781524237, 4, 13, 540.0809528724242],\n",
       " [72781524237, 72785794129, 13, 4, 540.0809528724242],\n",
       " [72785794129, 72788324220, 4, 14, 357.13279391570444],\n",
       " [72788324220, 72785794129, 14, 4, 357.13279391570444],\n",
       " [72785794129, 72698824219, 4, 15, 780.4774458332262],\n",
       " [72698824219, 72785794129, 15, 4, 780.4774458332262],\n",
       " [72785794129, 72793894274, 4, 16, 804.444912957628],\n",
       " [72793894274, 72785794129, 16, 4, 804.444912957628],\n",
       " [72785794129, 74206024207, 4, 17, 791.9148221844617],\n",
       " [74206024207, 72785794129, 17, 4, 791.9148221844617],\n",
       " [72785794129, 72782724110, 4, 18, 454.1201925958629],\n",
       " [72782724110, 72785794129, 18, 4, 454.1201925958629],\n",
       " [72785794129, 72793724222, 4, 19, 734.2694292819892],\n",
       " [72793724222, 72785794129, 19, 4, 734.2694292819892],\n",
       " [72785794129, 72792594227, 4, 20, 830.6418814848106],\n",
       " [72792594227, 72785794129, 20, 4, 830.6418814848106],\n",
       " [72785794129, 72782594239, 4, 21, 467.37603414879794],\n",
       " [72782594239, 72785794129, 21, 4, 467.37603414879794],\n",
       " [72785794129, 72794504205, 4, 22, 840.9110561664348],\n",
       " [72794504205, 72785794129, 22, 4, 840.9110561664348],\n",
       " [72785794129, 72792394225, 4, 23, 929.3740741921883],\n",
       " [72792394225, 72785794129, 23, 4, 929.3740741921883],\n",
       " [72785794129, 72784524163, 4, 24, 673.3913688173589],\n",
       " [72784524163, 72785794129, 24, 4, 673.3913688173589],\n",
       " [72785794129, 72792024227, 4, 25, 839.7968409183139],\n",
       " [72792024227, 72785794129, 25, 4, 839.7968409183139],\n",
       " [72785794129, 72785694176, 4, 26, 209.78499150005845],\n",
       " [72785694176, 72785794129, 26, 4, 209.78499150005845],\n",
       " [72788594266, 72797624217, 5, 6, 110.15242913456197],\n",
       " [72797624217, 72788594266, 6, 5, 110.15242913456197],\n",
       " [72788594266, 72785024157, 5, 7, 777.4585937545535],\n",
       " [72785024157, 72788594266, 7, 5, 777.4585937545535],\n",
       " [72788594266, 72797094240, 5, 8, 85.13925057078639],\n",
       " [72797094240, 72788594266, 8, 5, 85.13925057078639],\n",
       " [72788594266, 72798594276, 5, 9, 74.97130998219406],\n",
       " [72798594276, 72788594266, 9, 5, 74.97130998219406],\n",
       " [72788594266, 72792424223, 5, 10, 240.43942234567194],\n",
       " [72792424223, 72788594266, 10, 5, 240.43942234567194],\n",
       " [72788594266, 72792894263, 5, 11, 103.9757703201679],\n",
       " [72792894263, 72788594266, 11, 5, 103.9757703201679],\n",
       " [72788594266, 72781024243, 5, 12, 369.69695959787833],\n",
       " [72781024243, 72788594266, 12, 5, 369.69695959787833],\n",
       " [72788594266, 72781524237, 5, 13, 1139.0521959187856],\n",
       " [72781524237, 72788594266, 13, 5, 1139.0521959187856],\n",
       " [72788594266, 72788324220, 5, 14, 516.0490346912795],\n",
       " [72788324220, 72788594266, 14, 5, 516.0490346912795],\n",
       " [72788594266, 72698824219, 5, 15, 330.5216959336131],\n",
       " [72698824219, 72788594266, 15, 5, 330.5216959336131],\n",
       " [72788594266, 72793894274, 5, 16, 117.91278236100351],\n",
       " [72793894274, 72788594266, 16, 5, 117.91278236100351],\n",
       " [72788594266, 74206024207, 5, 17, 133.34207595548997],\n",
       " [74206024207, 72788594266, 17, 5, 133.34207595548997],\n",
       " [72788594266, 72782724110, 5, 18, 428.7153028579556],\n",
       " [72782724110, 72788594266, 18, 5, 428.7153028579556],\n",
       " [72788594266, 72793724222, 5, 19, 125.7282441840991],\n",
       " [72793724222, 72788594266, 19, 5, 125.7282441840991],\n",
       " [72788594266, 72792594227, 5, 20, 101.97987733596389],\n",
       " [72792594227, 72788594266, 20, 5, 101.97987733596389],\n",
       " [72788594266, 72782594239, 5, 21, 392.60154297249403],\n",
       " [72782594239, 72788594266, 21, 5, 392.60154297249403],\n",
       " [72788594266, 72794504205, 5, 22, 108.31645122446824],\n",
       " [72794504205, 72788594266, 22, 5, 108.31645122446824],\n",
       " [72788594266, 72792394225, 5, 23, 153.40108385187867],\n",
       " [72792394225, 72788594266, 23, 5, 153.40108385187867],\n",
       " [72788594266, 72784524163, 5, 24, 392.1898886399527],\n",
       " [72784524163, 72788594266, 24, 5, 392.1898886399527],\n",
       " [72788594266, 72792024227, 5, 25, 137.25092367847324],\n",
       " [72792024227, 72788594266, 25, 5, 137.25092367847324],\n",
       " [72788594266, 72785694176, 5, 26, 690.174750212649],\n",
       " [72785694176, 72788594266, 26, 5, 690.174750212649],\n",
       " [72797624217, 72785024157, 6, 7, 778.6874797868297],\n",
       " [72785024157, 72797624217, 7, 6, 778.6874797868297],\n",
       " [72797624217, 72797094240, 6, 8, 177.46961023734613],\n",
       " [72797094240, 72797624217, 8, 6, 177.46961023734613],\n",
       " [72797624217, 72798594276, 6, 9, 48.71149879169117],\n",
       " [72798594276, 72797624217, 9, 6, 48.71149879169117],\n",
       " [72797624217, 72792424223, 6, 10, 302.0846527899822],\n",
       " [72792424223, 72797624217, 10, 6, 302.0846527899822],\n",
       " [72797624217, 72792894263, 6, 11, 172.27646695285324],\n",
       " [72792894263, 72797624217, 11, 6, 172.27646695285324],\n",
       " [72797624217, 72781024243, 6, 12, 399.97805818217563],\n",
       " [72781024243, 72797624217, 12, 6, 399.97805818217563],\n",
       " [72797624217, 72781524237, 6, 13, 1176.4852153080844],\n",
       " [72781524237, 72797624217, 13, 6, 1176.4852153080844],\n",
       " [72797624217, 72788324220, 6, 14, 545.6427005439592],\n",
       " [72788324220, 72797624217, 14, 6, 545.6427005439592],\n",
       " [72797624217, 72698824219, 6, 15, 369.3977887607593],\n",
       " [72698824219, 72797624217, 15, 6, 369.3977887607593],\n",
       " [72797624217, 72793894274, 6, 16, 175.55537172802033],\n",
       " [72793894274, 72797624217, 16, 6, 175.55537172802033],\n",
       " [72797624217, 74206024207, 6, 17, 190.70446482220075],\n",
       " [74206024207, 72797624217, 17, 6, 190.70446482220075],\n",
       " [72797624217, 72782724110, 6, 18, 431.096161984872],\n",
       " [72782724110, 72797624217, 18, 6, 431.096161984872],\n",
       " [72797624217, 72793724222, 6, 19, 156.64330089805705],\n",
       " [72793724222, 72797624217, 19, 6, 156.64330089805705],\n",
       " [72797624217, 72792594227, 6, 20, 183.24573042058552],\n",
       " [72792594227, 72797624217, 20, 6, 183.24573042058552],\n",
       " [72797624217, 72782594239, 6, 21, 405.4909815288361],\n",
       " [72782594239, 72797624217, 21, 6, 405.4909815288361],\n",
       " [72797624217, 72794504205, 6, 22, 76.4859126118275],\n",
       " [72794504205, 72797624217, 22, 6, 76.4859126118275],\n",
       " [72797624217, 72792394225, 6, 23, 231.7098185991757],\n",
       " [72792394225, 72797624217, 23, 6, 231.7098185991757],\n",
       " [72797624217, 72784524163, 6, 24, 388.4359336032824],\n",
       " [72784524163, 72797624217, 24, 6, 388.4359336032824],\n",
       " [72797624217, 72792024227, 6, 25, 205.33116784821942],\n",
       " [72792024227, 72797624217, 25, 6, 205.33116784821942],\n",
       " [72797624217, 72785694176, 6, 26, 682.4260680955291],\n",
       " [72785694176, 72797624217, 26, 6, 682.4260680955291],\n",
       " [72785024157, 72797094240, 7, 8, 845.1366146343045],\n",
       " [72797094240, 72785024157, 8, 7, 845.1366146343045],\n",
       " [72785024157, 72798594276, 7, 9, 802.4776134962176],\n",
       " [72798594276, 72785024157, 9, 7, 802.4776134962176],\n",
       " [72785024157, 72792424223, 7, 10, 837.0612376685274],\n",
       " [72792424223, 72785024157, 10, 7, 837.0612376685274],\n",
       " [72785024157, 72792894263, 7, 11, 702.8044715545802],\n",
       " [72792894263, 72785024157, 11, 7, 702.8044715545802],\n",
       " [72785024157, 72781024243, 7, 12, 472.23598917705993],\n",
       " [72781024243, 72785024157, 12, 7, 472.23598917705993],\n",
       " [72785024157, 72781524237, 7, 13, 567.9966156275789],\n",
       " [72781524237, 72785024157, 13, 7, 567.9966156275789],\n",
       " [72785024157, 72788324220, 7, 14, 299.672336109386],\n",
       " [72788324220, 72785024157, 14, 7, 299.672336109386],\n",
       " [72785024157, 72698824219, 7, 15, 738.0303386253106],\n",
       " [72698824219, 72785024157, 15, 7, 738.0303386253106],\n",
       " [72785024157, 72793894274, 7, 16, 735.9632694864449],\n",
       " [72793894274, 72785024157, 16, 7, 735.9632694864449],\n",
       " [72785024157, 74206024207, 7, 17, 725.1092500944276],\n",
       " [74206024207, 72785024157, 17, 7, 725.1092500944276],\n",
       " [72785024157, 72782724110, 7, 18, 388.53428759844127],\n",
       " [72782724110, 72785024157, 18, 7, 388.53428759844127],\n",
       " [72785024157, 72793724222, 7, 19, 656.1420174916371],\n",
       " [72793724222, 72785024157, 19, 7, 656.1420174916371],\n",
       " [72785024157, 72792594227, 7, 20, 762.4710980022056],\n",
       " [72792594227, 72785024157, 20, 7, 762.4710980022056],\n",
       " [72785024157, 72782594239, 7, 21, 395.81370216605853],\n",
       " [72782594239, 72785024157, 21, 7, 395.81370216605853],\n",
       " [72785024157, 72794504205, 7, 22, 761.3237576215986],\n",
       " [72794504205, 72785024157, 22, 7, 761.3237576215986],\n",
       " [72785024157, 72792394225, 7, 23, 864.2082908552868],\n",
       " [72792394225, 72785024157, 23, 7, 864.2082908552868],\n",
       " [72785024157, 72784524163, 7, 24, 626.0309963149288],\n",
       " [72784524163, 72785024157, 24, 7, 626.0309963149288],\n",
       " [72785024157, 72792024227, 7, 25, 775.1913077729471],\n",
       " [72792024227, 72785024157, 25, 7, 775.1913077729471],\n",
       " [72785024157, 72785694176, 7, 26, 124.27275180830249],\n",
       " [72785694176, 72785024157, 26, 7, 124.27275180830249],\n",
       " [72797094240, 72798594276, 8, 9, 132.5097212349861],\n",
       " [72798594276, 72797094240, 9, 8, 132.5097212349861],\n",
       " [72797094240, 72792424223, 8, 10, 243.7073144688935],\n",
       " [72792424223, 72797094240, 10, 8, 243.7073144688935],\n",
       " [72797094240, 72792894263, 8, 11, 163.47287592761103],\n",
       " [72792894263, 72797094240, 11, 8, 163.47287592761103],\n",
       " [72797094240, 72781024243, 8, 12, 430.6615227573649],\n",
       " [72781024243, 72797094240, 12, 8, 430.6615227573649],\n",
       " [72797094240, 72781524237, 8, 13, 1177.64872286148],\n",
       " [72781524237, 72797094240, 13, 8, 1177.64872286148],\n",
       " [72797094240, 72788324220, 8, 14, 572.9402144031785],\n",
       " [72788324220, 72797094240, 14, 8, 572.9402144031785],\n",
       " [72797094240, 72698824219, 8, 15, 365.1202761234219],\n",
       " [72698824219, 72797094240, 15, 8, 365.1202761234219],\n",
       " [72797094240, 72793894274, 8, 16, 169.20923499165121],\n",
       " [72793894274, 72797094240, 16, 8, 169.20923499165121],\n",
       " [72797094240, 74206024207, 8, 17, 183.39281681071296],\n",
       " [74206024207, 72797094240, 17, 8, 183.39281681071296],\n",
       " [72797094240, 72782724110, 8, 18, 501.4584011725272],\n",
       " [72782724110, 72797094240, 18, 8, 501.4584011725272],\n",
       " [72797094240, 72793724222, 8, 19, 202.41419212127153],\n",
       " [72793724222, 72797094240, 19, 8, 202.41419212127153],\n",
       " [72797094240, 72792594227, 8, 20, 134.47678138943274],\n",
       " [72792594227, 72797094240, 20, 8, 134.47678138943274],\n",
       " [72797094240, 72782594239, 8, 21, 461.61393826175714],\n",
       " [72782594239, 72797094240, 21, 8, 461.61393826175714],\n",
       " [72797094240, 72794504205, 8, 22, 180.6112148305096],\n",
       " [72794504205, 72797094240, 22, 8, 180.6112148305096],\n",
       " [72797094240, 72792394225, 8, 23, 128.08708542091654],\n",
       " [72792394225, 72797094240, 23, 8, 128.08708542091654],\n",
       " [72797094240, 72784524163, 8, 24, 456.1447880245876],\n",
       " [72784524163, 72797094240, 24, 8, 456.1447880245876],\n",
       " [72797094240, 72792024227, 8, 25, 164.101942102309],\n",
       " [72792024227, 72797094240, 25, 8, 164.101942102309],\n",
       " [72797094240, 72785694176, 8, 26, 763.0321197403028],\n",
       " [72785694176, 72797094240, 26, 8, 763.0321197403028],\n",
       " [72798594276, 72792424223, 9, 10, 268.89465547374556],\n",
       " [72792424223, 72798594276, 10, 9, 268.89465547374556],\n",
       " [72798594276, 72792894263, 9, 11, 154.53742133091865],\n",
       " [72792894263, 72798594276, 11, 9, 154.53742133091865],\n",
       " [72798594276, 72781024243, 9, 12, 405.60664848136616],\n",
       " [72781024243, 72798594276, 12, 9, 405.60664848136616],\n",
       " [72798594276, 72781524237, 9, 13, 1187.3176896736784],\n",
       " [72781524237, 72798594276, 13, 9, 1187.3176896736784],\n",
       " [72798594276, 72788324220, 9, 14, 557.1020923327553],\n",
       " [72788324220, 72798594276, 14, 9, 557.1020923327553],\n",
       " [72798594276, 72698824219, 9, 15, 353.92282978101207],\n",
       " [72698824219, 72798594276, 15, 9, 353.92282978101207],\n",
       " [72798594276, 72793894274, 9, 16, 153.1723115228679],\n",
       " [72793894274, 72798594276, 16, 9, 153.1723115228679],\n",
       " [72798594276, 74206024207, 9, 17, 170.09372061722416],\n",
       " [74206024207, 72798594276, 17, 9, 170.09372061722416],\n",
       " [72798594276, 72782724110, 9, 18, 449.65266636740307],\n",
       " [72782724110, 72798594276, 18, 9, 449.65266636740307],\n",
       " [72798594276, 72793724222, 9, 19, 158.4233084920311],\n",
       " [72793724222, 72798594276, 19, 9, 158.4233084920311],\n",
       " [72798594276, 72792594227, 9, 20, 151.2637681156068],\n",
       " [72792594227, 72798594276, 20, 9, 151.2637681156068],\n",
       " [72798594276, 72782594239, 9, 21, 421.5198568724629],\n",
       " [72782594239, 72798594276, 21, 9, 421.5198568724629],\n",
       " [72798594276, 72794504205, 9, 22, 75.9354339763056],\n",
       " [72794504205, 72798594276, 22, 9, 75.9354339763056],\n",
       " [72798594276, 72792394225, 9, 23, 187.20830867381414],\n",
       " [72792394225, 72798594276, 23, 9, 187.20830867381414],\n",
       " [72798594276, 72784524163, 9, 24, 396.00851322484175],\n",
       " [72784524163, 72798594276, 24, 9, 396.00851322484175],\n",
       " [72798594276, 72792024227, 9, 25, 174.23493854899442],\n",
       " [72792024227, 72798594276, 25, 9, 174.23493854899442],\n",
       " [72798594276, 72785694176, 9, 26, 708.5304039928845],\n",
       " [72785694176, 72798594276, 26, 9, 708.5304039928845],\n",
       " [72792424223, 72792894263, 10, 11, 199.69966656538062],\n",
       " [72792894263, 72792424223, 11, 10, 199.69966656538062],\n",
       " [72792424223, 72781024243, 10, 12, 366.6429361022303],\n",
       " [72781024243, 72792424223, 12, 10, 366.6429361022303],\n",
       " [72792424223, 72781524237, 10, 13, 1213.3303112543872],\n",
       " [72781524237, 72792424223, 13, 10, 1213.3303112543872],\n",
       " [72792424223, 72788324220, 10, 14, 565.7742092205691],\n",
       " [72788324220, 72792424223, 14, 10, 565.7742092205691],\n",
       " [72792424223, 72698824219, 10, 15, 158.75602862100996],\n",
       " [72698824219, 72792424223, 15, 10, 158.75602862100996],\n",
       " [72792424223, 72793894274, 10, 16, 154.05814985745886],\n",
       " [72793894274, 72792424223, 16, 10, 154.05814985745886],\n",
       " [72792424223, 74206024207, 10, 17, 150.49523188191975],\n",
       " [74206024207, 72792424223, 17, 10, 150.49523188191975],\n",
       " [72792424223, 72782724110, 10, 18, 459.96707604415764],\n",
       " [72782724110, 72792424223, 18, 10, 459.96707604415764],\n",
       " [72792424223, 72793724222, 10, 19, 261.56576121699044],\n",
       " [72793724222, 72792424223, 19, 10, 261.56576121699044],\n",
       " [72792424223, 72792594227, 10, 20, 148.37325199067052],\n",
       " [72792594227, 72792424223, 20, 10, 148.37325199067052],\n",
       " [72792424223, 72782594239, 10, 21, 447.531376407682],\n",
       " [72782594239, 72792424223, 21, 10, 447.531376407682],\n",
       " [72792424223, 72794504205, 10, 22, 236.6757463314917],\n",
       " [72794504205, 72792424223, 22, 10, 236.6757463314917],\n",
       " [72792424223, 72792394225, 10, 23, 123.91187323342717],\n",
       " [72792394225, 72792424223, 23, 10, 123.91187323342717],\n",
       " [72792424223, 72784524163, 10, 24, 313.3574949442864],\n",
       " [72784524163, 72792424223, 24, 10, 313.3574949442864],\n",
       " [72792424223, 72792024227, 10, 25, 109.93432994721853],\n",
       " [72792024227, 72792424223, 25, 10, 109.93432994721853],\n",
       " [72792424223, 72785694176, 10, 26, 745.579780595948],\n",
       " [72785694176, 72792424223, 26, 10, 745.579780595948],\n",
       " [72792894263, 72781024243, 11, 12, 271.20003239227077],\n",
       " [72781024243, 72792894263, 12, 11, 271.20003239227077],\n",
       " [72792894263, 72781524237, 11, 13, 1077.0374760399814],\n",
       " [72781524237, 72792894263, 13, 11, 1077.0374760399814],\n",
       " [72792894263, 72788324220, 11, 14, 434.420916611982],\n",
       " [72788324220, 72792894263, 14, 11, 434.420916611982],\n",
       " [72792894263, 72698824219, 11, 15, 249.12813853110475],\n",
       " [72698824219, 72792894263, 15, 11, 249.12813853110475],\n",
       " [72792894263, 72793894274, 11, 16, 54.60340565924095],\n",
       " [72793894274, 72792894263, 16, 11, 54.60340565924095],\n",
       " [72792894263, 74206024207, 11, 17, 56.69074125354086],\n",
       " [74206024207, 72792894263, 17, 11, 56.69074125354086],\n",
       " [72792894263, 72782724110, 11, 18, 342.83747887822733],\n",
       " [72782724110, 72792894263, 18, 11, 342.83747887822733],\n",
       " [72792894263, 72793724222, 11, 19, 68.6532776654429],\n",
       " [72793724222, 72792894263, 19, 11, 68.6532776654429],\n",
       " [72792894263, 72792594227, 11, 20, 64.41876337172579],\n",
       " [72792594227, 72792894263, 20, 11, 64.41876337172579],\n",
       " [72792894263, 72782594239, 11, 21, 309.79708423419913],\n",
       " [72782594239, 72792894263, 21, 11, 309.79708423419913],\n",
       " [72792894263, 72794504205, 11, 22, 128.34321457181807],\n",
       " [72794504205, 72792894263, 22, 11, 128.34321457181807],\n",
       " [72792894263, 72792394225, 11, 23, 167.4998627149666],\n",
       " [72792394225, 72792894263, 23, 11, 167.4998627149666],\n",
       " [72792894263, 72784524163, 11, 24, 308.66359444651886],\n",
       " [72784524163, 72792894263, 24, 11, 308.66359444651886],\n",
       " [72792894263, 72792024227, 11, 25, 94.10375597750865],\n",
       " [72792024227, 72792894263, 25, 11, 94.10375597750865],\n",
       " [72792894263, 72785694176, 11, 26, 614.9675457923169],\n",
       " [72785694176, 72792894263, 26, 11, 614.9675457923169],\n",
       " [72781024243, 72781524237, 12, 13, 891.3237029023409],\n",
       " [72781524237, 72781024243, 13, 12, 891.3237029023409],\n",
       " [72781024243, 72788324220, 12, 14, 217.85555378560997],\n",
       " [72788324220, 72781024243, 14, 12, 217.85555378560997],\n",
       " [72781024243, 72698824219, 12, 15, 275.33106494313824],\n",
       " [72698824219, 72781024243, 15, 12, 275.33106494313824],\n",
       " [72781024243, 72793894274, 12, 16, 290.27386357894756],\n",
       " [72793894274, 72781024243, 16, 12, 290.27386357894756],\n",
       " [72781024243, 74206024207, 12, 17, 275.46064771106325],\n",
       " [74206024207, 72781024243, 17, 12, 275.46064771106325],\n",
       " [72781024243, 72782724110, 12, 18, 121.40670159689395],\n",
       " [72782724110, 72781024243, 18, 12, 121.40670159689395],\n",
       " [72781024243, 72793724222, 12, 19, 252.86361822595595],\n",
       " [72793724222, 72781024243, 19, 12, 252.86361822595595],\n",
       " [72781024243, 72792594227, 12, 20, 317.66456605180196],\n",
       " [72792594227, 72781024243, 20, 12, 317.66456605180196],\n",
       " [72781024243, 72782594239, 12, 21, 111.45482222908821],\n",
       " [72782594239, 72781024243, 21, 12, 111.45482222908821],\n",
       " [72781024243, 72794504205, 12, 22, 352.74223587837065],\n",
       " [72794504205, 72781024243, 22, 12, 352.74223587837065],\n",
       " [72781024243, 72792394225, 12, 23, 411.2440393694844],\n",
       " [72792394225, 72781024243, 23, 12, 411.2440393694844],\n",
       " [72781024243, 72784524163, 12, 24, 228.97116863460516],\n",
       " [72784524163, 72781024243, 24, 12, 228.97116863460516],\n",
       " [72781024243, 72792024227, 12, 25, 319.85603194172165],\n",
       " [72792024227, 72781024243, 25, 12, 319.85603194172165],\n",
       " [72781024243, 72785694176, 12, 26, 386.6621826263452],\n",
       " [72785694176, 72781024243, 26, 12, 386.6621826263452],\n",
       " [72781524237, 72788324220, 13, 14, 677.4867035602812],\n",
       " [72788324220, 72781524237, 14, 13, 677.4867035602812],\n",
       " [72781524237, 72698824219, 13, 15, 1150.4386824494006],\n",
       " [72698824219, 72781524237, 15, 13, 1150.4386824494006],\n",
       " [72781524237, 72793894274, 13, 16, 1122.20080799066],\n",
       " [72793894274, 72781524237, 16, 13, 1122.20080799066],\n",
       " [72781524237, 74206024207, 13, 17, 1112.0640517140919],\n",
       " [74206024207, 72781524237, 17, 13, 1112.0640517140919],\n",
       " [72781524237, 72782724110, 13, 18, 864.0517998527835],\n",
       " [72782724110, 72781524237, 18, 13, 864.0517998527835],\n",
       " [72781524237, 72793724222, 13, 19, 1044.4930844703567],\n",
       " [72793724222, 72781524237, 19, 13, 1044.4930844703567],\n",
       " [72781524237, 72792594227, 13, 20, 1130.7362905229766],\n",
       " [72792594227, 72781524237, 20, 13, 1130.7362905229766],\n",
       " [72781524237, 72782594239, 13, 21, 833.7147080759753],\n",
       " [72782594239, 72781524237, 21, 13, 833.7147080759753],\n",
       " [72781524237, 72794504205, 13, 22, 1170.6533446424978],\n",
       " [72794504205, 72781524237, 22, 13, 1170.6533446424978],\n",
       " [72781524237, 72792394225, 13, 23, 1218.574885845403],\n",
       " [72792394225, 72781524237, 23, 13, 1218.574885845403],\n",
       " [72781524237, 72784524163, 13, 24, 1103.3829323700534],\n",
       " [72784524163, 72781524237, 24, 13, 1103.3829323700534],\n",
       " [72781524237, 72792024227, 13, 25, 1152.414996794818],\n",
       " [72792024227, 72781524237, 25, 13, 1152.414996794818],\n",
       " [72781524237, 72785694176, 13, 26, 683.9489018806388],\n",
       " [72785694176, 72781524237, 26, 13, 683.9489018806388],\n",
       " [72788324220, 72698824219, 14, 15, 489.8134161401565],\n",
       " [72698824219, 72788324220, 15, 14, 489.8134161401565],\n",
       " [72788324220, 72793894274, 14, 16, 470.9676802550459],\n",
       " [72793894274, 72788324220, 16, 14, 470.9676802550459],\n",
       " [72788324220, 74206024207, 14, 17, 459.0274176052015],\n",
       " [74206024207, 72788324220, 17, 14, 459.0274176052015],\n",
       " [72788324220, 72782724110, 14, 18, 199.59384465449898],\n",
       " [72782724110, 72788324220, 18, 14, 199.59384465449898],\n",
       " [72788324220, 72793724222, 14, 19, 400.7840441174383],\n",
       " [72793724222, 72788324220, 19, 14, 400.7840441174383],\n",
       " [72788324220, 72792594227, 14, 20, 490.3741382897129],\n",
       " [72792594227, 72788324220, 20, 14, 490.3741382897129],\n",
       " [72788324220, 72782594239, 14, 21, 162.11769623669784],\n",
       " [72782594239, 72788324220, 21, 14, 162.11769623669784],\n",
       " [72788324220, 72794504205, 14, 22, 520.9373250801362],\n",
       " [72794504205, 72788324220, 22, 14, 520.9373250801362],\n",
       " [72788324220, 72792394225, 14, 23, 587.5949150283049],\n",
       " [72792394225, 72788324220, 23, 14, 587.5949150283049],\n",
       " [72788324220, 72784524163, 14, 24, 432.7632336033511],\n",
       " [72784524163, 72788324220, 24, 14, 432.7632336033511],\n",
       " [72788324220, 72792024227, 14, 25, 504.8208347435879],\n",
       " [72792024227, 72788324220, 25, 14, 504.8208347435879],\n",
       " [72788324220, 72785694176, 14, 26, 259.92185050812566],\n",
       " [72785694176, 72788324220, 26, 14, 259.92185050812566],\n",
       " [72698824219, 72793894274, 15, 16, 213.47199631168857],\n",
       " [72793894274, 72698824219, 16, 15, 213.47199631168857],\n",
       " [72698824219, 74206024207, 15, 17, 199.7906856396986],\n",
       " [74206024207, 72698824219, 17, 15, 199.7906856396986],\n",
       " [72698824219, 72782724110, 15, 18, 363.3873739871944],\n",
       " [72782724110, 72698824219, 18, 15, 363.3873739871944],\n",
       " [72698824219, 72793724222, 15, 19, 286.49766075950384],\n",
       " [72793724222, 72698824219, 19, 15, 286.49766075950384],\n",
       " [72698824219, 72792594227, 15, 20, 235.6189290359573],\n",
       " [72792594227, 72698824219, 20, 15, 235.6189290359573],\n",
       " [72698824219, 72782594239, 15, 21, 371.98475050597],\n",
       " [72782594239, 72698824219, 21, 15, 371.98475050597],\n",
       " [72698824219, 72794504205, 15, 22, 293.97835633481196],\n",
       " [72794504205, 72698824219, 22, 15, 293.97835633481196],\n",
       " [72698824219, 72792394225, 15, 23, 268.7494270590419],\n",
       " [72792394225, 72698824219, 23, 15, 268.7494270590419],\n",
       " [72698824219, 72784524163, 15, 24, 181.34059793217904],\n",
       " [72784524163, 72698824219, 24, 15, 181.34059793217904],\n",
       " [72698824219, 72792024227, 15, 25, 201.61183866011444],\n",
       " [72792024227, 72698824219, 25, 15, 201.61183866011444],\n",
       " [72698824219, 72785694176, 15, 26, 642.4950004501114],\n",
       " [72785694176, 72698824219, 26, 15, 642.4950004501114],\n",
       " [72793894274, 74206024207, 16, 17, 17.716247962574563],\n",
       " [74206024207, 72793894274, 17, 16, 17.716247962574563],\n",
       " [72793894274, 72782724110, 16, 18, 363.9821600522806],\n",
       " [72782724110, 72793894274, 18, 16, 363.9821600522806],\n",
       " [72793894274, 72793724222, 16, 19, 109.51765254130393],\n",
       " [72793724222, 72793894274, 19, 16, 109.51765254130393],\n",
       " [72793894274, 72792594227, 16, 20, 42.988933127483634],\n",
       " [72792594227, 72793894274, 20, 16, 42.988933127483634],\n",
       " [72793894274, 72782594239, 16, 21, 340.3237239231559],\n",
       " [72782594239, 72793894274, 21, 16, 340.3237239231559],\n",
       " [72793894274, 72794504205, 16, 22, 114.05670021657697],\n",
       " [72794504205, 72793894274, 22, 16, 114.05670021657697],\n",
       " [72793894274, 72792394225, 16, 23, 136.43815241862657],\n",
       " [72792394225, 72793894274, 23, 16, 136.43815241862657],\n",
       " [72793894274, 72784524163, 16, 24, 287.7450730575042],\n",
       " [72784524163, 72793894274, 24, 16, 287.7450730575042],\n",
       " [72793894274, 72792024227, 16, 25, 49.41202473657626],\n",
       " [72792024227, 72793894274, 25, 16, 49.41202473657626],\n",
       " [72793894274, 72785694176, 16, 26, 643.6682517559431],\n",
       " [72785694176, 72793894274, 26, 16, 643.6682517559431],\n",
       " [74206024207, 72782724110, 17, 18, 352.2158347034709],\n",
       " [72782724110, 74206024207, 18, 17, 352.2158347034709],\n",
       " [74206024207, 72793724222, 17, 19, 111.26188855442645],\n",
       " [72793724222, 74206024207, 19, 17, 111.26188855442645],\n",
       " [74206024207, 72792594227, 17, 20, 52.561875466910436],\n",
       " [72792594227, 74206024207, 20, 17, 52.561875466910436],\n",
       " [74206024207, 72782594239, 17, 21, 329.3368670529716],\n",
       " [72782594239, 74206024207, 21, 17, 329.3368670529716],\n",
       " [74206024207, 72794504205, 17, 22, 128.01154758942099],\n",
       " [72794504205, 74206024207, 22, 17, 128.01154758942099],\n",
       " [74206024207, 72792394225, 17, 23, 145.49531716211806],\n",
       " [72792394225, 74206024207, 23, 17, 145.49531716211806],\n",
       " [74206024207, 72784524163, 17, 24, 275.6435250697721],\n",
       " [72784524163, 74206024207, 24, 17, 275.6435250697721],\n",
       " [74206024207, 72792024227, 17, 25, 52.80087090274467],\n",
       " [72792024227, 74206024207, 25, 17, 52.80087090274467],\n",
       " [74206024207, 72785694176, 17, 26, 633.1197418989807],\n",
       " [72785694176, 74206024207, 26, 17, 633.1197418989807],\n",
       " [72782724110, 72793724222, 18, 19, 303.31516235822005],\n",
       " [72793724222, 72782724110, 19, 18, 303.31516235822005],\n",
       " [72782724110, 72792594227, 18, 20, 396.9755284116139],\n",
       " [72792594227, 72782724110, 20, 18, 396.9755284116139],\n",
       " [72782724110, 72782594239, 18, 21, 73.74026824660453],\n",
       " [72782594239, 72782724110, 21, 18, 73.74026824660453],\n",
       " [72782724110, 72794504205, 18, 22, 394.74559894207687],\n",
       " [72794504205, 72782724110, 22, 18, 394.74559894207687],\n",
       " [72782724110, 72792394225, 18, 23, 496.478460291939],\n",
       " [72792394225, 72782724110, 23, 18, 496.478460291939],\n",
       " [72782724110, 72784524163, 18, 24, 256.1592053257525],\n",
       " [72784524163, 72782724110, 24, 18, 256.1592053257525],\n",
       " [72782724110, 72792024227, 18, 25, 402.1731282608459],\n",
       " [72792024227, 72782724110, 25, 18, 402.1731282608459],\n",
       " [72782724110, 72785694176, 18, 26, 286.80868520838226],\n",
       " [72785694176, 72782724110, 26, 18, 286.80868520838226],\n",
       " [72793724222, 72792594227, 19, 20, 129.67576043022495],\n",
       " [72792594227, 72793724222, 20, 19, 129.67576043022495],\n",
       " [72793724222, 72782594239, 19, 21, 268.31498195846285],\n",
       " [72782594239, 72793724222, 21, 19, 268.31498195846285],\n",
       " [72793724222, 72794504205, 19, 22, 128.38153131965652],\n",
       " [72794504205, 72793724222, 22, 19, 128.38153131965652],\n",
       " [72793724222, 72792394225, 19, 23, 230.11767302669344],\n",
       " [72792394225, 72793724222, 23, 19, 230.11767302669344],\n",
       " [72793724222, 72784524163, 19, 24, 305.30416194483115],\n",
       " [72784524163, 72793724222, 24, 19, 305.30416194483115],\n",
       " [72793724222, 72792024227, 19, 25, 156.88223261226412],\n",
       " [72792024227, 72793724222, 25, 19, 156.88223261226412],\n",
       " [72793724222, 72785694176, 19, 26, 566.3246868369048],\n",
       " [72785694176, 72793724222, 26, 19, 566.3246868369048],\n",
       " [72792594227, 72782594239, 20, 21, 368.04423285388],\n",
       " [72782594239, 72792594227, 21, 20, 368.04423285388],\n",
       " [72792594227, 72794504205, 20, 22, 133.10933796080906],\n",
       " [72794504205, 72792594227, 22, 20, 133.10933796080906],\n",
       " [72792594227, 72792394225, 20, 23, 103.88553424589651],\n",
       " [72792394225, 72792594227, 23, 20, 103.88553424589651],\n",
       " [72792594227, 72784524163, 20, 24, 327.04970158433304],\n",
       " [72784524163, 72792594227, 24, 20, 327.04970158433304],\n",
       " [72792594227, 72792024227, 20, 25, 41.59043713208587],\n",
       " [72792024227, 72792594227, 25, 20, 41.59043713208587],\n",
       " [72792594227, 72785694176, 20, 26, 674.0342619771884],\n",
       " [72785694176, 72792594227, 26, 20, 674.0342619771884],\n",
       " [72782594239, 72794504205, 21, 22, 375.88513957717953],\n",
       " [72794504205, 72782594239, 22, 21, 375.88513957717953],\n",
       " [72782594239, 72792394225, 21, 23, 469.86938496204897],\n",
       " [72792394225, 72782594239, 23, 21, 469.86938496204897],\n",
       " [72782594239, 72784524163, 21, 24, 296.22180486691445],\n",
       " [72784524163, 72782594239, 24, 21, 296.22180486691445],\n",
       " [72782594239, 72792024227, 21, 25, 379.7335338263519],\n",
       " [72792024227, 72782594239, 25, 21, 379.7335338263519],\n",
       " [72782594239, 72785694176, 21, 26, 308.0303254221726],\n",
       " [72785694176, 72782594239, 26, 21, 308.0303254221726],\n",
       " [72794504205, 72792394225, 22, 23, 191.05105434406818],\n",
       " [72792394225, 72794504205, 23, 22, 191.05105434406818],\n",
       " [72794504205, 72784524163, 22, 24, 321.52952520777865],\n",
       " [72784524163, 72794504205, 24, 22, 321.52952520777865],\n",
       " [72794504205, 72792024227, 22, 25, 144.61575947894278],\n",
       " [72792024227, 72794504205, 25, 22, 144.61575947894278],\n",
       " [72794504205, 72785694176, 22, 26, 662.014427725483],\n",
       " [72785694176, 72794504205, 26, 22, 662.014427725483],\n",
       " [72792394225, 72784524163, 23, 24, 393.67944122771735],\n",
       " [72784524163, 72792394225, 24, 23, 393.67944122771735],\n",
       " [72792394225, 72792024227, 23, 25, 96.11729883125135],\n",
       " [72792024227, 72792394225, 25, 23, 96.11729883125135],\n",
       " [72792394225, 72785694176, 23, 26, 776.1835969743106],\n",
       " [72785694176, 72792394225, 26, 23, 776.1835969743106],\n",
       " [72784524163, 72792024227, 24, 25, 305.7627599001508],\n",
       " [72792024227, 72784524163, 25, 24, 305.7627599001508],\n",
       " [72784524163, 72785694176, 24, 26, 516.3645983655035],\n",
       " [72785694176, 72784524163, 26, 24, 516.3645983655035],\n",
       " [72792024227, 72785694176, 25, 26, 684.1746652775099],\n",
       " [72785694176, 72792024227, 26, 25, 684.1746652775099]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[395.4679457314314,\n",
       " 395.4679457314314,\n",
       " 129.15783117310457,\n",
       " 129.15783117310457,\n",
       " 342.5330553210191,\n",
       " 342.5330553210191,\n",
       " 438.10430559873856,\n",
       " 438.10430559873856,\n",
       " 432.0504921333856,\n",
       " 432.0504921333856,\n",
       " 437.11321302131137,\n",
       " 437.11321302131137,\n",
       " 369.0087247919059,\n",
       " 369.0087247919059,\n",
       " 503.7031148557305,\n",
       " 503.7031148557305,\n",
       " 455.6811328123654,\n",
       " 455.6811328123654,\n",
       " 474.59654599988056,\n",
       " 474.59654599988056,\n",
       " 348.0950737502581,\n",
       " 348.0950737502581,\n",
       " 128.6980383337319,\n",
       " 128.6980383337319,\n",
       " 835.9738346074198,\n",
       " 835.9738346074198,\n",
       " 171.59537540199554,\n",
       " 171.59537540199554,\n",
       " 384.56540064951065,\n",
       " 384.56540064951065,\n",
       " 373.58764051749137,\n",
       " 373.58764051749137,\n",
       " 362.1843030623904,\n",
       " 362.1843030623904,\n",
       " 32.53657446575969,\n",
       " 32.53657446575969,\n",
       " 306.4908073667999,\n",
       " 306.4908073667999,\n",
       " 404.50991220772727,\n",
       " 404.50991220772727,\n",
       " 52.81240627430448,\n",
       " 52.81240627430448,\n",
       " 404.8085057270694,\n",
       " 404.8085057270694,\n",
       " 505.4288197147213,\n",
       " 505.4288197147213,\n",
       " 286.0836462058196,\n",
       " 286.0836462058196,\n",
       " 412.6227567328569,\n",
       " 412.6227567328569,\n",
       " 272.19719043646785,\n",
       " 272.19719043646785,\n",
       " 390.14029117618804,\n",
       " 390.14029117618804,\n",
       " 727.6917541369755,\n",
       " 727.6917541369755,\n",
       " 109.85904921372176,\n",
       " 109.85904921372176,\n",
       " 798.995937782049,\n",
       " 798.995937782049,\n",
       " 802.4581375032445,\n",
       " 802.4581375032445,\n",
       " 33.690956835057186,\n",
       " 33.690956835057186,\n",
       " 865.1737430163511,\n",
       " 865.1737430163511,\n",
       " 825.6499844120128,\n",
       " 825.6499844120128,\n",
       " 860.6728795779842,\n",
       " 860.6728795779842,\n",
       " 724.9963923066945,\n",
       " 724.9963923066945,\n",
       " 495.92270395996894,\n",
       " 495.92270395996894,\n",
       " 535.6266022869553,\n",
       " 535.6266022869553,\n",
       " 314.54161595295784,\n",
       " 314.54161595295784,\n",
       " 763.6141112163369,\n",
       " 763.6141112163369,\n",
       " 759.3841400612968,\n",
       " 759.3841400612968,\n",
       " 748.5486591666476,\n",
       " 748.5486591666476,\n",
       " 416.12981588895354,\n",
       " 416.12981588895354,\n",
       " 678.7815012654146,\n",
       " 678.7815012654146,\n",
       " 784.7770299622911,\n",
       " 784.7770299622911,\n",
       " 419.79175595055284,\n",
       " 419.79175595055284,\n",
       " 786.18443118649,\n",
       " 786.18443118649,\n",
       " 886.2658783050533,\n",
       " 886.2658783050533,\n",
       " 655.5895444605051,\n",
       " 655.5895444605051,\n",
       " 798.318691116012,\n",
       " 798.318691116012,\n",
       " 157.5431429141368,\n",
       " 157.5431429141368,\n",
       " 370.42190175405364,\n",
       " 370.42190175405364,\n",
       " 460.5219705003162,\n",
       " 460.5219705003162,\n",
       " 432.51979230044606,\n",
       " 432.51979230044606,\n",
       " 417.3828379309705,\n",
       " 417.3828379309705,\n",
       " 365.04041085309774,\n",
       " 365.04041085309774,\n",
       " 508.89691489063597,\n",
       " 508.89691489063597,\n",
       " 445.736559884783,\n",
       " 445.736559884783,\n",
       " 534.5520542530354,\n",
       " 534.5520542530354,\n",
       " 372.6745336577991,\n",
       " 372.6745336577991,\n",
       " 236.9285970548754,\n",
       " 236.9285970548754,\n",
       " 831.0617187024466,\n",
       " 831.0617187024466,\n",
       " 221.69557651346355,\n",
       " 221.69557651346355,\n",
       " 470.9577296531585,\n",
       " 470.9577296531585,\n",
       " 406.24280451088293,\n",
       " 406.24280451088293,\n",
       " 399.71114545506987,\n",
       " 399.71114545506987,\n",
       " 147.68778122386612,\n",
       " 147.68778122386612,\n",
       " 313.8982068438734,\n",
       " 313.8982068438734,\n",
       " 435.3656912848407,\n",
       " 435.3656912848407,\n",
       " 130.20708355028395,\n",
       " 130.20708355028395,\n",
       " 406.9699486305622,\n",
       " 406.9699486305622,\n",
       " 539.0132078425393,\n",
       " 539.0132078425393,\n",
       " 368.91523621650714,\n",
       " 368.91523621650714,\n",
       " 452.3832910640423,\n",
       " 452.3832910640423,\n",
       " 270.2281521689557,\n",
       " 270.2281521689557,\n",
       " 775.0588941479449,\n",
       " 775.0588941479449,\n",
       " 120.26611512741678,\n",
       " 120.26611512741678,\n",
       " 165.52848353942377,\n",
       " 165.52848353942377,\n",
       " 704.0879836514051,\n",
       " 704.0879836514051,\n",
       " 185.28412975619443,\n",
       " 185.28412975619443,\n",
       " 152.43166309928537,\n",
       " 152.43166309928537,\n",
       " 187.27787071696568,\n",
       " 187.27787071696568,\n",
       " 41.159846544112625,\n",
       " 41.159846544112625,\n",
       " 266.9618608270944,\n",
       " 266.9618608270944,\n",
       " 1096.8318771247875,\n",
       " 1096.8318771247875,\n",
       " 443.4038293955239,\n",
       " 443.4038293955239,\n",
       " 225.02216925780087,\n",
       " 225.02216925780087,\n",
       " 36.85266364881154,\n",
       " 36.85266364881154,\n",
       " 37.9895765777111,\n",
       " 37.9895765777111,\n",
       " 333.6917238582668,\n",
       " 333.6917238582668,\n",
       " 76.27723241526142,\n",
       " 76.27723241526142,\n",
       " 72.18185980856553,\n",
       " 72.18185980856553,\n",
       " 309.0749255571238,\n",
       " 309.0749255571238,\n",
       " 107.09860018045799,\n",
       " 107.09860018045799,\n",
       " 171.23674671831446,\n",
       " 171.23674671831446,\n",
       " 276.0731537992283,\n",
       " 276.0731537992283,\n",
       " 85.99186556469607,\n",
       " 85.99186556469607,\n",
       " 611.0889227645558,\n",
       " 611.0889227645558,\n",
       " 856.701667766572,\n",
       " 856.701667766572,\n",
       " 865.4792367557994,\n",
       " 865.4792367557994,\n",
       " 117.74480456011335,\n",
       " 117.74480456011335,\n",
       " 921.4842980890041,\n",
       " 921.4842980890041,\n",
       " 885.9031028312606,\n",
       " 885.9031028312606,\n",
       " 890.5893542480043,\n",
       " 890.5893542480043,\n",
       " 774.366157782777,\n",
       " 774.366157782777,\n",
       " 524.6337306289196,\n",
       " 524.6337306289196,\n",
       " 540.0809528724242,\n",
       " 540.0809528724242,\n",
       " 357.13279391570444,\n",
       " 357.13279391570444,\n",
       " 780.4774458332262,\n",
       " 780.4774458332262,\n",
       " 804.444912957628,\n",
       " 804.444912957628,\n",
       " 791.9148221844617,\n",
       " 791.9148221844617,\n",
       " 454.1201925958629,\n",
       " 454.1201925958629,\n",
       " 734.2694292819892,\n",
       " 734.2694292819892,\n",
       " 830.6418814848106,\n",
       " 830.6418814848106,\n",
       " 467.37603414879794,\n",
       " 467.37603414879794,\n",
       " 840.9110561664348,\n",
       " 840.9110561664348,\n",
       " 929.3740741921883,\n",
       " 929.3740741921883,\n",
       " 673.3913688173589,\n",
       " 673.3913688173589,\n",
       " 839.7968409183139,\n",
       " 839.7968409183139,\n",
       " 209.78499150005845,\n",
       " 209.78499150005845,\n",
       " 110.15242913456197,\n",
       " 110.15242913456197,\n",
       " 777.4585937545535,\n",
       " 777.4585937545535,\n",
       " 85.13925057078639,\n",
       " 85.13925057078639,\n",
       " 74.97130998219406,\n",
       " 74.97130998219406,\n",
       " 240.43942234567194,\n",
       " 240.43942234567194,\n",
       " 103.9757703201679,\n",
       " 103.9757703201679,\n",
       " 369.69695959787833,\n",
       " 369.69695959787833,\n",
       " 1139.0521959187856,\n",
       " 1139.0521959187856,\n",
       " 516.0490346912795,\n",
       " 516.0490346912795,\n",
       " 330.5216959336131,\n",
       " 330.5216959336131,\n",
       " 117.91278236100351,\n",
       " 117.91278236100351,\n",
       " 133.34207595548997,\n",
       " 133.34207595548997,\n",
       " 428.7153028579556,\n",
       " 428.7153028579556,\n",
       " 125.7282441840991,\n",
       " 125.7282441840991,\n",
       " 101.97987733596389,\n",
       " 101.97987733596389,\n",
       " 392.60154297249403,\n",
       " 392.60154297249403,\n",
       " 108.31645122446824,\n",
       " 108.31645122446824,\n",
       " 153.40108385187867,\n",
       " 153.40108385187867,\n",
       " 392.1898886399527,\n",
       " 392.1898886399527,\n",
       " 137.25092367847324,\n",
       " 137.25092367847324,\n",
       " 690.174750212649,\n",
       " 690.174750212649,\n",
       " 778.6874797868297,\n",
       " 778.6874797868297,\n",
       " 177.46961023734613,\n",
       " 177.46961023734613,\n",
       " 48.71149879169117,\n",
       " 48.71149879169117,\n",
       " 302.0846527899822,\n",
       " 302.0846527899822,\n",
       " 172.27646695285324,\n",
       " 172.27646695285324,\n",
       " 399.97805818217563,\n",
       " 399.97805818217563,\n",
       " 1176.4852153080844,\n",
       " 1176.4852153080844,\n",
       " 545.6427005439592,\n",
       " 545.6427005439592,\n",
       " 369.3977887607593,\n",
       " 369.3977887607593,\n",
       " 175.55537172802033,\n",
       " 175.55537172802033,\n",
       " 190.70446482220075,\n",
       " 190.70446482220075,\n",
       " 431.096161984872,\n",
       " 431.096161984872,\n",
       " 156.64330089805705,\n",
       " 156.64330089805705,\n",
       " 183.24573042058552,\n",
       " 183.24573042058552,\n",
       " 405.4909815288361,\n",
       " 405.4909815288361,\n",
       " 76.4859126118275,\n",
       " 76.4859126118275,\n",
       " 231.7098185991757,\n",
       " 231.7098185991757,\n",
       " 388.4359336032824,\n",
       " 388.4359336032824,\n",
       " 205.33116784821942,\n",
       " 205.33116784821942,\n",
       " 682.4260680955291,\n",
       " 682.4260680955291,\n",
       " 845.1366146343045,\n",
       " 845.1366146343045,\n",
       " 802.4776134962176,\n",
       " 802.4776134962176,\n",
       " 837.0612376685274,\n",
       " 837.0612376685274,\n",
       " 702.8044715545802,\n",
       " 702.8044715545802,\n",
       " 472.23598917705993,\n",
       " 472.23598917705993,\n",
       " 567.9966156275789,\n",
       " 567.9966156275789,\n",
       " 299.672336109386,\n",
       " 299.672336109386,\n",
       " 738.0303386253106,\n",
       " 738.0303386253106,\n",
       " 735.9632694864449,\n",
       " 735.9632694864449,\n",
       " 725.1092500944276,\n",
       " 725.1092500944276,\n",
       " 388.53428759844127,\n",
       " 388.53428759844127,\n",
       " 656.1420174916371,\n",
       " 656.1420174916371,\n",
       " 762.4710980022056,\n",
       " 762.4710980022056,\n",
       " 395.81370216605853,\n",
       " 395.81370216605853,\n",
       " 761.3237576215986,\n",
       " 761.3237576215986,\n",
       " 864.2082908552868,\n",
       " 864.2082908552868,\n",
       " 626.0309963149288,\n",
       " 626.0309963149288,\n",
       " 775.1913077729471,\n",
       " 775.1913077729471,\n",
       " 124.27275180830249,\n",
       " 124.27275180830249,\n",
       " 132.5097212349861,\n",
       " 132.5097212349861,\n",
       " 243.7073144688935,\n",
       " 243.7073144688935,\n",
       " 163.47287592761103,\n",
       " 163.47287592761103,\n",
       " 430.6615227573649,\n",
       " 430.6615227573649,\n",
       " 1177.64872286148,\n",
       " 1177.64872286148,\n",
       " 572.9402144031785,\n",
       " 572.9402144031785,\n",
       " 365.1202761234219,\n",
       " 365.1202761234219,\n",
       " 169.20923499165121,\n",
       " 169.20923499165121,\n",
       " 183.39281681071296,\n",
       " 183.39281681071296,\n",
       " 501.4584011725272,\n",
       " 501.4584011725272,\n",
       " 202.41419212127153,\n",
       " 202.41419212127153,\n",
       " 134.47678138943274,\n",
       " 134.47678138943274,\n",
       " 461.61393826175714,\n",
       " 461.61393826175714,\n",
       " 180.6112148305096,\n",
       " 180.6112148305096,\n",
       " 128.08708542091654,\n",
       " 128.08708542091654,\n",
       " 456.1447880245876,\n",
       " 456.1447880245876,\n",
       " 164.101942102309,\n",
       " 164.101942102309,\n",
       " 763.0321197403028,\n",
       " 763.0321197403028,\n",
       " 268.89465547374556,\n",
       " 268.89465547374556,\n",
       " 154.53742133091865,\n",
       " 154.53742133091865,\n",
       " 405.60664848136616,\n",
       " 405.60664848136616,\n",
       " 1187.3176896736784,\n",
       " 1187.3176896736784,\n",
       " 557.1020923327553,\n",
       " 557.1020923327553,\n",
       " 353.92282978101207,\n",
       " 353.92282978101207,\n",
       " 153.1723115228679,\n",
       " 153.1723115228679,\n",
       " 170.09372061722416,\n",
       " 170.09372061722416,\n",
       " 449.65266636740307,\n",
       " 449.65266636740307,\n",
       " 158.4233084920311,\n",
       " 158.4233084920311,\n",
       " 151.2637681156068,\n",
       " 151.2637681156068,\n",
       " 421.5198568724629,\n",
       " 421.5198568724629,\n",
       " 75.9354339763056,\n",
       " 75.9354339763056,\n",
       " 187.20830867381414,\n",
       " 187.20830867381414,\n",
       " 396.00851322484175,\n",
       " 396.00851322484175,\n",
       " 174.23493854899442,\n",
       " 174.23493854899442,\n",
       " 708.5304039928845,\n",
       " 708.5304039928845,\n",
       " 199.69966656538062,\n",
       " 199.69966656538062,\n",
       " 366.6429361022303,\n",
       " 366.6429361022303,\n",
       " 1213.3303112543872,\n",
       " 1213.3303112543872,\n",
       " 565.7742092205691,\n",
       " 565.7742092205691,\n",
       " 158.75602862100996,\n",
       " 158.75602862100996,\n",
       " 154.05814985745886,\n",
       " 154.05814985745886,\n",
       " 150.49523188191975,\n",
       " 150.49523188191975,\n",
       " 459.96707604415764,\n",
       " 459.96707604415764,\n",
       " 261.56576121699044,\n",
       " 261.56576121699044,\n",
       " 148.37325199067052,\n",
       " 148.37325199067052,\n",
       " 447.531376407682,\n",
       " 447.531376407682,\n",
       " 236.6757463314917,\n",
       " 236.6757463314917,\n",
       " 123.91187323342717,\n",
       " 123.91187323342717,\n",
       " 313.3574949442864,\n",
       " 313.3574949442864,\n",
       " 109.93432994721853,\n",
       " 109.93432994721853,\n",
       " 745.579780595948,\n",
       " 745.579780595948,\n",
       " 271.20003239227077,\n",
       " 271.20003239227077,\n",
       " 1077.0374760399814,\n",
       " 1077.0374760399814,\n",
       " 434.420916611982,\n",
       " 434.420916611982,\n",
       " 249.12813853110475,\n",
       " 249.12813853110475,\n",
       " 54.60340565924095,\n",
       " 54.60340565924095,\n",
       " 56.69074125354086,\n",
       " 56.69074125354086,\n",
       " 342.83747887822733,\n",
       " 342.83747887822733,\n",
       " 68.6532776654429,\n",
       " 68.6532776654429,\n",
       " 64.41876337172579,\n",
       " 64.41876337172579,\n",
       " 309.79708423419913,\n",
       " 309.79708423419913,\n",
       " 128.34321457181807,\n",
       " 128.34321457181807,\n",
       " 167.4998627149666,\n",
       " 167.4998627149666,\n",
       " 308.66359444651886,\n",
       " 308.66359444651886,\n",
       " 94.10375597750865,\n",
       " 94.10375597750865,\n",
       " 614.9675457923169,\n",
       " 614.9675457923169,\n",
       " 891.3237029023409,\n",
       " 891.3237029023409,\n",
       " 217.85555378560997,\n",
       " 217.85555378560997,\n",
       " 275.33106494313824,\n",
       " 275.33106494313824,\n",
       " 290.27386357894756,\n",
       " 290.27386357894756,\n",
       " 275.46064771106325,\n",
       " 275.46064771106325,\n",
       " 121.40670159689395,\n",
       " 121.40670159689395,\n",
       " 252.86361822595595,\n",
       " 252.86361822595595,\n",
       " 317.66456605180196,\n",
       " 317.66456605180196,\n",
       " 111.45482222908821,\n",
       " 111.45482222908821,\n",
       " 352.74223587837065,\n",
       " 352.74223587837065,\n",
       " 411.2440393694844,\n",
       " 411.2440393694844,\n",
       " 228.97116863460516,\n",
       " 228.97116863460516,\n",
       " 319.85603194172165,\n",
       " 319.85603194172165,\n",
       " 386.6621826263452,\n",
       " 386.6621826263452,\n",
       " 677.4867035602812,\n",
       " 677.4867035602812,\n",
       " 1150.4386824494006,\n",
       " 1150.4386824494006,\n",
       " 1122.20080799066,\n",
       " 1122.20080799066,\n",
       " 1112.0640517140919,\n",
       " 1112.0640517140919,\n",
       " 864.0517998527835,\n",
       " 864.0517998527835,\n",
       " 1044.4930844703567,\n",
       " 1044.4930844703567,\n",
       " 1130.7362905229766,\n",
       " 1130.7362905229766,\n",
       " 833.7147080759753,\n",
       " 833.7147080759753,\n",
       " 1170.6533446424978,\n",
       " 1170.6533446424978,\n",
       " 1218.574885845403,\n",
       " 1218.574885845403,\n",
       " 1103.3829323700534,\n",
       " 1103.3829323700534,\n",
       " 1152.414996794818,\n",
       " 1152.414996794818,\n",
       " 683.9489018806388,\n",
       " 683.9489018806388,\n",
       " 489.8134161401565,\n",
       " 489.8134161401565,\n",
       " 470.9676802550459,\n",
       " 470.9676802550459,\n",
       " 459.0274176052015,\n",
       " 459.0274176052015,\n",
       " 199.59384465449898,\n",
       " 199.59384465449898,\n",
       " 400.7840441174383,\n",
       " 400.7840441174383,\n",
       " 490.3741382897129,\n",
       " 490.3741382897129,\n",
       " 162.11769623669784,\n",
       " 162.11769623669784,\n",
       " 520.9373250801362,\n",
       " 520.9373250801362,\n",
       " 587.5949150283049,\n",
       " 587.5949150283049,\n",
       " 432.7632336033511,\n",
       " 432.7632336033511,\n",
       " 504.8208347435879,\n",
       " 504.8208347435879,\n",
       " 259.92185050812566,\n",
       " 259.92185050812566,\n",
       " 213.47199631168857,\n",
       " 213.47199631168857,\n",
       " 199.7906856396986,\n",
       " 199.7906856396986,\n",
       " 363.3873739871944,\n",
       " 363.3873739871944,\n",
       " 286.49766075950384,\n",
       " 286.49766075950384,\n",
       " 235.6189290359573,\n",
       " 235.6189290359573,\n",
       " 371.98475050597,\n",
       " 371.98475050597,\n",
       " 293.97835633481196,\n",
       " 293.97835633481196,\n",
       " 268.7494270590419,\n",
       " 268.7494270590419,\n",
       " 181.34059793217904,\n",
       " 181.34059793217904,\n",
       " 201.61183866011444,\n",
       " 201.61183866011444,\n",
       " 642.4950004501114,\n",
       " 642.4950004501114,\n",
       " 17.716247962574563,\n",
       " 17.716247962574563,\n",
       " 363.9821600522806,\n",
       " 363.9821600522806,\n",
       " 109.51765254130393,\n",
       " 109.51765254130393,\n",
       " 42.988933127483634,\n",
       " 42.988933127483634,\n",
       " 340.3237239231559,\n",
       " 340.3237239231559,\n",
       " 114.05670021657697,\n",
       " 114.05670021657697,\n",
       " 136.43815241862657,\n",
       " 136.43815241862657,\n",
       " 287.7450730575042,\n",
       " 287.7450730575042,\n",
       " 49.41202473657626,\n",
       " 49.41202473657626,\n",
       " 643.6682517559431,\n",
       " 643.6682517559431,\n",
       " 352.2158347034709,\n",
       " 352.2158347034709,\n",
       " 111.26188855442645,\n",
       " 111.26188855442645,\n",
       " 52.561875466910436,\n",
       " 52.561875466910436,\n",
       " 329.3368670529716,\n",
       " 329.3368670529716,\n",
       " 128.01154758942099,\n",
       " 128.01154758942099,\n",
       " 145.49531716211806,\n",
       " 145.49531716211806,\n",
       " 275.6435250697721,\n",
       " 275.6435250697721,\n",
       " 52.80087090274467,\n",
       " 52.80087090274467,\n",
       " 633.1197418989807,\n",
       " 633.1197418989807,\n",
       " 303.31516235822005,\n",
       " 303.31516235822005,\n",
       " 396.9755284116139,\n",
       " 396.9755284116139,\n",
       " 73.74026824660453,\n",
       " 73.74026824660453,\n",
       " 394.74559894207687,\n",
       " 394.74559894207687,\n",
       " 496.478460291939,\n",
       " 496.478460291939,\n",
       " 256.1592053257525,\n",
       " 256.1592053257525,\n",
       " 402.1731282608459,\n",
       " 402.1731282608459,\n",
       " 286.80868520838226,\n",
       " 286.80868520838226,\n",
       " 129.67576043022495,\n",
       " 129.67576043022495,\n",
       " 268.31498195846285,\n",
       " 268.31498195846285,\n",
       " 128.38153131965652,\n",
       " 128.38153131965652,\n",
       " 230.11767302669344,\n",
       " 230.11767302669344,\n",
       " 305.30416194483115,\n",
       " 305.30416194483115,\n",
       " 156.88223261226412,\n",
       " 156.88223261226412,\n",
       " 566.3246868369048,\n",
       " 566.3246868369048,\n",
       " 368.04423285388,\n",
       " 368.04423285388,\n",
       " 133.10933796080906,\n",
       " 133.10933796080906,\n",
       " 103.88553424589651,\n",
       " 103.88553424589651,\n",
       " 327.04970158433304,\n",
       " 327.04970158433304,\n",
       " 41.59043713208587,\n",
       " 41.59043713208587,\n",
       " 674.0342619771884,\n",
       " 674.0342619771884,\n",
       " 375.88513957717953,\n",
       " 375.88513957717953,\n",
       " 469.86938496204897,\n",
       " 469.86938496204897,\n",
       " 296.22180486691445,\n",
       " 296.22180486691445,\n",
       " 379.7335338263519,\n",
       " 379.7335338263519,\n",
       " 308.0303254221726,\n",
       " 308.0303254221726,\n",
       " 191.05105434406818,\n",
       " 191.05105434406818,\n",
       " 321.52952520777865,\n",
       " 321.52952520777865,\n",
       " 144.61575947894278,\n",
       " 144.61575947894278,\n",
       " 662.014427725483,\n",
       " 662.014427725483,\n",
       " 393.67944122771735,\n",
       " 393.67944122771735,\n",
       " 96.11729883125135,\n",
       " 96.11729883125135,\n",
       " 776.1835969743106,\n",
       " 776.1835969743106,\n",
       " 305.7627599001508,\n",
       " 305.7627599001508,\n",
       " 516.3645983655035,\n",
       " 516.3645983655035,\n",
       " 684.1746652775099,\n",
       " 684.1746652775099]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_edge_attributes(dataframes, location_datamap_df, edge_index):\n",
    "    edge_attr = []\n",
    "    edge_attr_verifications = []\n",
    "    keys = list(dataframes.keys())\n",
    "    for i, j in edge_index:\n",
    "        station_i = location_datamap_df[location_datamap_df['STATION'] == keys[i]]\n",
    "        station_j = location_datamap_df[location_datamap_df['STATION'] == keys[j]]\n",
    "        lati, loni, eli = station_i['LATITUDE'].values[0], station_i['LONGITUDE'].values[0], station_i['ELEVATION'].values[0]\n",
    "        latj, lonj, elj = station_j['LATITUDE'].values[0], station_j['LONGITUDE'].values[0], station_j['ELEVATION'].values[0]\n",
    "        edge_attr_verifications.append(([keys[i], keys[j], i, j, haversine_distance(lati, loni, latj, lonj, eli, elj)]))\n",
    "        edge_attr.append(haversine_distance(lati, loni, latj, lonj, eli, elj))\n",
    "    display(edge_attr_verifications)\n",
    "    return edge_attr\n",
    "\n",
    "edge_attr = create_edge_attributes(dataframes, location_datamap_df, edge_index)\n",
    "display(edge_attr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 702])\n",
      "tensor([[ 0,  1,  0,  ..., 26, 25, 26],\n",
      "        [ 1,  0,  2,  ..., 24, 26, 25]])\n"
     ]
    }
   ],
   "source": [
    "# Convert edge_index to a torch tensor\n",
    "edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "\n",
    "# Print the shape to verify\n",
    "print(edge_index.shape)\n",
    "print(edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([702])\n",
      "tensor([0.3146, 0.3146, 0.0928, 0.0928, 0.2705, 0.2705, 0.3501, 0.3501, 0.3450,\n",
      "        0.3450, 0.3492, 0.3492, 0.2925, 0.2925, 0.4047, 0.4047, 0.3647, 0.3647,\n",
      "        0.3805, 0.3805, 0.2751, 0.2751, 0.0924, 0.0924, 0.6814, 0.6814, 0.1281,\n",
      "        0.1281, 0.3055, 0.3055, 0.2963, 0.2963, 0.2869, 0.2869, 0.0123, 0.0123,\n",
      "        0.2405, 0.2405, 0.3221, 0.3221, 0.0292, 0.0292, 0.3223, 0.3223, 0.4061,\n",
      "        0.4061, 0.2235, 0.2235, 0.3289, 0.3289, 0.2119, 0.2119, 0.3101, 0.3101,\n",
      "        0.5912, 0.5912, 0.0767, 0.0767, 0.6506, 0.6506, 0.6535, 0.6535, 0.0133,\n",
      "        0.0133, 0.7057, 0.7057, 0.6728, 0.6728, 0.7020, 0.7020, 0.5890, 0.5890,\n",
      "        0.3982, 0.3982, 0.4313, 0.4313, 0.2472, 0.2472, 0.6211, 0.6211, 0.6176,\n",
      "        0.6176, 0.6086, 0.6086, 0.3318, 0.3318, 0.5505, 0.5505, 0.6388, 0.6388,\n",
      "        0.3348, 0.3348, 0.6399, 0.6399, 0.7233, 0.7233, 0.5312, 0.5312, 0.6500,\n",
      "        0.6500, 0.1164, 0.1164, 0.2937, 0.2937, 0.3687, 0.3687, 0.3454, 0.3454,\n",
      "        0.3328, 0.3328, 0.2892, 0.2892, 0.4090, 0.4090, 0.3564, 0.3564, 0.4304,\n",
      "        0.4304, 0.2956, 0.2956, 0.1825, 0.1825, 0.6773, 0.6773, 0.1699, 0.1699,\n",
      "        0.3774, 0.3774, 0.3235, 0.3235, 0.3181, 0.3181, 0.1082, 0.1082, 0.2466,\n",
      "        0.2466, 0.3478, 0.3478, 0.0937, 0.0937, 0.3241, 0.3241, 0.4341, 0.4341,\n",
      "        0.2925, 0.2925, 0.3620, 0.3620, 0.2103, 0.2103, 0.6307, 0.6307, 0.0854,\n",
      "        0.0854, 0.1231, 0.1231, 0.5716, 0.5716, 0.1395, 0.1395, 0.1122, 0.1122,\n",
      "        0.1412, 0.1412, 0.0195, 0.0195, 0.2076, 0.2076, 0.8986, 0.8986, 0.3545,\n",
      "        0.3545, 0.1726, 0.1726, 0.0159, 0.0159, 0.0169, 0.0169, 0.2631, 0.2631,\n",
      "        0.0488, 0.0488, 0.0454, 0.0454, 0.2426, 0.2426, 0.0744, 0.0744, 0.1278,\n",
      "        0.1278, 0.2151, 0.2151, 0.0569, 0.0569, 0.4941, 0.4941, 0.6987, 0.6987,\n",
      "        0.7060, 0.7060, 0.0833, 0.0833, 0.7526, 0.7526, 0.7230, 0.7230, 0.7269,\n",
      "        0.7269, 0.6301, 0.6301, 0.4221, 0.4221, 0.4350, 0.4350, 0.2826, 0.2826,\n",
      "        0.6352, 0.6352, 0.6551, 0.6551, 0.6447, 0.6447, 0.3634, 0.3634, 0.5967,\n",
      "        0.5967, 0.6770, 0.6770, 0.3744, 0.3744, 0.6855, 0.6855, 0.7592, 0.7592,\n",
      "        0.5460, 0.5460, 0.6846, 0.6846, 0.1599, 0.1599, 0.0770, 0.0770, 0.6327,\n",
      "        0.6327, 0.0561, 0.0561, 0.0477, 0.0477, 0.1855, 0.1855, 0.0718, 0.0718,\n",
      "        0.2931, 0.2931, 0.9338, 0.9338, 0.4150, 0.4150, 0.2605, 0.2605, 0.0834,\n",
      "        0.0834, 0.0963, 0.0963, 0.3423, 0.3423, 0.0899, 0.0899, 0.0702, 0.0702,\n",
      "        0.3122, 0.3122, 0.0754, 0.0754, 0.1130, 0.1130, 0.3118, 0.3118, 0.0995,\n",
      "        0.0995, 0.5600, 0.5600, 0.6337, 0.6337, 0.1330, 0.1330, 0.0258, 0.0258,\n",
      "        0.2368, 0.2368, 0.1287, 0.1287, 0.3183, 0.3183, 0.9650, 0.9650, 0.4396,\n",
      "        0.4396, 0.2929, 0.2929, 0.1314, 0.1314, 0.1441, 0.1441, 0.3442, 0.3442,\n",
      "        0.1157, 0.1157, 0.1378, 0.1378, 0.3229, 0.3229, 0.0489, 0.0489, 0.1782,\n",
      "        0.1782, 0.3087, 0.3087, 0.1562, 0.1562, 0.5535, 0.5535, 0.6890, 0.6890,\n",
      "        0.6535, 0.6535, 0.6823, 0.6823, 0.5705, 0.5705, 0.3785, 0.3785, 0.4582,\n",
      "        0.4582, 0.2348, 0.2348, 0.5998, 0.5998, 0.5981, 0.5981, 0.5891, 0.5891,\n",
      "        0.3088, 0.3088, 0.5316, 0.5316, 0.6202, 0.6202, 0.3149, 0.3149, 0.6192,\n",
      "        0.6192, 0.7049, 0.7049, 0.5066, 0.5066, 0.6308, 0.6308, 0.0887, 0.0887,\n",
      "        0.0956, 0.0956, 0.1882, 0.1882, 0.1214, 0.1214, 0.3439, 0.3439, 0.9659,\n",
      "        0.9659, 0.4624, 0.4624, 0.2893, 0.2893, 0.1262, 0.1262, 0.1380, 0.1380,\n",
      "        0.4028, 0.4028, 0.1538, 0.1538, 0.0972, 0.0972, 0.3697, 0.3697, 0.1356,\n",
      "        0.1356, 0.0919, 0.0919, 0.3651, 0.3651, 0.1219, 0.1219, 0.6207, 0.6207,\n",
      "        0.2092, 0.2092, 0.1139, 0.1139, 0.3230, 0.3230, 0.9740, 0.9740, 0.4492,\n",
      "        0.4492, 0.2800, 0.2800, 0.1128, 0.1128, 0.1269, 0.1269, 0.3597, 0.3597,\n",
      "        0.1172, 0.1172, 0.1112, 0.1112, 0.3363, 0.3363, 0.0485, 0.0485, 0.1411,\n",
      "        0.1411, 0.3150, 0.3150, 0.1303, 0.1303, 0.5753, 0.5753, 0.1515, 0.1515,\n",
      "        0.2906, 0.2906, 0.9956, 0.9956, 0.4564, 0.4564, 0.1174, 0.1174, 0.1135,\n",
      "        0.1135, 0.1106, 0.1106, 0.3683, 0.3683, 0.2031, 0.2031, 0.1088, 0.1088,\n",
      "        0.3579, 0.3579, 0.1823, 0.1823, 0.0884, 0.0884, 0.2462, 0.2462, 0.0768,\n",
      "        0.0768, 0.6061, 0.6061, 0.2111, 0.2111, 0.8821, 0.8821, 0.3470, 0.3470,\n",
      "        0.1927, 0.1927, 0.0307, 0.0307, 0.0325, 0.0325, 0.2707, 0.2707, 0.0424,\n",
      "        0.0424, 0.0389, 0.0389, 0.2432, 0.2432, 0.0921, 0.0921, 0.1247, 0.1247,\n",
      "        0.2423, 0.2423, 0.0636, 0.0636, 0.4974, 0.4974, 0.7275, 0.7275, 0.1667,\n",
      "        0.1667, 0.2145, 0.2145, 0.2270, 0.2270, 0.2146, 0.2146, 0.0863, 0.0863,\n",
      "        0.1958, 0.1958, 0.2498, 0.2498, 0.0781, 0.0781, 0.2790, 0.2790, 0.3277,\n",
      "        0.3277, 0.1759, 0.1759, 0.2516, 0.2516, 0.3072, 0.3072, 0.5494, 0.5494,\n",
      "        0.9433, 0.9433, 0.9197, 0.9197, 0.9113, 0.9113, 0.7048, 0.7048, 0.8550,\n",
      "        0.8550, 0.9269, 0.9269, 0.6795, 0.6795, 0.9601, 0.9601, 1.0000, 1.0000,\n",
      "        0.9041, 0.9041, 0.9449, 0.9449, 0.5548, 0.5548, 0.3931, 0.3931, 0.3774,\n",
      "        0.3774, 0.3675, 0.3675, 0.1515, 0.1515, 0.3190, 0.3190, 0.3936, 0.3936,\n",
      "        0.1202, 0.1202, 0.4191, 0.4191, 0.4746, 0.4746, 0.3456, 0.3456, 0.4056,\n",
      "        0.4056, 0.2017, 0.2017, 0.1630, 0.1630, 0.1516, 0.1516, 0.2879, 0.2879,\n",
      "        0.2238, 0.2238, 0.1815, 0.1815, 0.2950, 0.2950, 0.2301, 0.2301, 0.2090,\n",
      "        0.2090, 0.1363, 0.1363, 0.1531, 0.1531, 0.5203, 0.5203, 0.0000, 0.0000,\n",
      "        0.2883, 0.2883, 0.0764, 0.0764, 0.0210, 0.0210, 0.2686, 0.2686, 0.0802,\n",
      "        0.0802, 0.0989, 0.0989, 0.2249, 0.2249, 0.0264, 0.0264, 0.5213, 0.5213,\n",
      "        0.2786, 0.2786, 0.0779, 0.0779, 0.0290, 0.0290, 0.2595, 0.2595, 0.0918,\n",
      "        0.0918, 0.1064, 0.1064, 0.2148, 0.2148, 0.0292, 0.0292, 0.5125, 0.5125,\n",
      "        0.2378, 0.2378, 0.3158, 0.3158, 0.0467, 0.0467, 0.3140, 0.3140, 0.3987,\n",
      "        0.3987, 0.1986, 0.1986, 0.3202, 0.3202, 0.2241, 0.2241, 0.0932, 0.0932,\n",
      "        0.2087, 0.2087, 0.0922, 0.0922, 0.1769, 0.1769, 0.2395, 0.2395, 0.1159,\n",
      "        0.1159, 0.4568, 0.4568, 0.2917, 0.2917, 0.0961, 0.0961, 0.0718, 0.0718,\n",
      "        0.2576, 0.2576, 0.0199, 0.0199, 0.5465, 0.5465, 0.2983, 0.2983, 0.3765,\n",
      "        0.3765, 0.2319, 0.2319, 0.3015, 0.3015, 0.2418, 0.2418, 0.1443, 0.1443,\n",
      "        0.2530, 0.2530, 0.1057, 0.1057, 0.5365, 0.5365, 0.3131, 0.3131, 0.0653,\n",
      "        0.0653, 0.6316, 0.6316, 0.2399, 0.2399, 0.4152, 0.4152, 0.5550, 0.5550])\n"
     ]
    }
   ],
   "source": [
    "# Convert edge_attr to a torch tensor\n",
    "edge_attr_tensor = torch.tensor(edge_attr, dtype=torch.float)\n",
    "\n",
    "# Min-Max normalization to scale the values to the range [0, 1]\n",
    "edge_attr_min = edge_attr_tensor.min()\n",
    "edge_attr_max = edge_attr_tensor.max()\n",
    "edge_attr = (edge_attr_tensor - edge_attr_min) / (edge_attr_max - edge_attr_min)\n",
    "\n",
    "# Print the normalized edge_attr tensor to verify\n",
    "print(edge_attr.shape)\n",
    "print(edge_attr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">**ZE EPIC GPU IMPORT**</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move data to device\n",
    "edge_index = edge_index.to(device)\n",
    "edge_attr = edge_attr.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">**ZE EPIC BATCHING**</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 176\n",
      "Number of full batches: 1408\n",
      "Batched input train dimensionality: torch.Size([8, 176, 27, 69])\n",
      "Batched output train dimensionality: torch.Size([8, 176, 27, 67])\n",
      "Batched input train dimensionality: torch.Size([176, 27, 69])\n",
      "Batched output train dimensionality: torch.Size([176, 27, 67])\n"
     ]
    }
   ],
   "source": [
    "batch_size = node_features_sequence_input_train.shape[0] // 8 # Adjust this value based on your GPU memory capacity\n",
    "# Calculate the number of full batches\n",
    "num_full_batches_train = (node_features_sequence_input_train.size(0) // batch_size) * batch_size\n",
    "\n",
    "# Trim the input and output tensors to have a size divisible by the batch size\n",
    "trimmed_input_train = node_features_sequence_input_train[:num_full_batches_train]\n",
    "trimmed_output_train = node_features_sequence_output_train[:num_full_batches_train]\n",
    "\n",
    "# Create batches of data\n",
    "batched_input_train = trimmed_input_train.view(-1, batch_size, trimmed_input_train.size(1), trimmed_input_train.size(2))\n",
    "batched_output_train = trimmed_output_train.view(-1, batch_size, trimmed_output_train.size(1), trimmed_output_train.size(2))\n",
    "\n",
    "# Adjust the number of batches\n",
    "num_batches_train = batched_input_train.size(0)\n",
    "\n",
    "# Print the batch size and number of full batches to verify\n",
    "print(\"Batch size:\", batch_size)\n",
    "print(\"Number of full batches:\", num_full_batches_train)\n",
    "\n",
    "print(\"Batched input train dimensionality:\", batched_input_train.shape)\n",
    "print(\"Batched output train dimensionality:\", batched_output_train.shape)\n",
    "print(\"Batched input train dimensionality:\", batched_input_train[0].shape)\n",
    "print(\"Batched output train dimensionality:\", batched_output_train[0].shape)\n",
    "\n",
    "# Check for NaN or Inf in input data\n",
    "assert not torch.isnan(batched_input_train).any(), \"NaN values found in batched_input_train\"\n",
    "assert not torch.isinf(batched_input_train).any(), \"Inf values found in batched_input_train\"\n",
    "\n",
    "# Check for NaN or Inf in output data\n",
    "assert not torch.isnan(batched_output_train).any(), \"NaN values found in batched_output_train\"\n",
    "assert not torch.isinf(batched_output_train).any(), \"Inf values found in batched_output_train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 88\n",
      "Number of full batches: 1408\n",
      "Batched input test dimensionality: torch.Size([4, 88, 27, 69])\n",
      "Batched output test dimensionality: torch.Size([4, 88, 27, 67])\n"
     ]
    }
   ],
   "source": [
    "batch_size = node_features_sequence_input_test.shape[0] // 4# Adjust this value based on your GPU memory capacity\n",
    "# Calculate the number of full batches\n",
    "num_full_batches_test = (node_features_sequence_input_test.size(0) // batch_size) * batch_size\n",
    "\n",
    "# Trim the input and output tensors to have a size divisible by the batch size\n",
    "trimmed_input_test = node_features_sequence_input_test[:num_full_batches_test]\n",
    "trimmed_output_test = node_features_sequence_output_test[:num_full_batches_test]\n",
    "\n",
    "# Create batches of data\n",
    "batched_input_test = trimmed_input_test.view(-1, batch_size, trimmed_input_test.size(1), trimmed_input_test.size(2))\n",
    "batched_output_test = trimmed_output_test.view(-1, batch_size, trimmed_output_test.size(1), trimmed_output_test.size(2))\n",
    "\n",
    "# Adjust the number of batches\n",
    "num_batches_test = batched_input_test.size(0)\n",
    "print(\"Batch size:\", batch_size)\n",
    "print(\"Number of full batches:\", num_full_batches_train)\n",
    "\n",
    "print(\"Batched input test dimensionality:\", batched_input_test.shape)\n",
    "print(\"Batched output test dimensionality:\", batched_output_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">**ZE EPIC MODEL**</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer model for the hybrid model\n",
    "# Input Dimension: [batch_length ,num_nodes, num_features]\n",
    "# Output Dimension: [batch_length, num_nodes, hidden_channels]\n",
    "class TransformerModule(nn.Module):\n",
    "    def __init__(self, features_channels, out_channels, transformer_layers):\n",
    "        super(TransformerModule, self).__init__()\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=features_channels, nhead=features_channels, batch_first=True),\n",
    "            num_layers=transformer_layers\n",
    "        )\n",
    "        self.output_linear = nn.Linear(features_channels, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        temporal_outputs = self.transformer_encoder(x)  # Shape: [num_timesteps, num_nodes, hidden_channels]\n",
    "        x = self.output_linear(temporal_outputs)  # Shape: [num_timesteps, num_nodes, out_channels]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNN module for the hybrid model\n",
    "# Input Dimension: [batch_length ,num_nodes, num_features]\n",
    "# Output Dimension: [batch_length, num_nodes, hidden_channels]\n",
    "class GNNModule(torch.nn.Module):\n",
    "    def __init__(self, features_channels, hidden_channels, edge_in_channels):\n",
    "        super(GNNModule, self).__init__()\n",
    "        self.conv1 = GATv2Conv(features_channels, hidden_channels, edge_dim=edge_in_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GraphConv(hidden_channels, hidden_channels)\n",
    "        self.batch_norm1 = BatchNorm(hidden_channels)\n",
    "        self.batch_norm2 = BatchNorm(hidden_channels)\n",
    "        self.batch_norm3 = BatchNorm(hidden_channels)\n",
    "        self.dropout = torch.nn.Dropout(p=0.5)\n",
    "\n",
    "    def gnn_forward(self, x, edge_index, edge_attr):\n",
    "\n",
    "        # print(\"input\",x.shape)\n",
    "        # print(torch.isnan(x).any())\n",
    "        # print(torch.isinf(x).any())\n",
    "\n",
    "        edge_weight = 1.0 / (edge_attr + 1e-6)\n",
    "        x1 = self.conv1(x, edge_index, edge_attr=edge_weight)\n",
    "        x1 = self.batch_norm1(x1)\n",
    "        x1 = F.relu(x1)\n",
    "        x1 = self.dropout(x1)\n",
    "\n",
    "        # print(\"Gat\",x1.shape)\n",
    "        # print(torch.isnan(x1).any())\n",
    "        # print(torch.isinf(x1).any())\n",
    "\n",
    "        x2 = self.conv2(x1, edge_index, edge_weight=edge_attr)\n",
    "        x2 = self.batch_norm2(x2)\n",
    "        x2 = F.relu(x2)        \n",
    "        x2 = self.dropout(x2)\n",
    "\n",
    "        # print(\"gnc\",x2.shape)\n",
    "        # print(torch.isnan(x2).any())\n",
    "        # print(torch.isinf(x2).any())\n",
    "\n",
    "        x3 = self.conv3(x2, edge_index)\n",
    "        x3 = self.batch_norm3(x3)\n",
    "        x3 = F.relu(x3)\n",
    "        x3 = self.dropout(x3)\n",
    "\n",
    "        # print(\"graph\",x3.shape)\n",
    "        # print(torch.isnan(x3).any())\n",
    "        # print(torch.isinf(x3).any())\n",
    "\n",
    "        x = x1 + x2 + x3  # Residual connection\n",
    "\n",
    "        # print(\"residual\",x.shape)\n",
    "        # print(torch.isnan(x).any())\n",
    "        # print(torch.isinf(x).any())\n",
    "\n",
    "        return x  # Shape: [num_nodes, hidden_channels]\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        spatial_outputs = []\n",
    "        for t in range(x.size(0)):\n",
    "            x_t = self.gnn_forward(x[t], edge_index, edge_attr)\n",
    "            spatial_outputs.append(x_t)\n",
    "\n",
    "        x = torch.stack(spatial_outputs, dim=0)  # Shape: [num_timesteps, num_nodes, hidden_channels]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid model combining the Transformer and GNN modules\n",
    "# Input Dimension: [batch_length ,num_nodes, num_features]\n",
    "# Output Dimension: [batch_length, num_nodes, out_channels]\n",
    "class HybridModel_Transformer(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, features_channels, edge_in_channels, hidden_channels, transformer_layers):\n",
    "        super(HybridModel_Transformer, self).__init__()\n",
    "        self.transformer = TransformerModule(features_channels, hidden_channels, transformer_layers)\n",
    "        self.gnn = GNNModule(hidden_channels, hidden_channels, edge_in_channels)\n",
    "        self.output_layer = torch.nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # Project the input features to the hidden dimension\n",
    "        # Transform the input features using the transformer\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # print(\"transformer forward\",x.shape)\n",
    "        # print(torch.isnan(x).any())\n",
    "        # print(torch.isinf(x).any())\n",
    "\n",
    "        # Pass the transformed features to the GNN\n",
    "        x = self.gnn(x, edge_index, edge_attr)\n",
    "\n",
    "        # print(\"gnn\",x.shape)\n",
    "        # print(torch.isnan(x).any())\n",
    "        # print(torch.isinf(x).any())\n",
    "\n",
    "        # Pass the GNN outputs through a linear layer\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridModel_GNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, features_channels, edge_in_channels, hidden_channels, transformer_layers):\n",
    "        super(HybridModel_GNN, self).__init__()\n",
    "        self.gnn = GNNModule(features_channels, hidden_channels, edge_in_channels)\n",
    "        self.transformer = TransformerModule(hidden_channels, out_channels, transformer_layers)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # Apply GNN first\n",
    "        x = self.gnn(x, edge_index, edge_attr)\n",
    "\n",
    "        # Apply transformer\n",
    "        x = self.transformer(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridModel_Parallel(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, features_channels, edge_in_channels, hidden_channels, transformer_layers):\n",
    "        super(HybridModel_Parallel, self).__init__()\n",
    "        self.gnn = GNNModule(features_channels, hidden_channels, edge_in_channels)\n",
    "        self.transformer = TransformerModule(features_channels, hidden_channels, transformer_layers)\n",
    "        self.output_layer = torch.nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # Project the input features to the hidden dimension\n",
    "        # Transform the input features using the transformer\n",
    "        x_gnn = self.gnn(x, edge_index, edge_attr)\n",
    "        x_transformer = self.transformer(x)\n",
    "\n",
    "        # Pass the transformed features to the GNN\n",
    "        x = x_gnn + x_transformer\n",
    "\n",
    "        # Pass the GNN outputs through a linear layer\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleModel_Transformer(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, features_channels, edge_in_channels, hidden_channels, transformer_layers):\n",
    "        super(SingleModel_Transformer, self).__init__()\n",
    "        self.transformer = TransformerModule(features_channels, out_channels, transformer_layers)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # Project the input features to the hidden dimension\n",
    "        # Transform the input features using the transformer\n",
    "        x = self.transformer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleModel_GNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, features_channels, edge_in_channels, hidden_channels, transformer_layers):\n",
    "        super(SingleModel_GNN, self).__init__()\n",
    "        self.gnn = GNNModule(features_channels, hidden_channels, edge_in_channels)\n",
    "        self.output_layer = torch.nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # Apply GNN first\n",
    "        x = self.gnn(x, edge_index, edge_attr)\n",
    "\n",
    "        # Apply transformer\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">**ZE EPIC MODEL TRAIN**</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, scheduler, loss_fn, num_epochs, num_batches_train, batched_input_train, batched_output_train, device):\n",
    "    epoch_losses = []\n",
    "    epoch_maes = []\n",
    "\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        total_mae = 0\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for b in range(num_batches_train):\n",
    "            # Get the batched node features and desired output\n",
    "            node_features_batch = batched_input_train[b].to(device)\n",
    "            desired_output_batch = batched_output_train[b].to(device)\n",
    "\n",
    "            # print(\"Node features batch: \", node_features_batch.shape)\n",
    "            # print(torch.isnan(node_features_batch).any())\n",
    "            # print(torch.isinf(node_features_batch).any())\n",
    "            # Forward pass with batch_size parameter\n",
    "\n",
    "            model_output_batch = model(node_features_batch, edge_index, edge_attr)\n",
    "            \n",
    "            # print(\"Model be outputing: \", model_output_batch.shape)\n",
    "            # print(torch.isnan(model_output_batch).any())\n",
    "            # print(torch.isinf(model_output_batch).any())\n",
    "\n",
    "            # print(\"Desired output: \", desired_output_batch.shape)\n",
    "            # print(torch.isnan(desired_output_batch).any())\n",
    "            # print(torch.isinf(desired_output_batch).any())\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn(model_output_batch, desired_output_batch)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update the optimizer\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Compute MAE for debugging\n",
    "            mae = torch.mean(torch.abs(model_output_batch - desired_output_batch))\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_mae += mae.item()\n",
    "\n",
    "        average_loss = total_loss / num_batches_train\n",
    "        average_mae = total_mae / num_batches_train\n",
    "        epoch_losses.append(average_loss)\n",
    "        epoch_maes.append(average_mae)\n",
    "\n",
    "        scheduler.step(average_loss)\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        current_patience = scheduler.num_bad_epochs\n",
    "        print(f\"Epoch {epoch+1}, Average Loss: {average_loss}, Average MAE: {average_mae}, Learning Rate: {current_lr}, Current Patience: {current_patience}\")\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return epoch_losses, epoch_maes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of epochs: 200\n",
      "Learning rate: 0.01\n",
      "Scheduler mode: min\n",
      "Scheduler factor: 0.8\n",
      "Scheduler patience: 5\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters\n",
    "num_epochs = 200  # Adjust the number of epochs as needed\n",
    "learning_rate = 0.01\n",
    "scheduler_mode = 'min'\n",
    "scheduler_factor = 0.8\n",
    "scheduler_patience = 5\n",
    "\n",
    "#Print all parameter \n",
    "print(\"Number of epochs:\", num_epochs)\n",
    "print(\"Learning rate:\", learning_rate)\n",
    "print(\"Scheduler mode:\", scheduler_mode)\n",
    "print(\"Scheduler factor:\", scheduler_factor)\n",
    "print(\"Scheduler patience:\", scheduler_patience)\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Train Toggle\n",
    "new_iteration = True\n",
    "\n",
    "run_model_parallel = True\n",
    "run_model_transformer_gnn = True\n",
    "run_model_gnn_transformer = True\n",
    "run_model_single_transformer = True\n",
    "run_model_single_gnn = True\n",
    "run_plotting = True\n",
    "\n",
    "hidden_channels = 1024\n",
    "transformer_layers = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def save_model_data(model, optimizer, scheduler, epoch_losses, epoch_maes, hidden_channels, transformer_layers, new_iteration, model_name):\n",
    "    # Create the directory if it doesn't exist\n",
    "    current_time = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    current_date = datetime.now().strftime('%Y%m%d')\n",
    "    parent_dir = f'saved_models{current_date}'\n",
    "    # Find the number of existing folders with the prefix\n",
    "    existing_folders = [d for d in os.listdir('.') if d.startswith(f'saved_models_{current_date}')]\n",
    "    folder_num = len(existing_folders)\n",
    "    if (new_iteration):\n",
    "        folder_num = folder_num + 1\n",
    "        new_iteration = False\n",
    "    folder_path = f'saved_models_{current_date}_3/{model_name}_{current_time}'\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    # Save the model, optimizer, and scheduler states\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'hidden_channels': hidden_channels,\n",
    "        'transformer_layers': transformer_layers\n",
    "    }, os.path.join(folder_path, f'{model_name}.pt'))\n",
    "\n",
    "    # Save epoch losses and MAEs to a CSV file\n",
    "    df = pd.DataFrame({\n",
    "        'epoch': range(1, len(epoch_losses) + 1),\n",
    "        'loss': epoch_losses,\n",
    "        'mae': epoch_maes\n",
    "    })\n",
    "    df.to_csv(os.path.join(folder_path, f'{model_name}_metrics.csv'), index=False)\n",
    "    print(f\"Model data saved in {folder_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Channels: 27\n",
      "Features Channels: 69\n",
      "Output Channels: 67\n",
      "Edge Input Channels: 1\n",
      "Hidden Channels: 1024\n",
      "Transformer Layers: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/HDD/RyanFolder/projects/Weather-Predition-Project/.venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(\n",
      "/mnt/HDD/RyanFolder/projects/Weather-Predition-Project/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 97.2294774055481, Average MAE: 6.05114221572876, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 2, Average Loss: 28.19705581665039, Average MAE: 3.301598012447357, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 3, Average Loss: 20.608396530151367, Average MAE: 2.1070550233125687, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 4, Average Loss: 16.793187499046326, Average MAE: 1.8312254399061203, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 5, Average Loss: 14.940422654151917, Average MAE: 1.5427388399839401, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 6, Average Loss: 13.894131660461426, Average MAE: 1.3755286186933517, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 7, Average Loss: 13.36318850517273, Average MAE: 1.3104499131441116, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 8, Average Loss: 12.897597670555115, Average MAE: 1.259735181927681, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 9, Average Loss: 12.51944637298584, Average MAE: 1.2172104269266129, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 10, Average Loss: 12.274487853050232, Average MAE: 1.185770332813263, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 11, Average Loss: 12.037314295768738, Average MAE: 1.1607666164636612, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 12, Average Loss: 11.829299449920654, Average MAE: 1.1362863779067993, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 13, Average Loss: 11.661885023117065, Average MAE: 1.1152058094739914, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 14, Average Loss: 11.504494309425354, Average MAE: 1.0952082723379135, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 15, Average Loss: 11.319283962249756, Average MAE: 1.0763118267059326, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 16, Average Loss: 11.140030026435852, Average MAE: 1.056398183107376, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 17, Average Loss: 10.939573526382446, Average MAE: 1.037966251373291, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 18, Average Loss: 10.765364289283752, Average MAE: 1.0201407074928284, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 19, Average Loss: 10.550126552581787, Average MAE: 1.0011401921510696, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 20, Average Loss: 10.320420384407043, Average MAE: 0.9833576008677483, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 21, Average Loss: 10.010769128799438, Average MAE: 0.9636498019099236, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 22, Average Loss: 9.649415969848633, Average MAE: 0.9413949996232986, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 23, Average Loss: 9.3966224193573, Average MAE: 0.9219990372657776, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 24, Average Loss: 9.060253024101257, Average MAE: 0.900654137134552, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 25, Average Loss: 8.728532195091248, Average MAE: 0.879190981388092, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 26, Average Loss: 8.464556872844696, Average MAE: 0.860103078186512, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 27, Average Loss: 8.25094223022461, Average MAE: 0.8487258180975914, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 28, Average Loss: 7.9695428013801575, Average MAE: 0.8329294100403786, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 29, Average Loss: 7.700587153434753, Average MAE: 0.8165836185216904, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 30, Average Loss: 7.475837588310242, Average MAE: 0.7984465658664703, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 31, Average Loss: 7.313956260681152, Average MAE: 0.7874453067779541, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 32, Average Loss: 7.18998646736145, Average MAE: 0.7759749442338943, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 33, Average Loss: 6.921066880226135, Average MAE: 0.758241631090641, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 34, Average Loss: 6.780828833580017, Average MAE: 0.7466822490096092, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 35, Average Loss: 6.643303334712982, Average MAE: 0.7327027395367622, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 36, Average Loss: 6.391807198524475, Average MAE: 0.7125506475567818, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 37, Average Loss: 6.280148565769196, Average MAE: 0.7049807459115982, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 38, Average Loss: 6.168676197528839, Average MAE: 0.6963793709874153, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 39, Average Loss: 5.991612255573273, Average MAE: 0.6822690889239311, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 40, Average Loss: 5.935886085033417, Average MAE: 0.678327277302742, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 41, Average Loss: 5.9167264103889465, Average MAE: 0.6764351427555084, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 42, Average Loss: 6.002609193325043, Average MAE: 0.6754250377416611, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 43, Average Loss: 5.774783551692963, Average MAE: 0.6615917906165123, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 44, Average Loss: 5.7181649804115295, Average MAE: 0.6514494493603706, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 45, Average Loss: 5.775687098503113, Average MAE: 0.6525198891758919, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 46, Average Loss: 5.606835424900055, Average MAE: 0.6425490230321884, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 47, Average Loss: 5.341901123523712, Average MAE: 0.6240228936076164, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 48, Average Loss: 5.267774522304535, Average MAE: 0.615971714258194, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 49, Average Loss: 5.246278166770935, Average MAE: 0.6137196868658066, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 50, Average Loss: 5.152466773986816, Average MAE: 0.6085192933678627, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 51, Average Loss: 5.2380558252334595, Average MAE: 0.6105354353785515, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 52, Average Loss: 5.419312477111816, Average MAE: 0.620447538793087, Learning Rate: 0.01, Current Patience: 2\n",
      "Epoch 53, Average Loss: 5.6857041120529175, Average MAE: 0.6282208636403084, Learning Rate: 0.01, Current Patience: 3\n",
      "Epoch 54, Average Loss: 5.363075494766235, Average MAE: 0.6070777177810669, Learning Rate: 0.01, Current Patience: 4\n",
      "Epoch 55, Average Loss: 5.4828155636787415, Average MAE: 0.6118926480412483, Learning Rate: 0.01, Current Patience: 5\n",
      "Epoch 56, Average Loss: 5.499801397323608, Average MAE: 0.6159896850585938, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 57, Average Loss: 5.052155494689941, Average MAE: 0.5882250592112541, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 58, Average Loss: 4.943733811378479, Average MAE: 0.5766227319836617, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 59, Average Loss: 4.794367253780365, Average MAE: 0.5671946927905083, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 60, Average Loss: 4.735483467578888, Average MAE: 0.564807765185833, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 61, Average Loss: 4.678955078125, Average MAE: 0.5652210041880608, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 62, Average Loss: 4.619833052158356, Average MAE: 0.5574775636196136, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 63, Average Loss: 4.568773329257965, Average MAE: 0.5554163306951523, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 64, Average Loss: 4.53974711894989, Average MAE: 0.5520145744085312, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 65, Average Loss: 4.491514503955841, Average MAE: 0.5475954785943031, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 66, Average Loss: 4.44705456495285, Average MAE: 0.5427236184477806, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 67, Average Loss: 4.422635555267334, Average MAE: 0.5412269830703735, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 68, Average Loss: 4.3924813866615295, Average MAE: 0.5378208830952644, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 69, Average Loss: 4.3998483419418335, Average MAE: 0.5386963188648224, Learning Rate: 0.008, Current Patience: 1\n",
      "Epoch 70, Average Loss: 4.419536054134369, Average MAE: 0.5378468930721283, Learning Rate: 0.008, Current Patience: 2\n",
      "Epoch 71, Average Loss: 4.372328758239746, Average MAE: 0.5332983657717705, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 72, Average Loss: 4.541484951972961, Average MAE: 0.5401487573981285, Learning Rate: 0.008, Current Patience: 1\n",
      "Epoch 73, Average Loss: 4.697014927864075, Average MAE: 0.5564924106001854, Learning Rate: 0.008, Current Patience: 2\n",
      "Epoch 74, Average Loss: 4.8603768944740295, Average MAE: 0.5678142458200455, Learning Rate: 0.008, Current Patience: 3\n",
      "Epoch 75, Average Loss: 4.541044771671295, Average MAE: 0.5441686138510704, Learning Rate: 0.008, Current Patience: 4\n",
      "Epoch 76, Average Loss: 4.435914754867554, Average MAE: 0.5346273556351662, Learning Rate: 0.008, Current Patience: 5\n",
      "Epoch 77, Average Loss: 4.382020473480225, Average MAE: 0.5303246304392815, Learning Rate: 0.0064, Current Patience: 0\n",
      "Epoch 78, Average Loss: 4.274938881397247, Average MAE: 0.5250458940863609, Learning Rate: 0.0064, Current Patience: 0\n",
      "Epoch 79, Average Loss: 4.226246625185013, Average MAE: 0.5230167210102081, Learning Rate: 0.0064, Current Patience: 0\n",
      "Epoch 80, Average Loss: 4.1947848200798035, Average MAE: 0.5197134166955948, Learning Rate: 0.0064, Current Patience: 0\n",
      "Epoch 81, Average Loss: 4.140662282705307, Average MAE: 0.5141702629625797, Learning Rate: 0.0064, Current Patience: 0\n",
      "Epoch 82, Average Loss: 4.127737611532211, Average MAE: 0.5154114253818989, Learning Rate: 0.0064, Current Patience: 0\n",
      "Epoch 83, Average Loss: 4.0985830426216125, Average MAE: 0.5133349448442459, Learning Rate: 0.0064, Current Patience: 0\n",
      "Epoch 84, Average Loss: 4.077665686607361, Average MAE: 0.5108022131025791, Learning Rate: 0.0064, Current Patience: 0\n",
      "Epoch 85, Average Loss: 4.049831390380859, Average MAE: 0.5082142613828182, Learning Rate: 0.0064, Current Patience: 0\n",
      "Epoch 86, Average Loss: 4.054242879152298, Average MAE: 0.5069981813430786, Learning Rate: 0.0064, Current Patience: 1\n",
      "Epoch 87, Average Loss: 4.062977582216263, Average MAE: 0.5071447044610977, Learning Rate: 0.0064, Current Patience: 2\n",
      "Epoch 88, Average Loss: 3.9830265939235687, Average MAE: 0.5041642002761364, Learning Rate: 0.0064, Current Patience: 0\n",
      "Epoch 89, Average Loss: 4.044422090053558, Average MAE: 0.5066127553582191, Learning Rate: 0.0064, Current Patience: 1\n",
      "Epoch 90, Average Loss: 4.102384299039841, Average MAE: 0.509268581867218, Learning Rate: 0.0064, Current Patience: 2\n",
      "Epoch 91, Average Loss: 4.102539926767349, Average MAE: 0.5111935324966908, Learning Rate: 0.0064, Current Patience: 3\n",
      "Epoch 92, Average Loss: 3.9766427278518677, Average MAE: 0.502398893237114, Learning Rate: 0.0064, Current Patience: 0\n",
      "Epoch 93, Average Loss: 4.098793685436249, Average MAE: 0.510349690914154, Learning Rate: 0.0064, Current Patience: 1\n",
      "Epoch 94, Average Loss: 4.08850023150444, Average MAE: 0.5073963813483715, Learning Rate: 0.0064, Current Patience: 2\n",
      "Epoch 95, Average Loss: 4.0300100445747375, Average MAE: 0.5046564191579819, Learning Rate: 0.0064, Current Patience: 3\n",
      "Epoch 96, Average Loss: 3.9297324419021606, Average MAE: 0.49648621678352356, Learning Rate: 0.0064, Current Patience: 0\n",
      "Epoch 97, Average Loss: 4.046622335910797, Average MAE: 0.5054426416754723, Learning Rate: 0.0064, Current Patience: 1\n",
      "Epoch 98, Average Loss: 4.1852624118328094, Average MAE: 0.5159448049962521, Learning Rate: 0.0064, Current Patience: 2\n",
      "Epoch 99, Average Loss: 4.026791572570801, Average MAE: 0.5063103213906288, Learning Rate: 0.0064, Current Patience: 3\n",
      "Epoch 100, Average Loss: 4.145201772451401, Average MAE: 0.5151148587465286, Learning Rate: 0.0064, Current Patience: 4\n",
      "Epoch 101, Average Loss: 4.153551489114761, Average MAE: 0.5126287043094635, Learning Rate: 0.0064, Current Patience: 5\n",
      "Epoch 102, Average Loss: 3.989825516939163, Average MAE: 0.5004039444029331, Learning Rate: 0.00512, Current Patience: 0\n",
      "Epoch 103, Average Loss: 3.9849773943424225, Average MAE: 0.5007314160466194, Learning Rate: 0.00512, Current Patience: 1\n",
      "Epoch 104, Average Loss: 4.184233486652374, Average MAE: 0.5087398737668991, Learning Rate: 0.00512, Current Patience: 2\n",
      "Epoch 105, Average Loss: 4.256195574998856, Average MAE: 0.5137542523443699, Learning Rate: 0.00512, Current Patience: 3\n",
      "Epoch 106, Average Loss: 4.478303909301758, Average MAE: 0.5295610949397087, Learning Rate: 0.00512, Current Patience: 4\n",
      "Epoch 107, Average Loss: 4.7046217024326324, Average MAE: 0.5445717461407185, Learning Rate: 0.00512, Current Patience: 5\n",
      "Epoch 108, Average Loss: 4.934930711984634, Average MAE: 0.560375913977623, Learning Rate: 0.004096000000000001, Current Patience: 0\n",
      "Epoch 109, Average Loss: 5.201098680496216, Average MAE: 0.5720616430044174, Learning Rate: 0.004096000000000001, Current Patience: 1\n",
      "Epoch 110, Average Loss: 4.652598530054092, Average MAE: 0.547119501978159, Learning Rate: 0.004096000000000001, Current Patience: 2\n",
      "Epoch 111, Average Loss: 3.955353021621704, Average MAE: 0.4996005743741989, Learning Rate: 0.004096000000000001, Current Patience: 3\n",
      "Epoch 112, Average Loss: 3.8164986073970795, Average MAE: 0.48955756425857544, Learning Rate: 0.004096000000000001, Current Patience: 0\n",
      "Epoch 113, Average Loss: 3.8170144855976105, Average MAE: 0.48798710107803345, Learning Rate: 0.004096000000000001, Current Patience: 1\n",
      "Epoch 114, Average Loss: 3.9045116305351257, Average MAE: 0.49308574199676514, Learning Rate: 0.004096000000000001, Current Patience: 2\n",
      "Epoch 115, Average Loss: 3.924245297908783, Average MAE: 0.4937475435435772, Learning Rate: 0.004096000000000001, Current Patience: 3\n",
      "Epoch 116, Average Loss: 3.8477196395397186, Average MAE: 0.4884776249527931, Learning Rate: 0.004096000000000001, Current Patience: 4\n",
      "Epoch 117, Average Loss: 3.6995518505573273, Average MAE: 0.4792611487209797, Learning Rate: 0.004096000000000001, Current Patience: 0\n",
      "Epoch 118, Average Loss: 3.664992570877075, Average MAE: 0.4761331118643284, Learning Rate: 0.004096000000000001, Current Patience: 0\n",
      "Epoch 119, Average Loss: 3.6165661811828613, Average MAE: 0.4736071862280369, Learning Rate: 0.004096000000000001, Current Patience: 0\n",
      "Epoch 120, Average Loss: 3.6509178578853607, Average MAE: 0.4770916514098644, Learning Rate: 0.004096000000000001, Current Patience: 1\n",
      "Epoch 121, Average Loss: 3.6770738661289215, Average MAE: 0.47886667773127556, Learning Rate: 0.004096000000000001, Current Patience: 2\n",
      "Epoch 122, Average Loss: 3.719901204109192, Average MAE: 0.4807170294225216, Learning Rate: 0.004096000000000001, Current Patience: 3\n",
      "Epoch 123, Average Loss: 3.7193864583969116, Average MAE: 0.4799334965646267, Learning Rate: 0.004096000000000001, Current Patience: 4\n",
      "Epoch 124, Average Loss: 3.6904112696647644, Average MAE: 0.4810250923037529, Learning Rate: 0.004096000000000001, Current Patience: 5\n",
      "Epoch 125, Average Loss: 3.687967598438263, Average MAE: 0.47853051498532295, Learning Rate: 0.0032768000000000007, Current Patience: 0\n",
      "Epoch 126, Average Loss: 3.6544280350208282, Average MAE: 0.4781483821570873, Learning Rate: 0.0032768000000000007, Current Patience: 1\n",
      "Epoch 127, Average Loss: 3.6863844990730286, Average MAE: 0.4810786843299866, Learning Rate: 0.0032768000000000007, Current Patience: 2\n",
      "Epoch 128, Average Loss: 3.6591266691684723, Average MAE: 0.47888945788145065, Learning Rate: 0.0032768000000000007, Current Patience: 3\n",
      "Epoch 129, Average Loss: 3.6755471229553223, Average MAE: 0.47788113355636597, Learning Rate: 0.0032768000000000007, Current Patience: 4\n",
      "Epoch 130, Average Loss: 3.686823397874832, Average MAE: 0.4792129918932915, Learning Rate: 0.0032768000000000007, Current Patience: 5\n",
      "Epoch 131, Average Loss: 3.7727145552635193, Average MAE: 0.48627738282084465, Learning Rate: 0.002621440000000001, Current Patience: 0\n",
      "Epoch 132, Average Loss: 3.839653193950653, Average MAE: 0.49041252955794334, Learning Rate: 0.002621440000000001, Current Patience: 1\n",
      "Epoch 133, Average Loss: 4.217525511980057, Average MAE: 0.5159201845526695, Learning Rate: 0.002621440000000001, Current Patience: 2\n",
      "Epoch 134, Average Loss: 4.508104473352432, Average MAE: 0.536477118730545, Learning Rate: 0.002621440000000001, Current Patience: 3\n",
      "Epoch 135, Average Loss: 4.589039623737335, Average MAE: 0.5391538813710213, Learning Rate: 0.002621440000000001, Current Patience: 4\n",
      "Epoch 136, Average Loss: 4.461082369089127, Average MAE: 0.5315097495913506, Learning Rate: 0.002621440000000001, Current Patience: 5\n",
      "Epoch 137, Average Loss: 3.9296959340572357, Average MAE: 0.4954126141965389, Learning Rate: 0.002097152000000001, Current Patience: 0\n",
      "Epoch 138, Average Loss: 3.6175247728824615, Average MAE: 0.4730424992740154, Learning Rate: 0.002097152000000001, Current Patience: 1\n",
      "Epoch 139, Average Loss: 3.5417267978191376, Average MAE: 0.46833614259958267, Learning Rate: 0.002097152000000001, Current Patience: 0\n",
      "Epoch 140, Average Loss: 3.5264971256256104, Average MAE: 0.4653025344014168, Learning Rate: 0.002097152000000001, Current Patience: 0\n",
      "Epoch 141, Average Loss: 3.479095071554184, Average MAE: 0.46281053870916367, Learning Rate: 0.002097152000000001, Current Patience: 0\n",
      "Epoch 142, Average Loss: 3.4061242043972015, Average MAE: 0.4576989412307739, Learning Rate: 0.002097152000000001, Current Patience: 0\n",
      "Epoch 143, Average Loss: 3.3853867650032043, Average MAE: 0.45588382333517075, Learning Rate: 0.002097152000000001, Current Patience: 0\n",
      "Epoch 144, Average Loss: 3.3840765953063965, Average MAE: 0.45559410005807877, Learning Rate: 0.002097152000000001, Current Patience: 0\n",
      "Epoch 145, Average Loss: 3.364458352327347, Average MAE: 0.45365817844867706, Learning Rate: 0.002097152000000001, Current Patience: 0\n",
      "Epoch 146, Average Loss: 3.3495908677577972, Average MAE: 0.45359916612505913, Learning Rate: 0.002097152000000001, Current Patience: 0\n",
      "Epoch 147, Average Loss: 3.342956006526947, Average MAE: 0.4523743651807308, Learning Rate: 0.002097152000000001, Current Patience: 0\n",
      "Epoch 148, Average Loss: 3.337468206882477, Average MAE: 0.45263638719916344, Learning Rate: 0.002097152000000001, Current Patience: 0\n",
      "Epoch 149, Average Loss: 3.3374677300453186, Average MAE: 0.4516453295946121, Learning Rate: 0.002097152000000001, Current Patience: 1\n",
      "Epoch 150, Average Loss: 3.324081152677536, Average MAE: 0.4511554539203644, Learning Rate: 0.002097152000000001, Current Patience: 0\n",
      "Epoch 151, Average Loss: 3.326206237077713, Average MAE: 0.4503169171512127, Learning Rate: 0.002097152000000001, Current Patience: 1\n",
      "Epoch 152, Average Loss: 3.30682909488678, Average MAE: 0.4503256492316723, Learning Rate: 0.002097152000000001, Current Patience: 0\n",
      "Epoch 153, Average Loss: 3.299297660589218, Average MAE: 0.4496043249964714, Learning Rate: 0.002097152000000001, Current Patience: 0\n",
      "Epoch 154, Average Loss: 3.309286057949066, Average MAE: 0.45021380484104156, Learning Rate: 0.002097152000000001, Current Patience: 1\n",
      "Epoch 155, Average Loss: 3.2967523634433746, Average MAE: 0.4487568847835064, Learning Rate: 0.002097152000000001, Current Patience: 0\n",
      "Epoch 156, Average Loss: 3.2772955000400543, Average MAE: 0.4476275146007538, Learning Rate: 0.002097152000000001, Current Patience: 0\n",
      "Epoch 157, Average Loss: 3.300912708044052, Average MAE: 0.4493083022534847, Learning Rate: 0.002097152000000001, Current Patience: 1\n",
      "Epoch 158, Average Loss: 3.289659857749939, Average MAE: 0.4475891813635826, Learning Rate: 0.002097152000000001, Current Patience: 2\n",
      "Epoch 159, Average Loss: 3.2602142095565796, Average MAE: 0.44665880501270294, Learning Rate: 0.002097152000000001, Current Patience: 0\n",
      "Epoch 160, Average Loss: 3.2387398183345795, Average MAE: 0.4452580213546753, Learning Rate: 0.002097152000000001, Current Patience: 0\n",
      "Epoch 161, Average Loss: 3.2529839873313904, Average MAE: 0.4455542340874672, Learning Rate: 0.002097152000000001, Current Patience: 1\n",
      "Epoch 162, Average Loss: 3.2684954702854156, Average MAE: 0.44721023738384247, Learning Rate: 0.002097152000000001, Current Patience: 2\n",
      "Epoch 163, Average Loss: 3.2454456984996796, Average MAE: 0.44614676386117935, Learning Rate: 0.002097152000000001, Current Patience: 3\n",
      "Epoch 164, Average Loss: 3.2513753473758698, Average MAE: 0.4460865557193756, Learning Rate: 0.002097152000000001, Current Patience: 4\n",
      "Epoch 165, Average Loss: 3.264982670545578, Average MAE: 0.4465787187218666, Learning Rate: 0.002097152000000001, Current Patience: 5\n",
      "Epoch 166, Average Loss: 3.238003045320511, Average MAE: 0.44506945088505745, Learning Rate: 0.002097152000000001, Current Patience: 0\n",
      "Epoch 167, Average Loss: 3.2144399285316467, Average MAE: 0.44356540963053703, Learning Rate: 0.002097152000000001, Current Patience: 0\n",
      "Epoch 168, Average Loss: 3.2027271389961243, Average MAE: 0.44227825477719307, Learning Rate: 0.002097152000000001, Current Patience: 0\n",
      "Epoch 169, Average Loss: 3.1850178837776184, Average MAE: 0.44194889441132545, Learning Rate: 0.002097152000000001, Current Patience: 0\n",
      "Epoch 170, Average Loss: 3.180821180343628, Average MAE: 0.44030993059277534, Learning Rate: 0.002097152000000001, Current Patience: 0\n",
      "Epoch 171, Average Loss: 3.2143132388591766, Average MAE: 0.4423532001674175, Learning Rate: 0.002097152000000001, Current Patience: 1\n",
      "Epoch 172, Average Loss: 3.2107664048671722, Average MAE: 0.44277649745345116, Learning Rate: 0.002097152000000001, Current Patience: 2\n",
      "Epoch 173, Average Loss: 3.185480386018753, Average MAE: 0.44132935628294945, Learning Rate: 0.002097152000000001, Current Patience: 3\n",
      "Epoch 174, Average Loss: 3.189343124628067, Average MAE: 0.44134487211704254, Learning Rate: 0.002097152000000001, Current Patience: 4\n",
      "Epoch 175, Average Loss: 3.1648667454719543, Average MAE: 0.44064077734947205, Learning Rate: 0.002097152000000001, Current Patience: 0\n",
      "Epoch 176, Average Loss: 3.1757262349128723, Average MAE: 0.44066349789500237, Learning Rate: 0.002097152000000001, Current Patience: 1\n",
      "Epoch 177, Average Loss: 3.14583420753479, Average MAE: 0.4385906755924225, Learning Rate: 0.002097152000000001, Current Patience: 0\n",
      "Epoch 178, Average Loss: 3.1431995928287506, Average MAE: 0.4380660466849804, Learning Rate: 0.002097152000000001, Current Patience: 0\n",
      "Epoch 179, Average Loss: 3.1371114253997803, Average MAE: 0.438299011439085, Learning Rate: 0.002097152000000001, Current Patience: 0\n",
      "Epoch 180, Average Loss: 3.1430092453956604, Average MAE: 0.4388809874653816, Learning Rate: 0.002097152000000001, Current Patience: 1\n",
      "Epoch 181, Average Loss: 3.158362239599228, Average MAE: 0.4392971657216549, Learning Rate: 0.002097152000000001, Current Patience: 2\n",
      "Epoch 182, Average Loss: 3.1650661528110504, Average MAE: 0.44056450575590134, Learning Rate: 0.002097152000000001, Current Patience: 3\n",
      "Epoch 183, Average Loss: 3.183348387479782, Average MAE: 0.44063058122992516, Learning Rate: 0.002097152000000001, Current Patience: 4\n",
      "Epoch 184, Average Loss: 3.180146634578705, Average MAE: 0.4406561106443405, Learning Rate: 0.002097152000000001, Current Patience: 5\n",
      "Epoch 185, Average Loss: 3.1995308697223663, Average MAE: 0.44188912585377693, Learning Rate: 0.001677721600000001, Current Patience: 0\n",
      "Epoch 186, Average Loss: 3.189585506916046, Average MAE: 0.4413825683295727, Learning Rate: 0.001677721600000001, Current Patience: 1\n",
      "Epoch 187, Average Loss: 3.2235805690288544, Average MAE: 0.44192487746477127, Learning Rate: 0.001677721600000001, Current Patience: 2\n",
      "Epoch 188, Average Loss: 3.18779718875885, Average MAE: 0.4413566291332245, Learning Rate: 0.001677721600000001, Current Patience: 3\n",
      "Epoch 189, Average Loss: 3.195822775363922, Average MAE: 0.4421216584742069, Learning Rate: 0.001677721600000001, Current Patience: 4\n",
      "Epoch 190, Average Loss: 3.1606016755104065, Average MAE: 0.44001322612166405, Learning Rate: 0.001677721600000001, Current Patience: 5\n",
      "Epoch 191, Average Loss: 3.15152308344841, Average MAE: 0.44030047953128815, Learning Rate: 0.0013421772800000008, Current Patience: 0\n",
      "Epoch 192, Average Loss: 3.1238816380500793, Average MAE: 0.4382549561560154, Learning Rate: 0.0013421772800000008, Current Patience: 0\n",
      "Epoch 193, Average Loss: 3.125310033559799, Average MAE: 0.43882474303245544, Learning Rate: 0.0013421772800000008, Current Patience: 1\n",
      "Epoch 194, Average Loss: 3.1031764447689056, Average MAE: 0.43660297989845276, Learning Rate: 0.0013421772800000008, Current Patience: 0\n",
      "Epoch 195, Average Loss: 3.0622867941856384, Average MAE: 0.4332891143858433, Learning Rate: 0.0013421772800000008, Current Patience: 0\n",
      "Epoch 196, Average Loss: 3.0568118691444397, Average MAE: 0.4329151213169098, Learning Rate: 0.0013421772800000008, Current Patience: 0\n",
      "Epoch 197, Average Loss: 3.086807429790497, Average MAE: 0.4336725175380707, Learning Rate: 0.0013421772800000008, Current Patience: 1\n",
      "Epoch 198, Average Loss: 3.078187048435211, Average MAE: 0.4336456283926964, Learning Rate: 0.0013421772800000008, Current Patience: 2\n",
      "Epoch 199, Average Loss: 3.057900995016098, Average MAE: 0.43197566643357277, Learning Rate: 0.0013421772800000008, Current Patience: 3\n",
      "Epoch 200, Average Loss: 3.0508669316768646, Average MAE: 0.4313877075910568, Learning Rate: 0.0013421772800000008, Current Patience: 0\n",
      "Model data saved in saved_models_20241212_3/model_parallel_20241212_072432\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if run_model_parallel:\n",
    "    # Call the function\n",
    "    # Define the parameters\n",
    "    in_channels = batched_input_train.shape[2]\n",
    "    features_channels = batched_input_train.shape[3]\n",
    "    out_channels = batched_output_train.shape[3]\n",
    "    edge_in_channels = 1\n",
    "\n",
    "    # Print the parameters to verify\n",
    "    print(\"Input Channels:\", in_channels)\n",
    "    print(\"Features Channels:\", features_channels)\n",
    "    print(\"Output Channels:\", out_channels)\n",
    "    print(\"Edge Input Channels:\", edge_in_channels)\n",
    "    print(\"Hidden Channels:\", hidden_channels)\n",
    "    print(\"Transformer Layers:\", transformer_layers)\n",
    "\n",
    "    model_parallel = HybridModel_Parallel(in_channels, out_channels, features_channels, edge_in_channels, hidden_channels, transformer_layers).to(device)\n",
    "\n",
    "    # Define the optimizer and loss function for model_parallel\n",
    "    optimizer_parallel = torch.optim.Adam(model_parallel.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    scheduler_parallel = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_parallel, mode=scheduler_mode, factor=scheduler_factor, patience=scheduler_patience, verbose=True, min_lr=1e-5)\n",
    "\n",
    "    parallel_epoch_losses, parallel_epoch_maes = train_model(model_parallel, optimizer_parallel, scheduler_parallel, loss_fn, num_epochs, num_batches_train, batched_input_train, batched_output_train, device)\n",
    "    save_model_data(model_parallel, optimizer_parallel, scheduler_parallel, parallel_epoch_losses, parallel_epoch_maes, hidden_channels, transformer_layers,new_iteration, 'model_parallel')\n",
    "    del model_parallel\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Channels: 27\n",
      "Features Channels: 69\n",
      "Output Channels: 67\n",
      "Edge Input Channels: 1\n",
      "Hidden Channels: 1024\n",
      "Transformer Layers: 8\n",
      "Epoch 1, Average Loss: 120.29445266723633, Average MAE: 6.562188476324081, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 2, Average Loss: 46.203734159469604, Average MAE: 4.221675872802734, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 3, Average Loss: 28.47173285484314, Average MAE: 2.9659784138202667, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 4, Average Loss: 21.549693822860718, Average MAE: 2.255363807082176, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 5, Average Loss: 19.067057609558105, Average MAE: 1.8566427528858185, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 6, Average Loss: 17.006380081176758, Average MAE: 1.5501118004322052, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 7, Average Loss: 15.940709352493286, Average MAE: 1.3787061423063278, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 8, Average Loss: 15.257993936538696, Average MAE: 1.2931702882051468, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 9, Average Loss: 14.671279668807983, Average MAE: 1.2367422133684158, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 10, Average Loss: 14.341349601745605, Average MAE: 1.2031554728746414, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 11, Average Loss: 14.234719038009644, Average MAE: 1.1842419505119324, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 12, Average Loss: 14.101364254951477, Average MAE: 1.170354425907135, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 13, Average Loss: 14.015517115592957, Average MAE: 1.156657725572586, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 14, Average Loss: 13.91724407672882, Average MAE: 1.1469980776309967, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 15, Average Loss: 13.87289845943451, Average MAE: 1.1374555975198746, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 16, Average Loss: 13.717662334442139, Average MAE: 1.1238392442464828, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 17, Average Loss: 13.678961634635925, Average MAE: 1.115101546049118, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 18, Average Loss: 13.617804408073425, Average MAE: 1.10523222386837, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 19, Average Loss: 13.552509427070618, Average MAE: 1.0946457535028458, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 20, Average Loss: 13.431545734405518, Average MAE: 1.084310919046402, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 21, Average Loss: 13.327171444892883, Average MAE: 1.0736809000372887, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 22, Average Loss: 13.291084051132202, Average MAE: 1.066336713731289, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 23, Average Loss: 13.182949781417847, Average MAE: 1.0547281950712204, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 24, Average Loss: 13.163092613220215, Average MAE: 1.0477280542254448, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 25, Average Loss: 13.08779001235962, Average MAE: 1.0383717641234398, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 26, Average Loss: 13.090439796447754, Average MAE: 1.031604640185833, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 27, Average Loss: 13.00525188446045, Average MAE: 1.0240379944443703, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 28, Average Loss: 12.978770852088928, Average MAE: 1.0151605308055878, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 29, Average Loss: 12.91893470287323, Average MAE: 1.0075044631958008, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 30, Average Loss: 12.884055137634277, Average MAE: 1.0011751428246498, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 31, Average Loss: 12.872568130493164, Average MAE: 0.9948917478322983, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 32, Average Loss: 12.860271692276001, Average MAE: 0.9894252419471741, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 33, Average Loss: 12.82786512374878, Average MAE: 0.9817377626895905, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 34, Average Loss: 12.756640315055847, Average MAE: 0.9743724465370178, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 35, Average Loss: 12.761956214904785, Average MAE: 0.969236247241497, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 36, Average Loss: 12.733140230178833, Average MAE: 0.9636509343981743, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 37, Average Loss: 12.697903156280518, Average MAE: 0.9589030370116234, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 38, Average Loss: 12.702896475791931, Average MAE: 0.9543942287564278, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 39, Average Loss: 12.699404954910278, Average MAE: 0.9503465443849564, Learning Rate: 0.01, Current Patience: 2\n",
      "Epoch 40, Average Loss: 12.647767066955566, Average MAE: 0.9430676624178886, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 41, Average Loss: 12.656389832496643, Average MAE: 0.940506599843502, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 42, Average Loss: 12.64099383354187, Average MAE: 0.9359203577041626, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 43, Average Loss: 12.636308312416077, Average MAE: 0.932740718126297, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 44, Average Loss: 12.609659433364868, Average MAE: 0.9282663688063622, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 45, Average Loss: 12.598540186882019, Average MAE: 0.923948772251606, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 46, Average Loss: 12.596053123474121, Average MAE: 0.9207906052470207, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 47, Average Loss: 12.573248147964478, Average MAE: 0.9170857518911362, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 48, Average Loss: 12.563396453857422, Average MAE: 0.9138040021061897, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 49, Average Loss: 12.604117751121521, Average MAE: 0.9118595942854881, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 50, Average Loss: 12.549232482910156, Average MAE: 0.9075853899121284, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 51, Average Loss: 12.555759191513062, Average MAE: 0.905439481139183, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 52, Average Loss: 12.533573389053345, Average MAE: 0.9023042470216751, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 53, Average Loss: 12.5581716299057, Average MAE: 0.9006392359733582, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 54, Average Loss: 12.519512414932251, Average MAE: 0.8971216157078743, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 55, Average Loss: 12.53299069404602, Average MAE: 0.8951504528522491, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 56, Average Loss: 12.542405486106873, Average MAE: 0.8940879628062248, Learning Rate: 0.01, Current Patience: 2\n",
      "Epoch 57, Average Loss: 12.53817343711853, Average MAE: 0.8920976296067238, Learning Rate: 0.01, Current Patience: 3\n",
      "Epoch 58, Average Loss: 12.500471353530884, Average MAE: 0.8884513974189758, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 59, Average Loss: 12.494324684143066, Average MAE: 0.8861865550279617, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 60, Average Loss: 12.487584352493286, Average MAE: 0.8845132440328598, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 61, Average Loss: 12.507821917533875, Average MAE: 0.8842260390520096, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 62, Average Loss: 12.495798230171204, Average MAE: 0.8818671777844429, Learning Rate: 0.01, Current Patience: 2\n",
      "Epoch 63, Average Loss: 12.48016345500946, Average MAE: 0.8805472999811172, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 64, Average Loss: 12.492526412010193, Average MAE: 0.8796709924936295, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 65, Average Loss: 12.492443919181824, Average MAE: 0.8780248165130615, Learning Rate: 0.01, Current Patience: 2\n",
      "Epoch 66, Average Loss: 12.476208209991455, Average MAE: 0.8759708553552628, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 67, Average Loss: 12.451041340827942, Average MAE: 0.873697966337204, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 68, Average Loss: 12.459247827529907, Average MAE: 0.8733662515878677, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 69, Average Loss: 12.483455896377563, Average MAE: 0.8725490495562553, Learning Rate: 0.01, Current Patience: 2\n",
      "Epoch 70, Average Loss: 12.437476515769958, Average MAE: 0.870311051607132, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 71, Average Loss: 12.475249409675598, Average MAE: 0.8706381916999817, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 72, Average Loss: 12.458930730819702, Average MAE: 0.8692840337753296, Learning Rate: 0.01, Current Patience: 2\n",
      "Epoch 73, Average Loss: 12.446535229682922, Average MAE: 0.8675942122936249, Learning Rate: 0.01, Current Patience: 3\n",
      "Epoch 74, Average Loss: 12.428426861763, Average MAE: 0.8660961911082268, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 75, Average Loss: 12.433105945587158, Average MAE: 0.8658829554915428, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 76, Average Loss: 12.434418678283691, Average MAE: 0.8645361438393593, Learning Rate: 0.01, Current Patience: 2\n",
      "Epoch 77, Average Loss: 12.43191397190094, Average MAE: 0.8646597042679787, Learning Rate: 0.01, Current Patience: 3\n",
      "Epoch 78, Average Loss: 12.432542085647583, Average MAE: 0.8633255437016487, Learning Rate: 0.01, Current Patience: 4\n",
      "Epoch 79, Average Loss: 12.427956581115723, Average MAE: 0.8619456589221954, Learning Rate: 0.01, Current Patience: 5\n",
      "Epoch 80, Average Loss: 12.429561972618103, Average MAE: 0.8617209941148758, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 81, Average Loss: 12.36106014251709, Average MAE: 0.8599593788385391, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 82, Average Loss: 12.370757460594177, Average MAE: 0.8574545681476593, Learning Rate: 0.008, Current Patience: 1\n",
      "Epoch 83, Average Loss: 12.336183667182922, Average MAE: 0.853444829583168, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 84, Average Loss: 12.310744047164917, Average MAE: 0.8521125167608261, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 85, Average Loss: 12.293505311012268, Average MAE: 0.8503068536520004, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 86, Average Loss: 12.32420563697815, Average MAE: 0.851679265499115, Learning Rate: 0.008, Current Patience: 1\n",
      "Epoch 87, Average Loss: 12.312817215919495, Average MAE: 0.850802406668663, Learning Rate: 0.008, Current Patience: 2\n",
      "Epoch 88, Average Loss: 12.302609205245972, Average MAE: 0.8504058048129082, Learning Rate: 0.008, Current Patience: 3\n",
      "Epoch 89, Average Loss: 12.302451133728027, Average MAE: 0.8489506542682648, Learning Rate: 0.008, Current Patience: 4\n",
      "Epoch 90, Average Loss: 12.317773342132568, Average MAE: 0.8498927131295204, Learning Rate: 0.008, Current Patience: 5\n",
      "Epoch 91, Average Loss: 12.293698906898499, Average MAE: 0.8486176207661629, Learning Rate: 0.0064, Current Patience: 0\n",
      "Epoch 92, Average Loss: 12.271365642547607, Average MAE: 0.8469219654798508, Learning Rate: 0.0064, Current Patience: 0\n",
      "Epoch 93, Average Loss: 12.253891468048096, Average MAE: 0.8454670682549477, Learning Rate: 0.0064, Current Patience: 0\n",
      "Epoch 94, Average Loss: 12.229065179824829, Average MAE: 0.8447350561618805, Learning Rate: 0.0064, Current Patience: 0\n",
      "Epoch 95, Average Loss: 12.271612286567688, Average MAE: 0.8469640389084816, Learning Rate: 0.0064, Current Patience: 1\n",
      "Epoch 96, Average Loss: 12.250202894210815, Average MAE: 0.8454546332359314, Learning Rate: 0.0064, Current Patience: 2\n",
      "Epoch 97, Average Loss: 12.263386845588684, Average MAE: 0.84612787514925, Learning Rate: 0.0064, Current Patience: 3\n",
      "Epoch 98, Average Loss: 12.256321430206299, Average MAE: 0.8453117609024048, Learning Rate: 0.0064, Current Patience: 4\n",
      "Epoch 99, Average Loss: 12.229451179504395, Average MAE: 0.8432954251766205, Learning Rate: 0.0064, Current Patience: 5\n",
      "Epoch 100, Average Loss: 12.265941858291626, Average MAE: 0.8446321114897728, Learning Rate: 0.00512, Current Patience: 0\n",
      "Epoch 101, Average Loss: 12.189658403396606, Average MAE: 0.840738408267498, Learning Rate: 0.00512, Current Patience: 0\n",
      "Epoch 102, Average Loss: 12.200197458267212, Average MAE: 0.8420066013932228, Learning Rate: 0.00512, Current Patience: 1\n",
      "Epoch 103, Average Loss: 12.193257808685303, Average MAE: 0.8415516912937164, Learning Rate: 0.00512, Current Patience: 2\n",
      "Epoch 104, Average Loss: 12.204779267311096, Average MAE: 0.8423814177513123, Learning Rate: 0.00512, Current Patience: 3\n",
      "Epoch 105, Average Loss: 12.218282341957092, Average MAE: 0.8432452976703644, Learning Rate: 0.00512, Current Patience: 4\n",
      "Epoch 106, Average Loss: 12.212984204292297, Average MAE: 0.8423152416944504, Learning Rate: 0.00512, Current Patience: 5\n",
      "Epoch 107, Average Loss: 12.195374488830566, Average MAE: 0.8420194983482361, Learning Rate: 0.004096000000000001, Current Patience: 0\n",
      "Epoch 108, Average Loss: 12.176701784133911, Average MAE: 0.8397849947214127, Learning Rate: 0.004096000000000001, Current Patience: 0\n",
      "Epoch 109, Average Loss: 12.159920334815979, Average MAE: 0.8397409543395042, Learning Rate: 0.004096000000000001, Current Patience: 0\n",
      "Epoch 110, Average Loss: 12.182271599769592, Average MAE: 0.8400796130299568, Learning Rate: 0.004096000000000001, Current Patience: 1\n",
      "Epoch 111, Average Loss: 12.173576831817627, Average MAE: 0.8388813436031342, Learning Rate: 0.004096000000000001, Current Patience: 2\n",
      "Epoch 112, Average Loss: 12.157146692276001, Average MAE: 0.8379500508308411, Learning Rate: 0.004096000000000001, Current Patience: 0\n",
      "Epoch 113, Average Loss: 12.153613567352295, Average MAE: 0.8375548645853996, Learning Rate: 0.004096000000000001, Current Patience: 0\n",
      "Epoch 114, Average Loss: 12.155246138572693, Average MAE: 0.8372462540864944, Learning Rate: 0.004096000000000001, Current Patience: 1\n",
      "Epoch 115, Average Loss: 12.153189539909363, Average MAE: 0.8365298360586166, Learning Rate: 0.004096000000000001, Current Patience: 2\n",
      "Epoch 116, Average Loss: 12.160897254943848, Average MAE: 0.8374265506863594, Learning Rate: 0.004096000000000001, Current Patience: 3\n",
      "Epoch 117, Average Loss: 12.165085792541504, Average MAE: 0.8370270729064941, Learning Rate: 0.004096000000000001, Current Patience: 4\n",
      "Epoch 118, Average Loss: 12.161898136138916, Average MAE: 0.8373149260878563, Learning Rate: 0.004096000000000001, Current Patience: 5\n",
      "Epoch 119, Average Loss: 12.176445364952087, Average MAE: 0.8378923907876015, Learning Rate: 0.0032768000000000007, Current Patience: 0\n",
      "Epoch 120, Average Loss: 12.124698877334595, Average MAE: 0.8357527703046799, Learning Rate: 0.0032768000000000007, Current Patience: 0\n",
      "Epoch 121, Average Loss: 12.134507417678833, Average MAE: 0.834774985909462, Learning Rate: 0.0032768000000000007, Current Patience: 1\n",
      "Epoch 122, Average Loss: 12.136634111404419, Average MAE: 0.83485546708107, Learning Rate: 0.0032768000000000007, Current Patience: 2\n",
      "Epoch 123, Average Loss: 12.136154770851135, Average MAE: 0.8342932909727097, Learning Rate: 0.0032768000000000007, Current Patience: 3\n",
      "Epoch 124, Average Loss: 12.121617794036865, Average MAE: 0.8332982510328293, Learning Rate: 0.0032768000000000007, Current Patience: 0\n",
      "Epoch 125, Average Loss: 12.124743342399597, Average MAE: 0.8334903866052628, Learning Rate: 0.0032768000000000007, Current Patience: 1\n",
      "Epoch 126, Average Loss: 12.13114333152771, Average MAE: 0.8336362689733505, Learning Rate: 0.0032768000000000007, Current Patience: 2\n",
      "Epoch 127, Average Loss: 12.132012009620667, Average MAE: 0.8334408327937126, Learning Rate: 0.0032768000000000007, Current Patience: 3\n",
      "Epoch 128, Average Loss: 12.131885170936584, Average MAE: 0.833729013800621, Learning Rate: 0.0032768000000000007, Current Patience: 4\n",
      "Epoch 129, Average Loss: 12.12541115283966, Average MAE: 0.8335733786225319, Learning Rate: 0.0032768000000000007, Current Patience: 5\n",
      "Epoch 130, Average Loss: 12.133767366409302, Average MAE: 0.8339800164103508, Learning Rate: 0.002621440000000001, Current Patience: 0\n",
      "Epoch 131, Average Loss: 12.099204063415527, Average MAE: 0.8319495990872383, Learning Rate: 0.002621440000000001, Current Patience: 0\n",
      "Epoch 132, Average Loss: 12.098618984222412, Average MAE: 0.8314655125141144, Learning Rate: 0.002621440000000001, Current Patience: 1\n",
      "Epoch 133, Average Loss: 12.122738718986511, Average MAE: 0.8318938910961151, Learning Rate: 0.002621440000000001, Current Patience: 2\n",
      "Epoch 134, Average Loss: 12.099594473838806, Average MAE: 0.8307128474116325, Learning Rate: 0.002621440000000001, Current Patience: 3\n",
      "Epoch 135, Average Loss: 12.10876977443695, Average MAE: 0.8316037505865097, Learning Rate: 0.002621440000000001, Current Patience: 4\n",
      "Epoch 136, Average Loss: 12.112740755081177, Average MAE: 0.831762783229351, Learning Rate: 0.002621440000000001, Current Patience: 5\n",
      "Epoch 137, Average Loss: 12.11889910697937, Average MAE: 0.8312640488147736, Learning Rate: 0.002097152000000001, Current Patience: 0\n",
      "Epoch 138, Average Loss: 12.08489739894867, Average MAE: 0.8304051533341408, Learning Rate: 0.002097152000000001, Current Patience: 0\n",
      "Epoch 139, Average Loss: 12.096236109733582, Average MAE: 0.8304681777954102, Learning Rate: 0.002097152000000001, Current Patience: 1\n",
      "Epoch 140, Average Loss: 12.074090957641602, Average MAE: 0.8296551704406738, Learning Rate: 0.002097152000000001, Current Patience: 0\n",
      "Epoch 141, Average Loss: 12.091530203819275, Average MAE: 0.8301194608211517, Learning Rate: 0.002097152000000001, Current Patience: 1\n",
      "Epoch 142, Average Loss: 12.083255887031555, Average MAE: 0.8304537162184715, Learning Rate: 0.002097152000000001, Current Patience: 2\n",
      "Epoch 143, Average Loss: 12.083038926124573, Average MAE: 0.8297822996973991, Learning Rate: 0.002097152000000001, Current Patience: 3\n",
      "Epoch 144, Average Loss: 12.09445035457611, Average MAE: 0.8299853280186653, Learning Rate: 0.002097152000000001, Current Patience: 4\n",
      "Epoch 145, Average Loss: 12.118046879768372, Average MAE: 0.8303981646895409, Learning Rate: 0.002097152000000001, Current Patience: 5\n",
      "Epoch 146, Average Loss: 12.113916039466858, Average MAE: 0.8303836807608604, Learning Rate: 0.001677721600000001, Current Patience: 0\n",
      "Epoch 147, Average Loss: 12.080841064453125, Average MAE: 0.8286252021789551, Learning Rate: 0.001677721600000001, Current Patience: 1\n",
      "Epoch 148, Average Loss: 12.086974024772644, Average MAE: 0.8293154910206795, Learning Rate: 0.001677721600000001, Current Patience: 2\n",
      "Epoch 149, Average Loss: 12.072383522987366, Average MAE: 0.828558474779129, Learning Rate: 0.001677721600000001, Current Patience: 0\n",
      "Epoch 150, Average Loss: 12.086653113365173, Average MAE: 0.8291920050978661, Learning Rate: 0.001677721600000001, Current Patience: 1\n",
      "Epoch 151, Average Loss: 12.070988893508911, Average MAE: 0.8285664692521095, Learning Rate: 0.001677721600000001, Current Patience: 0\n",
      "Epoch 152, Average Loss: 12.088005065917969, Average MAE: 0.8290065079927444, Learning Rate: 0.001677721600000001, Current Patience: 1\n",
      "Epoch 153, Average Loss: 12.085105419158936, Average MAE: 0.8280301839113235, Learning Rate: 0.001677721600000001, Current Patience: 2\n",
      "Epoch 154, Average Loss: 12.064228177070618, Average MAE: 0.8281321451067924, Learning Rate: 0.001677721600000001, Current Patience: 0\n",
      "Epoch 155, Average Loss: 12.07603371143341, Average MAE: 0.8288834542036057, Learning Rate: 0.001677721600000001, Current Patience: 1\n",
      "Epoch 156, Average Loss: 12.058875441551208, Average MAE: 0.8277709260582924, Learning Rate: 0.001677721600000001, Current Patience: 0\n",
      "Epoch 157, Average Loss: 12.05876088142395, Average MAE: 0.8283666148781776, Learning Rate: 0.001677721600000001, Current Patience: 1\n",
      "Epoch 158, Average Loss: 12.074125289916992, Average MAE: 0.8286004886031151, Learning Rate: 0.001677721600000001, Current Patience: 2\n",
      "Epoch 159, Average Loss: 12.089501142501831, Average MAE: 0.8283862322568893, Learning Rate: 0.001677721600000001, Current Patience: 3\n",
      "Epoch 160, Average Loss: 12.069412589073181, Average MAE: 0.8273943364620209, Learning Rate: 0.001677721600000001, Current Patience: 4\n",
      "Epoch 161, Average Loss: 12.066127896308899, Average MAE: 0.8279824182391167, Learning Rate: 0.001677721600000001, Current Patience: 5\n",
      "Epoch 162, Average Loss: 12.071074843406677, Average MAE: 0.8281210288405418, Learning Rate: 0.0013421772800000008, Current Patience: 0\n",
      "Epoch 163, Average Loss: 12.060291171073914, Average MAE: 0.8272951990365982, Learning Rate: 0.0013421772800000008, Current Patience: 1\n",
      "Epoch 164, Average Loss: 12.07620882987976, Average MAE: 0.8279594779014587, Learning Rate: 0.0013421772800000008, Current Patience: 2\n",
      "Epoch 165, Average Loss: 12.07105016708374, Average MAE: 0.8275768905878067, Learning Rate: 0.0013421772800000008, Current Patience: 3\n",
      "Epoch 166, Average Loss: 12.079280495643616, Average MAE: 0.8281197547912598, Learning Rate: 0.0013421772800000008, Current Patience: 4\n",
      "Epoch 167, Average Loss: 12.068852305412292, Average MAE: 0.8274598494172096, Learning Rate: 0.0013421772800000008, Current Patience: 5\n",
      "Epoch 168, Average Loss: 12.08092200756073, Average MAE: 0.8278410658240318, Learning Rate: 0.0010737418240000006, Current Patience: 0\n",
      "Epoch 169, Average Loss: 12.060758233070374, Average MAE: 0.8273823484778404, Learning Rate: 0.0010737418240000006, Current Patience: 1\n",
      "Epoch 170, Average Loss: 12.056671619415283, Average MAE: 0.8269322589039803, Learning Rate: 0.0010737418240000006, Current Patience: 0\n",
      "Epoch 171, Average Loss: 12.053055167198181, Average MAE: 0.8264010697603226, Learning Rate: 0.0010737418240000006, Current Patience: 0\n",
      "Epoch 172, Average Loss: 12.032529711723328, Average MAE: 0.8259533941745758, Learning Rate: 0.0010737418240000006, Current Patience: 0\n",
      "Epoch 173, Average Loss: 12.065456509590149, Average MAE: 0.8265185132622719, Learning Rate: 0.0010737418240000006, Current Patience: 1\n",
      "Epoch 174, Average Loss: 12.055216789245605, Average MAE: 0.8267798647284508, Learning Rate: 0.0010737418240000006, Current Patience: 2\n",
      "Epoch 175, Average Loss: 12.052889347076416, Average MAE: 0.8264200761914253, Learning Rate: 0.0010737418240000006, Current Patience: 3\n",
      "Epoch 176, Average Loss: 12.037994146347046, Average MAE: 0.8263350650668144, Learning Rate: 0.0010737418240000006, Current Patience: 4\n",
      "Epoch 177, Average Loss: 12.058437585830688, Average MAE: 0.8266293480992317, Learning Rate: 0.0010737418240000006, Current Patience: 5\n",
      "Epoch 178, Average Loss: 12.048310399055481, Average MAE: 0.8266659006476402, Learning Rate: 0.0008589934592000006, Current Patience: 0\n",
      "Epoch 179, Average Loss: 12.052188873291016, Average MAE: 0.8259196504950523, Learning Rate: 0.0008589934592000006, Current Patience: 1\n",
      "Epoch 180, Average Loss: 12.064276218414307, Average MAE: 0.8267176449298859, Learning Rate: 0.0008589934592000006, Current Patience: 2\n",
      "Epoch 181, Average Loss: 12.04470431804657, Average MAE: 0.8259477615356445, Learning Rate: 0.0008589934592000006, Current Patience: 3\n",
      "Epoch 182, Average Loss: 12.035058498382568, Average MAE: 0.8256967887282372, Learning Rate: 0.0008589934592000006, Current Patience: 4\n",
      "Epoch 183, Average Loss: 12.039376735687256, Average MAE: 0.8252115473151207, Learning Rate: 0.0008589934592000006, Current Patience: 5\n",
      "Epoch 184, Average Loss: 12.039177656173706, Average MAE: 0.8254834339022636, Learning Rate: 0.0006871947673600005, Current Patience: 0\n",
      "Epoch 185, Average Loss: 12.03491735458374, Average MAE: 0.825335219502449, Learning Rate: 0.0006871947673600005, Current Patience: 1\n",
      "Epoch 186, Average Loss: 12.028122186660767, Average MAE: 0.8248444199562073, Learning Rate: 0.0006871947673600005, Current Patience: 0\n",
      "Epoch 187, Average Loss: 12.045453786849976, Average MAE: 0.8256443589925766, Learning Rate: 0.0006871947673600005, Current Patience: 1\n",
      "Epoch 188, Average Loss: 12.025508284568787, Average MAE: 0.8255163207650185, Learning Rate: 0.0006871947673600005, Current Patience: 0\n",
      "Epoch 189, Average Loss: 12.032631278038025, Average MAE: 0.8250851556658745, Learning Rate: 0.0006871947673600005, Current Patience: 1\n",
      "Epoch 190, Average Loss: 12.049964904785156, Average MAE: 0.8259029313921928, Learning Rate: 0.0006871947673600005, Current Patience: 2\n",
      "Epoch 191, Average Loss: 12.037875175476074, Average MAE: 0.8259137496352196, Learning Rate: 0.0006871947673600005, Current Patience: 3\n",
      "Epoch 192, Average Loss: 12.047810912132263, Average MAE: 0.8256094083189964, Learning Rate: 0.0006871947673600005, Current Patience: 4\n",
      "Epoch 193, Average Loss: 12.036201000213623, Average MAE: 0.8253563717007637, Learning Rate: 0.0006871947673600005, Current Patience: 5\n",
      "Epoch 194, Average Loss: 12.032660484313965, Average MAE: 0.8247661888599396, Learning Rate: 0.0005497558138880005, Current Patience: 0\n",
      "Epoch 195, Average Loss: 12.043179154396057, Average MAE: 0.8250929564237595, Learning Rate: 0.0005497558138880005, Current Patience: 1\n",
      "Epoch 196, Average Loss: 12.003481149673462, Average MAE: 0.8242062404751778, Learning Rate: 0.0005497558138880005, Current Patience: 0\n",
      "Epoch 197, Average Loss: 12.008935809135437, Average MAE: 0.8243474513292313, Learning Rate: 0.0005497558138880005, Current Patience: 1\n",
      "Epoch 198, Average Loss: 12.040682673454285, Average MAE: 0.8251896798610687, Learning Rate: 0.0005497558138880005, Current Patience: 2\n",
      "Epoch 199, Average Loss: 12.048792958259583, Average MAE: 0.8258164748549461, Learning Rate: 0.0005497558138880005, Current Patience: 3\n",
      "Epoch 200, Average Loss: 12.03783643245697, Average MAE: 0.8249525427818298, Learning Rate: 0.0005497558138880005, Current Patience: 4\n",
      "Model data saved in saved_models_20241212_3/model_transformer_gnn_20241212_080644\n"
     ]
    }
   ],
   "source": [
    "# Toggle to run the code\n",
    "if run_model_transformer_gnn:\n",
    "    # Call the function\n",
    "    # Define the parameters\n",
    "    in_channels = batched_input_train.shape[2]\n",
    "    features_channels = batched_input_train.shape[3]\n",
    "    out_channels = batched_output_train.shape[3]  \n",
    "    edge_in_channels = 1\n",
    "\n",
    "    # Print the parameters to verify\n",
    "    print(\"Input Channels:\", in_channels)\n",
    "    print(\"Features Channels:\", features_channels)\n",
    "    print(\"Output Channels:\", out_channels)\n",
    "    print(\"Edge Input Channels:\", edge_in_channels)\n",
    "    print(\"Hidden Channels:\", hidden_channels)\n",
    "    print(\"Transformer Layers:\", transformer_layers)\n",
    "\n",
    "    model_transformer_gnn = HybridModel_Transformer(in_channels, out_channels, features_channels, edge_in_channels, hidden_channels, transformer_layers).to(device)\n",
    "\n",
    "    # Define the optimizer and loss function for model_transformer_gnn\n",
    "    optimizer_transformer_gnn = torch.optim.Adam(model_transformer_gnn.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    scheduler_transformer_gnn = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_transformer_gnn, mode=scheduler_mode, factor=scheduler_factor, patience=scheduler_patience, verbose=True, min_lr=1e-5)\n",
    "\n",
    "    transformer_gnn_epoch_losses, transformer_gnn_epoch_maes = train_model(model_transformer_gnn, optimizer_transformer_gnn, scheduler_transformer_gnn, loss_fn, num_epochs, num_batches_train, batched_input_train, batched_output_train, device)\n",
    "    save_model_data(model_transformer_gnn, optimizer_transformer_gnn, scheduler_transformer_gnn, transformer_gnn_epoch_losses, transformer_gnn_epoch_maes, hidden_channels, transformer_layers, new_iteration, 'model_transformer_gnn')\n",
    "    del model_transformer_gnn\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Channels: 27\n",
      "Features Channels: 69\n",
      "Output Channels: 67\n",
      "Edge Input Channels: 1\n",
      "Hidden Channels: 1024\n",
      "Transformer Layers: 8\n",
      "Epoch 1, Average Loss: 96.6826844215393, Average MAE: 4.9285076558589935, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 2, Average Loss: 27.552952527999878, Average MAE: 2.94597065448761, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 3, Average Loss: 19.35744857788086, Average MAE: 2.0946617871522903, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 4, Average Loss: 16.58505392074585, Average MAE: 1.6016438603401184, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 5, Average Loss: 15.319072484970093, Average MAE: 1.333859845995903, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 6, Average Loss: 14.795819282531738, Average MAE: 1.1632893830537796, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 7, Average Loss: 14.43172001838684, Average MAE: 1.0723362863063812, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 8, Average Loss: 14.443726062774658, Average MAE: 1.0078213140368462, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 9, Average Loss: 14.376783609390259, Average MAE: 0.9956315979361534, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 10, Average Loss: 14.345706582069397, Average MAE: 0.9818540513515472, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 11, Average Loss: 14.352799892425537, Average MAE: 0.9795335605740547, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 12, Average Loss: 14.347494840621948, Average MAE: 0.9751385226845741, Learning Rate: 0.01, Current Patience: 2\n",
      "Epoch 13, Average Loss: 14.35479724407196, Average MAE: 0.9733864292502403, Learning Rate: 0.01, Current Patience: 3\n",
      "Epoch 14, Average Loss: 14.357428669929504, Average MAE: 0.9712399691343307, Learning Rate: 0.01, Current Patience: 4\n",
      "Epoch 15, Average Loss: 14.358497381210327, Average MAE: 0.969842940568924, Learning Rate: 0.01, Current Patience: 5\n",
      "Epoch 16, Average Loss: 14.361891031265259, Average MAE: 0.9686789363622665, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 17, Average Loss: 14.332231163978577, Average MAE: 0.9663447961211205, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 18, Average Loss: 14.333376288414001, Average MAE: 0.9649154841899872, Learning Rate: 0.008, Current Patience: 1\n",
      "Epoch 19, Average Loss: 14.321081280708313, Average MAE: 0.9635272845625877, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 20, Average Loss: 14.331504702568054, Average MAE: 0.9626980721950531, Learning Rate: 0.008, Current Patience: 1\n",
      "Epoch 21, Average Loss: 14.331937551498413, Average MAE: 0.9619205594062805, Learning Rate: 0.008, Current Patience: 2\n",
      "Epoch 22, Average Loss: 14.329760909080505, Average MAE: 0.961011566221714, Learning Rate: 0.008, Current Patience: 3\n",
      "Epoch 23, Average Loss: 14.332993865013123, Average MAE: 0.9601082876324654, Learning Rate: 0.008, Current Patience: 4\n",
      "Epoch 24, Average Loss: 14.327906131744385, Average MAE: 0.9587684497237206, Learning Rate: 0.008, Current Patience: 5\n",
      "Epoch 25, Average Loss: 14.3375563621521, Average MAE: 0.9583152011036873, Learning Rate: 0.0064, Current Patience: 0\n",
      "Epoch 26, Average Loss: 14.310922265052795, Average MAE: 0.9558382034301758, Learning Rate: 0.0064, Current Patience: 0\n",
      "Epoch 27, Average Loss: 14.301440238952637, Average MAE: 0.9544580802321434, Learning Rate: 0.0064, Current Patience: 0\n",
      "Epoch 28, Average Loss: 14.307647109031677, Average MAE: 0.9537624418735504, Learning Rate: 0.0064, Current Patience: 1\n",
      "Epoch 29, Average Loss: 14.306328892707825, Average MAE: 0.95284453779459, Learning Rate: 0.0064, Current Patience: 2\n",
      "Epoch 30, Average Loss: 14.307034373283386, Average MAE: 0.9523341506719589, Learning Rate: 0.0064, Current Patience: 3\n",
      "Epoch 31, Average Loss: 14.310471653938293, Average MAE: 0.9516402930021286, Learning Rate: 0.0064, Current Patience: 4\n",
      "Epoch 32, Average Loss: 14.307923913002014, Average MAE: 0.9507758691906929, Learning Rate: 0.0064, Current Patience: 5\n",
      "Epoch 33, Average Loss: 14.306862115859985, Average MAE: 0.9496699795126915, Learning Rate: 0.00512, Current Patience: 0\n",
      "Epoch 34, Average Loss: 14.280282735824585, Average MAE: 0.9477690383791924, Learning Rate: 0.00512, Current Patience: 0\n",
      "Epoch 35, Average Loss: 14.28513240814209, Average MAE: 0.9469942077994347, Learning Rate: 0.00512, Current Patience: 1\n",
      "Epoch 36, Average Loss: 14.281794667243958, Average MAE: 0.9463115558028221, Learning Rate: 0.00512, Current Patience: 2\n",
      "Epoch 37, Average Loss: 14.28515613079071, Average MAE: 0.9458071663975716, Learning Rate: 0.00512, Current Patience: 3\n",
      "Epoch 38, Average Loss: 14.282551765441895, Average MAE: 0.9452801123261452, Learning Rate: 0.00512, Current Patience: 4\n",
      "Epoch 39, Average Loss: 14.284929513931274, Average MAE: 0.944650910794735, Learning Rate: 0.00512, Current Patience: 5\n",
      "Epoch 40, Average Loss: 14.281545996665955, Average MAE: 0.9439079686999321, Learning Rate: 0.004096000000000001, Current Patience: 0\n",
      "Epoch 41, Average Loss: 14.267454147338867, Average MAE: 0.942269392311573, Learning Rate: 0.004096000000000001, Current Patience: 0\n",
      "Epoch 42, Average Loss: 14.265897750854492, Average MAE: 0.941595271229744, Learning Rate: 0.004096000000000001, Current Patience: 0\n",
      "Epoch 43, Average Loss: 14.26499629020691, Average MAE: 0.9411806017160416, Learning Rate: 0.004096000000000001, Current Patience: 1\n",
      "Epoch 44, Average Loss: 14.26405668258667, Average MAE: 0.9404547661542892, Learning Rate: 0.004096000000000001, Current Patience: 0\n",
      "Epoch 45, Average Loss: 14.26244330406189, Average MAE: 0.9398469254374504, Learning Rate: 0.004096000000000001, Current Patience: 0\n",
      "Epoch 46, Average Loss: 14.268635272979736, Average MAE: 0.9395371973514557, Learning Rate: 0.004096000000000001, Current Patience: 1\n",
      "Epoch 47, Average Loss: 14.266175150871277, Average MAE: 0.9389918297529221, Learning Rate: 0.004096000000000001, Current Patience: 2\n",
      "Epoch 48, Average Loss: 14.264702677726746, Average MAE: 0.938390776515007, Learning Rate: 0.004096000000000001, Current Patience: 3\n",
      "Epoch 49, Average Loss: 14.26665449142456, Average MAE: 0.9379744380712509, Learning Rate: 0.004096000000000001, Current Patience: 4\n",
      "Epoch 50, Average Loss: 14.26584255695343, Average MAE: 0.9374951347708702, Learning Rate: 0.004096000000000001, Current Patience: 5\n",
      "Epoch 51, Average Loss: 14.264419078826904, Average MAE: 0.9367140233516693, Learning Rate: 0.0032768000000000007, Current Patience: 0\n",
      "Epoch 52, Average Loss: 14.251803636550903, Average MAE: 0.935464397072792, Learning Rate: 0.0032768000000000007, Current Patience: 0\n",
      "Epoch 53, Average Loss: 14.250579595565796, Average MAE: 0.9347918033599854, Learning Rate: 0.0032768000000000007, Current Patience: 1\n",
      "Epoch 54, Average Loss: 14.247738718986511, Average MAE: 0.9345004260540009, Learning Rate: 0.0032768000000000007, Current Patience: 0\n",
      "Epoch 55, Average Loss: 14.250861763954163, Average MAE: 0.9338797181844711, Learning Rate: 0.0032768000000000007, Current Patience: 1\n",
      "Epoch 56, Average Loss: 14.248974680900574, Average MAE: 0.9335688054561615, Learning Rate: 0.0032768000000000007, Current Patience: 2\n",
      "Epoch 57, Average Loss: 14.250720143318176, Average MAE: 0.9331898093223572, Learning Rate: 0.0032768000000000007, Current Patience: 3\n",
      "Epoch 58, Average Loss: 14.248713731765747, Average MAE: 0.9327163100242615, Learning Rate: 0.0032768000000000007, Current Patience: 4\n",
      "Epoch 59, Average Loss: 14.246655344963074, Average MAE: 0.9321710243821144, Learning Rate: 0.0032768000000000007, Current Patience: 5\n",
      "Epoch 60, Average Loss: 14.252560138702393, Average MAE: 0.9319842904806137, Learning Rate: 0.002621440000000001, Current Patience: 0\n",
      "Epoch 61, Average Loss: 14.237843036651611, Average MAE: 0.9308217391371727, Learning Rate: 0.002621440000000001, Current Patience: 0\n",
      "Epoch 62, Average Loss: 14.23541259765625, Average MAE: 0.9302252605557442, Learning Rate: 0.002621440000000001, Current Patience: 0\n",
      "Epoch 63, Average Loss: 14.23646855354309, Average MAE: 0.9297715649008751, Learning Rate: 0.002621440000000001, Current Patience: 1\n",
      "Epoch 64, Average Loss: 14.238430142402649, Average MAE: 0.9295955151319504, Learning Rate: 0.002621440000000001, Current Patience: 2\n",
      "Epoch 65, Average Loss: 14.235649108886719, Average MAE: 0.9290778562426567, Learning Rate: 0.002621440000000001, Current Patience: 3\n",
      "Epoch 66, Average Loss: 14.243321299552917, Average MAE: 0.9289756789803505, Learning Rate: 0.002621440000000001, Current Patience: 4\n",
      "Epoch 67, Average Loss: 14.243467450141907, Average MAE: 0.9287331253290176, Learning Rate: 0.002621440000000001, Current Patience: 5\n",
      "Epoch 68, Average Loss: 14.24373972415924, Average MAE: 0.9282208010554314, Learning Rate: 0.002097152000000001, Current Patience: 0\n",
      "Epoch 69, Average Loss: 14.231366395950317, Average MAE: 0.9274513274431229, Learning Rate: 0.002097152000000001, Current Patience: 0\n",
      "Epoch 70, Average Loss: 14.224040269851685, Average MAE: 0.9268178939819336, Learning Rate: 0.002097152000000001, Current Patience: 0\n",
      "Epoch 71, Average Loss: 14.22692322731018, Average MAE: 0.9267031475901604, Learning Rate: 0.002097152000000001, Current Patience: 1\n",
      "Epoch 72, Average Loss: 14.22348701953888, Average MAE: 0.9259625226259232, Learning Rate: 0.002097152000000001, Current Patience: 2\n",
      "Epoch 73, Average Loss: 14.22878885269165, Average MAE: 0.9261768013238907, Learning Rate: 0.002097152000000001, Current Patience: 3\n",
      "Epoch 74, Average Loss: 14.225108742713928, Average MAE: 0.9257165715098381, Learning Rate: 0.002097152000000001, Current Patience: 4\n",
      "Epoch 75, Average Loss: 14.23114550113678, Average MAE: 0.9255296215415001, Learning Rate: 0.002097152000000001, Current Patience: 5\n",
      "Epoch 76, Average Loss: 14.228891253471375, Average MAE: 0.9253778010606766, Learning Rate: 0.001677721600000001, Current Patience: 0\n",
      "Epoch 77, Average Loss: 14.221359729766846, Average MAE: 0.924503743648529, Learning Rate: 0.001677721600000001, Current Patience: 0\n",
      "Epoch 78, Average Loss: 14.222451448440552, Average MAE: 0.9243029430508614, Learning Rate: 0.001677721600000001, Current Patience: 1\n",
      "Epoch 79, Average Loss: 14.217687010765076, Average MAE: 0.9240216836333275, Learning Rate: 0.001677721600000001, Current Patience: 0\n",
      "Epoch 80, Average Loss: 14.22454559803009, Average MAE: 0.9242701530456543, Learning Rate: 0.001677721600000001, Current Patience: 1\n",
      "Epoch 81, Average Loss: 14.221154570579529, Average MAE: 0.9236755669116974, Learning Rate: 0.001677721600000001, Current Patience: 2\n",
      "Epoch 82, Average Loss: 14.220467805862427, Average MAE: 0.923467330634594, Learning Rate: 0.001677721600000001, Current Patience: 3\n",
      "Epoch 83, Average Loss: 14.219372391700745, Average MAE: 0.9232142567634583, Learning Rate: 0.001677721600000001, Current Patience: 4\n",
      "Epoch 84, Average Loss: 14.22393786907196, Average MAE: 0.9230619519948959, Learning Rate: 0.001677721600000001, Current Patience: 5\n",
      "Epoch 85, Average Loss: 14.222227692604065, Average MAE: 0.923019714653492, Learning Rate: 0.0013421772800000008, Current Patience: 0\n",
      "Epoch 86, Average Loss: 14.212752342224121, Average MAE: 0.922322653234005, Learning Rate: 0.0013421772800000008, Current Patience: 0\n",
      "Epoch 87, Average Loss: 14.21223497390747, Average MAE: 0.9220764860510826, Learning Rate: 0.0013421772800000008, Current Patience: 1\n",
      "Epoch 88, Average Loss: 14.214800715446472, Average MAE: 0.921955443918705, Learning Rate: 0.0013421772800000008, Current Patience: 2\n",
      "Epoch 89, Average Loss: 14.20957887172699, Average MAE: 0.921610951423645, Learning Rate: 0.0013421772800000008, Current Patience: 0\n",
      "Epoch 90, Average Loss: 14.215972185134888, Average MAE: 0.9217014014720917, Learning Rate: 0.0013421772800000008, Current Patience: 1\n",
      "Epoch 91, Average Loss: 14.211975336074829, Average MAE: 0.921242892742157, Learning Rate: 0.0013421772800000008, Current Patience: 2\n",
      "Epoch 92, Average Loss: 14.214491367340088, Average MAE: 0.9212218597531319, Learning Rate: 0.0013421772800000008, Current Patience: 3\n",
      "Epoch 93, Average Loss: 14.219263076782227, Average MAE: 0.9211932420730591, Learning Rate: 0.0013421772800000008, Current Patience: 4\n",
      "Epoch 94, Average Loss: 14.2177095413208, Average MAE: 0.9209874123334885, Learning Rate: 0.0013421772800000008, Current Patience: 5\n",
      "Epoch 95, Average Loss: 14.217399835586548, Average MAE: 0.9209001585841179, Learning Rate: 0.0010737418240000006, Current Patience: 0\n",
      "Epoch 96, Average Loss: 14.212126016616821, Average MAE: 0.9203942939639091, Learning Rate: 0.0010737418240000006, Current Patience: 1\n",
      "Epoch 97, Average Loss: 14.210032939910889, Average MAE: 0.9202072471380234, Learning Rate: 0.0010737418240000006, Current Patience: 2\n",
      "Epoch 98, Average Loss: 14.211941242218018, Average MAE: 0.9199794754385948, Learning Rate: 0.0010737418240000006, Current Patience: 3\n",
      "Epoch 99, Average Loss: 14.20851755142212, Average MAE: 0.9198650419712067, Learning Rate: 0.0010737418240000006, Current Patience: 4\n",
      "Epoch 100, Average Loss: 14.213232517242432, Average MAE: 0.9198947995901108, Learning Rate: 0.0010737418240000006, Current Patience: 5\n",
      "Epoch 101, Average Loss: 14.209134817123413, Average MAE: 0.9195332154631615, Learning Rate: 0.0008589934592000006, Current Patience: 0\n",
      "Epoch 102, Average Loss: 14.205666780471802, Average MAE: 0.9192771017551422, Learning Rate: 0.0008589934592000006, Current Patience: 0\n",
      "Epoch 103, Average Loss: 14.203347206115723, Average MAE: 0.9189897105097771, Learning Rate: 0.0008589934592000006, Current Patience: 0\n",
      "Epoch 104, Average Loss: 14.204016327857971, Average MAE: 0.9188800007104874, Learning Rate: 0.0008589934592000006, Current Patience: 1\n",
      "Epoch 105, Average Loss: 14.20689606666565, Average MAE: 0.9188814237713814, Learning Rate: 0.0008589934592000006, Current Patience: 2\n",
      "Epoch 106, Average Loss: 14.199466228485107, Average MAE: 0.9184413105249405, Learning Rate: 0.0008589934592000006, Current Patience: 0\n",
      "Epoch 107, Average Loss: 14.203681707382202, Average MAE: 0.9187625497579575, Learning Rate: 0.0008589934592000006, Current Patience: 1\n",
      "Epoch 108, Average Loss: 14.202523350715637, Average MAE: 0.918594129383564, Learning Rate: 0.0008589934592000006, Current Patience: 2\n",
      "Epoch 109, Average Loss: 14.20807659626007, Average MAE: 0.9185232818126678, Learning Rate: 0.0008589934592000006, Current Patience: 3\n",
      "Epoch 110, Average Loss: 14.205064535140991, Average MAE: 0.9182462319731712, Learning Rate: 0.0008589934592000006, Current Patience: 4\n",
      "Epoch 111, Average Loss: 14.202683568000793, Average MAE: 0.9180653393268585, Learning Rate: 0.0008589934592000006, Current Patience: 5\n",
      "Epoch 112, Average Loss: 14.202080249786377, Average MAE: 0.9178806245326996, Learning Rate: 0.0006871947673600005, Current Patience: 0\n",
      "Epoch 113, Average Loss: 14.199959874153137, Average MAE: 0.9176388829946518, Learning Rate: 0.0006871947673600005, Current Patience: 1\n",
      "Epoch 114, Average Loss: 14.19431447982788, Average MAE: 0.9175119996070862, Learning Rate: 0.0006871947673600005, Current Patience: 0\n",
      "Epoch 115, Average Loss: 14.202111601829529, Average MAE: 0.9176404029130936, Learning Rate: 0.0006871947673600005, Current Patience: 1\n",
      "Epoch 116, Average Loss: 14.201351523399353, Average MAE: 0.9175682440400124, Learning Rate: 0.0006871947673600005, Current Patience: 2\n",
      "Epoch 117, Average Loss: 14.203751564025879, Average MAE: 0.9174558818340302, Learning Rate: 0.0006871947673600005, Current Patience: 3\n",
      "Epoch 118, Average Loss: 14.198984026908875, Average MAE: 0.917224608361721, Learning Rate: 0.0006871947673600005, Current Patience: 4\n",
      "Epoch 119, Average Loss: 14.200238227844238, Average MAE: 0.9170889779925346, Learning Rate: 0.0006871947673600005, Current Patience: 5\n",
      "Epoch 120, Average Loss: 14.200722575187683, Average MAE: 0.9171194508671761, Learning Rate: 0.0005497558138880005, Current Patience: 0\n",
      "Epoch 121, Average Loss: 14.197695255279541, Average MAE: 0.9169044494628906, Learning Rate: 0.0005497558138880005, Current Patience: 1\n",
      "Epoch 122, Average Loss: 14.198187112808228, Average MAE: 0.9165876880288124, Learning Rate: 0.0005497558138880005, Current Patience: 2\n",
      "Epoch 123, Average Loss: 14.196689486503601, Average MAE: 0.9166893288493156, Learning Rate: 0.0005497558138880005, Current Patience: 3\n",
      "Epoch 124, Average Loss: 14.194545865058899, Average MAE: 0.916451558470726, Learning Rate: 0.0005497558138880005, Current Patience: 4\n",
      "Epoch 125, Average Loss: 14.2005056142807, Average MAE: 0.9165297150611877, Learning Rate: 0.0005497558138880005, Current Patience: 5\n",
      "Epoch 126, Average Loss: 14.19888174533844, Average MAE: 0.9163358584046364, Learning Rate: 0.0004398046511104004, Current Patience: 0\n",
      "Epoch 127, Average Loss: 14.195202231407166, Average MAE: 0.9163152053952217, Learning Rate: 0.0004398046511104004, Current Patience: 1\n",
      "Epoch 128, Average Loss: 14.194350838661194, Average MAE: 0.9160639643669128, Learning Rate: 0.0004398046511104004, Current Patience: 2\n",
      "Epoch 129, Average Loss: 14.19382655620575, Average MAE: 0.9160422459244728, Learning Rate: 0.0004398046511104004, Current Patience: 3\n",
      "Epoch 130, Average Loss: 14.195694208145142, Average MAE: 0.916035033762455, Learning Rate: 0.0004398046511104004, Current Patience: 4\n",
      "Epoch 131, Average Loss: 14.199954986572266, Average MAE: 0.9162208512425423, Learning Rate: 0.0004398046511104004, Current Patience: 5\n",
      "Epoch 132, Average Loss: 14.198412537574768, Average MAE: 0.9159101694822311, Learning Rate: 0.00035184372088832035, Current Patience: 0\n",
      "Epoch 133, Average Loss: 14.195690870285034, Average MAE: 0.9158035144209862, Learning Rate: 0.00035184372088832035, Current Patience: 1\n",
      "Epoch 134, Average Loss: 14.190582871437073, Average MAE: 0.9156045466661453, Learning Rate: 0.00035184372088832035, Current Patience: 0\n",
      "Epoch 135, Average Loss: 14.193257451057434, Average MAE: 0.9157799705862999, Learning Rate: 0.00035184372088832035, Current Patience: 1\n",
      "Epoch 136, Average Loss: 14.19074296951294, Average MAE: 0.9156284183263779, Learning Rate: 0.00035184372088832035, Current Patience: 2\n",
      "Epoch 137, Average Loss: 14.196146726608276, Average MAE: 0.9155892878770828, Learning Rate: 0.00035184372088832035, Current Patience: 3\n",
      "Epoch 138, Average Loss: 14.192677617073059, Average MAE: 0.9155318066477776, Learning Rate: 0.00035184372088832035, Current Patience: 4\n",
      "Epoch 139, Average Loss: 14.193715572357178, Average MAE: 0.9153597503900528, Learning Rate: 0.00035184372088832035, Current Patience: 5\n",
      "Epoch 140, Average Loss: 14.194600820541382, Average MAE: 0.9153309762477875, Learning Rate: 0.0002814749767106563, Current Patience: 0\n",
      "Epoch 141, Average Loss: 14.188914895057678, Average MAE: 0.9152169823646545, Learning Rate: 0.0002814749767106563, Current Patience: 0\n",
      "Epoch 142, Average Loss: 14.188948035240173, Average MAE: 0.9150552451610565, Learning Rate: 0.0002814749767106563, Current Patience: 1\n",
      "Epoch 143, Average Loss: 14.189877986907959, Average MAE: 0.9152096956968307, Learning Rate: 0.0002814749767106563, Current Patience: 2\n",
      "Epoch 144, Average Loss: 14.196679711341858, Average MAE: 0.9154641404747963, Learning Rate: 0.0002814749767106563, Current Patience: 3\n",
      "Epoch 145, Average Loss: 14.189956545829773, Average MAE: 0.9150288626551628, Learning Rate: 0.0002814749767106563, Current Patience: 4\n",
      "Epoch 146, Average Loss: 14.191418528556824, Average MAE: 0.9150193110108376, Learning Rate: 0.0002814749767106563, Current Patience: 5\n",
      "Epoch 147, Average Loss: 14.192696809768677, Average MAE: 0.9151478037238121, Learning Rate: 0.00022517998136852504, Current Patience: 0\n",
      "Epoch 148, Average Loss: 14.190094590187073, Average MAE: 0.9149085134267807, Learning Rate: 0.00022517998136852504, Current Patience: 1\n",
      "Epoch 149, Average Loss: 14.193291068077087, Average MAE: 0.9149097427725792, Learning Rate: 0.00022517998136852504, Current Patience: 2\n",
      "Epoch 150, Average Loss: 14.190038442611694, Average MAE: 0.9149423390626907, Learning Rate: 0.00022517998136852504, Current Patience: 3\n",
      "Epoch 151, Average Loss: 14.186719298362732, Average MAE: 0.914775013923645, Learning Rate: 0.00022517998136852504, Current Patience: 0\n",
      "Epoch 152, Average Loss: 14.186823606491089, Average MAE: 0.9145941585302353, Learning Rate: 0.00022517998136852504, Current Patience: 1\n",
      "Epoch 153, Average Loss: 14.189464807510376, Average MAE: 0.9147113859653473, Learning Rate: 0.00022517998136852504, Current Patience: 2\n",
      "Epoch 154, Average Loss: 14.189655661582947, Average MAE: 0.9146310389041901, Learning Rate: 0.00022517998136852504, Current Patience: 3\n",
      "Epoch 155, Average Loss: 14.189120531082153, Average MAE: 0.9145592153072357, Learning Rate: 0.00022517998136852504, Current Patience: 4\n",
      "Epoch 156, Average Loss: 14.187451124191284, Average MAE: 0.9144604057073593, Learning Rate: 0.00022517998136852504, Current Patience: 5\n",
      "Epoch 157, Average Loss: 14.192032933235168, Average MAE: 0.914601132273674, Learning Rate: 0.00018014398509482005, Current Patience: 0\n",
      "Epoch 158, Average Loss: 14.194501161575317, Average MAE: 0.9145530313253403, Learning Rate: 0.00018014398509482005, Current Patience: 1\n",
      "Epoch 159, Average Loss: 14.187807202339172, Average MAE: 0.9144091680645943, Learning Rate: 0.00018014398509482005, Current Patience: 2\n",
      "Epoch 160, Average Loss: 14.190601229667664, Average MAE: 0.9144683107733727, Learning Rate: 0.00018014398509482005, Current Patience: 3\n",
      "Epoch 161, Average Loss: 14.185662865638733, Average MAE: 0.9143632054328918, Learning Rate: 0.00018014398509482005, Current Patience: 4\n",
      "Epoch 162, Average Loss: 14.192836999893188, Average MAE: 0.9145496115088463, Learning Rate: 0.00018014398509482005, Current Patience: 5\n",
      "Epoch 163, Average Loss: 14.192944169044495, Average MAE: 0.914403609931469, Learning Rate: 0.00014411518807585605, Current Patience: 0\n",
      "Epoch 164, Average Loss: 14.189390659332275, Average MAE: 0.9141832292079926, Learning Rate: 0.00014411518807585605, Current Patience: 1\n",
      "Epoch 165, Average Loss: 14.18995463848114, Average MAE: 0.9143687710165977, Learning Rate: 0.00014411518807585605, Current Patience: 2\n",
      "Epoch 166, Average Loss: 14.195679426193237, Average MAE: 0.9143621176481247, Learning Rate: 0.00014411518807585605, Current Patience: 3\n",
      "Epoch 167, Average Loss: 14.191995859146118, Average MAE: 0.9143071398139, Learning Rate: 0.00014411518807585605, Current Patience: 4\n",
      "Epoch 168, Average Loss: 14.18750774860382, Average MAE: 0.9141673445701599, Learning Rate: 0.00014411518807585605, Current Patience: 5\n",
      "Epoch 169, Average Loss: 14.187605381011963, Average MAE: 0.9142124503850937, Learning Rate: 0.00011529215046068484, Current Patience: 0\n",
      "Epoch 170, Average Loss: 14.189488410949707, Average MAE: 0.9141558483242989, Learning Rate: 0.00011529215046068484, Current Patience: 1\n",
      "Epoch 171, Average Loss: 14.190376162528992, Average MAE: 0.9142009541392326, Learning Rate: 0.00011529215046068484, Current Patience: 2\n",
      "Epoch 172, Average Loss: 14.189428448677063, Average MAE: 0.9143093302845955, Learning Rate: 0.00011529215046068484, Current Patience: 3\n",
      "Epoch 173, Average Loss: 14.191153645515442, Average MAE: 0.9141386896371841, Learning Rate: 0.00011529215046068484, Current Patience: 4\n",
      "Epoch 174, Average Loss: 14.18851125240326, Average MAE: 0.9141039177775383, Learning Rate: 0.00011529215046068484, Current Patience: 5\n",
      "Epoch 175, Average Loss: 14.185651302337646, Average MAE: 0.9138811305165291, Learning Rate: 9.223372036854788e-05, Current Patience: 0\n",
      "Epoch 176, Average Loss: 14.190057873725891, Average MAE: 0.9139850437641144, Learning Rate: 9.223372036854788e-05, Current Patience: 1\n",
      "Epoch 177, Average Loss: 14.189837217330933, Average MAE: 0.9141833856701851, Learning Rate: 9.223372036854788e-05, Current Patience: 2\n",
      "Epoch 178, Average Loss: 14.18921446800232, Average MAE: 0.9139949381351471, Learning Rate: 9.223372036854788e-05, Current Patience: 3\n",
      "Epoch 179, Average Loss: 14.187599062919617, Average MAE: 0.9138041287660599, Learning Rate: 9.223372036854788e-05, Current Patience: 4\n",
      "Epoch 180, Average Loss: 14.191064596176147, Average MAE: 0.9139683321118355, Learning Rate: 9.223372036854788e-05, Current Patience: 5\n",
      "Epoch 181, Average Loss: 14.182798385620117, Average MAE: 0.9138390868902206, Learning Rate: 9.223372036854788e-05, Current Patience: 0\n",
      "Epoch 182, Average Loss: 14.188051581382751, Average MAE: 0.9138176068663597, Learning Rate: 9.223372036854788e-05, Current Patience: 1\n",
      "Epoch 183, Average Loss: 14.18467104434967, Average MAE: 0.9137795567512512, Learning Rate: 9.223372036854788e-05, Current Patience: 2\n",
      "Epoch 184, Average Loss: 14.18443250656128, Average MAE: 0.9137082397937775, Learning Rate: 9.223372036854788e-05, Current Patience: 3\n",
      "Epoch 185, Average Loss: 14.187996625900269, Average MAE: 0.9139076843857765, Learning Rate: 9.223372036854788e-05, Current Patience: 4\n",
      "Epoch 186, Average Loss: 14.188621759414673, Average MAE: 0.9138354659080505, Learning Rate: 9.223372036854788e-05, Current Patience: 5\n",
      "Epoch 187, Average Loss: 14.189143538475037, Average MAE: 0.9138600155711174, Learning Rate: 7.378697629483831e-05, Current Patience: 0\n",
      "Epoch 188, Average Loss: 14.186189413070679, Average MAE: 0.9137368276715279, Learning Rate: 7.378697629483831e-05, Current Patience: 1\n",
      "Epoch 189, Average Loss: 14.183433413505554, Average MAE: 0.913682833313942, Learning Rate: 7.378697629483831e-05, Current Patience: 2\n",
      "Epoch 190, Average Loss: 14.183752179145813, Average MAE: 0.9136910960078239, Learning Rate: 7.378697629483831e-05, Current Patience: 3\n",
      "Epoch 191, Average Loss: 14.187387824058533, Average MAE: 0.9137121364474297, Learning Rate: 7.378697629483831e-05, Current Patience: 4\n",
      "Epoch 192, Average Loss: 14.191697716712952, Average MAE: 0.9137971550226212, Learning Rate: 7.378697629483831e-05, Current Patience: 5\n",
      "Epoch 193, Average Loss: 14.188410878181458, Average MAE: 0.9136752635240555, Learning Rate: 5.902958103587065e-05, Current Patience: 0\n",
      "Epoch 194, Average Loss: 14.184187173843384, Average MAE: 0.913634791970253, Learning Rate: 5.902958103587065e-05, Current Patience: 1\n",
      "Epoch 195, Average Loss: 14.188099980354309, Average MAE: 0.9137635678052902, Learning Rate: 5.902958103587065e-05, Current Patience: 2\n",
      "Epoch 196, Average Loss: 14.18562400341034, Average MAE: 0.9138048365712166, Learning Rate: 5.902958103587065e-05, Current Patience: 3\n",
      "Epoch 197, Average Loss: 14.18763017654419, Average MAE: 0.9137480929493904, Learning Rate: 5.902958103587065e-05, Current Patience: 4\n",
      "Epoch 198, Average Loss: 14.187935590744019, Average MAE: 0.9136053249239922, Learning Rate: 5.902958103587065e-05, Current Patience: 5\n",
      "Epoch 199, Average Loss: 14.190232515335083, Average MAE: 0.9137671142816544, Learning Rate: 4.722366482869652e-05, Current Patience: 0\n",
      "Epoch 200, Average Loss: 14.189531803131104, Average MAE: 0.91362614184618, Learning Rate: 4.722366482869652e-05, Current Patience: 1\n",
      "Model data saved in saved_models_20241212_3/model_gnn_transformer_20241212_094055\n"
     ]
    }
   ],
   "source": [
    "# Toggle to run the code\n",
    "if run_model_gnn_transformer:\n",
    "    # Define the parameters\n",
    "    in_channels = batched_input_train.shape[2]\n",
    "    features_channels = batched_input_train.shape[3]\n",
    "    out_channels = batched_output_train.shape[3]  \n",
    "    edge_in_channels = 1\n",
    "    transformer_layers_gnn_transformer = 4\n",
    "\n",
    "    # Print the parameters to verify\n",
    "    print(\"Input Channels:\", in_channels)\n",
    "    print(\"Features Channels:\", features_channels)\n",
    "    print(\"Output Channels:\", out_channels)\n",
    "    print(\"Edge Input Channels:\", edge_in_channels)\n",
    "    print(\"Hidden Channels:\", hidden_channels)\n",
    "    print(\"Transformer Layers:\", transformer_layers)\n",
    "\n",
    "    model_gnn_transformer = HybridModel_GNN(in_channels, out_channels, features_channels, edge_in_channels, hidden_channels, transformer_layers_gnn_transformer).to(device)\n",
    "\n",
    "    # Define the optimizer and loss function for model_gnn_transformer\n",
    "    optimizer_gnn_transformer = torch.optim.Adam(model_gnn_transformer.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    scheduler_gnn_transformer = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_gnn_transformer, mode=scheduler_mode, factor=scheduler_factor, patience=scheduler_patience, verbose=True, min_lr=1e-5)\n",
    "\n",
    "    gnn_transformer_epoch_losses, gnn_transformer_epoch_maes = train_model(model_gnn_transformer, optimizer_gnn_transformer, scheduler_gnn_transformer, loss_fn, num_epochs, num_batches_train, batched_input_train, batched_output_train, device)\n",
    "    save_model_data(model_gnn_transformer, optimizer_gnn_transformer, scheduler_gnn_transformer, gnn_transformer_epoch_losses, gnn_transformer_epoch_maes, hidden_channels, transformer_layers_gnn_transformer, new_iteration, 'model_gnn_transformer')\n",
    "    del model_gnn_transformer\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Channels: 27\n",
      "Features Channels: 69\n",
      "Output Channels: 67\n",
      "Edge Input Channels: 1\n",
      "Hidden Channels: 1024\n",
      "Transformer Layers: 8\n",
      "Epoch 1, Average Loss: 197.56188774108887, Average MAE: 4.567760944366455, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 2, Average Loss: 151.38161373138428, Average MAE: 3.8086769580841064, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 3, Average Loss: 103.28258895874023, Average MAE: 2.9982439279556274, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 4, Average Loss: 60.08474922180176, Average MAE: 2.0171632021665573, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 5, Average Loss: 33.14969563484192, Average MAE: 1.452488273382187, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 6, Average Loss: 21.423484325408936, Average MAE: 1.312433585524559, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 7, Average Loss: 16.47177278995514, Average MAE: 1.0908333882689476, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 8, Average Loss: 14.863280892372131, Average MAE: 1.0079383179545403, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 9, Average Loss: 14.430396795272827, Average MAE: 0.9952389225363731, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 10, Average Loss: 14.31345009803772, Average MAE: 0.9562526792287827, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 11, Average Loss: 14.289058089256287, Average MAE: 0.9487051442265511, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 12, Average Loss: 14.276817917823792, Average MAE: 0.938070498406887, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 13, Average Loss: 14.266309142112732, Average MAE: 0.9319088011980057, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 14, Average Loss: 14.250822305679321, Average MAE: 0.9264420717954636, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 15, Average Loss: 14.247989535331726, Average MAE: 0.9227241799235344, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 16, Average Loss: 14.24205470085144, Average MAE: 0.9163257628679276, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 17, Average Loss: 14.252979278564453, Average MAE: 0.9131721556186676, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 18, Average Loss: 14.236315369606018, Average MAE: 0.9085120037198067, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 19, Average Loss: 14.236226320266724, Average MAE: 0.905450351536274, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 20, Average Loss: 14.236676692962646, Average MAE: 0.9031423330307007, Learning Rate: 0.01, Current Patience: 2\n",
      "Epoch 21, Average Loss: 14.227991819381714, Average MAE: 0.9010701403021812, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 22, Average Loss: 14.226210117340088, Average MAE: 0.8989201262593269, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 23, Average Loss: 14.236506581306458, Average MAE: 0.8977388888597488, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 24, Average Loss: 14.242125034332275, Average MAE: 0.898249626159668, Learning Rate: 0.01, Current Patience: 2\n",
      "Epoch 25, Average Loss: 14.222037196159363, Average MAE: 0.8959677666425705, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 26, Average Loss: 14.230553388595581, Average MAE: 0.8937786519527435, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 27, Average Loss: 14.224294781684875, Average MAE: 0.8924004882574081, Learning Rate: 0.01, Current Patience: 2\n",
      "Epoch 28, Average Loss: 14.226199507713318, Average MAE: 0.8909015208482742, Learning Rate: 0.01, Current Patience: 3\n",
      "Epoch 29, Average Loss: 14.232614874839783, Average MAE: 0.8909059539437294, Learning Rate: 0.01, Current Patience: 4\n",
      "Epoch 30, Average Loss: 14.219919681549072, Average MAE: 0.8899190351366997, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 31, Average Loss: 14.221329092979431, Average MAE: 0.8886412605643272, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 32, Average Loss: 14.2196946144104, Average MAE: 0.88828956335783, Learning Rate: 0.01, Current Patience: 2\n",
      "Epoch 33, Average Loss: 14.220407247543335, Average MAE: 0.8874817937612534, Learning Rate: 0.01, Current Patience: 3\n",
      "Epoch 34, Average Loss: 14.22152042388916, Average MAE: 0.8870411366224289, Learning Rate: 0.01, Current Patience: 4\n",
      "Epoch 35, Average Loss: 14.2262122631073, Average MAE: 0.8870229423046112, Learning Rate: 0.01, Current Patience: 5\n",
      "Epoch 36, Average Loss: 14.22146463394165, Average MAE: 0.8862229436635971, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 37, Average Loss: 14.21124005317688, Average MAE: 0.8843891248106956, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 38, Average Loss: 14.21071207523346, Average MAE: 0.8834501281380653, Learning Rate: 0.008, Current Patience: 1\n",
      "Epoch 39, Average Loss: 14.208547592163086, Average MAE: 0.8829453513026237, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 40, Average Loss: 14.20786452293396, Average MAE: 0.882566936314106, Learning Rate: 0.008, Current Patience: 1\n",
      "Epoch 41, Average Loss: 14.210506439208984, Average MAE: 0.8824470862746239, Learning Rate: 0.008, Current Patience: 2\n",
      "Epoch 42, Average Loss: 14.210843682289124, Average MAE: 0.881903350353241, Learning Rate: 0.008, Current Patience: 3\n",
      "Epoch 43, Average Loss: 14.20923364162445, Average MAE: 0.8817431032657623, Learning Rate: 0.008, Current Patience: 4\n",
      "Epoch 44, Average Loss: 14.206898093223572, Average MAE: 0.8813946396112442, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 45, Average Loss: 14.210753798484802, Average MAE: 0.8812816217541695, Learning Rate: 0.008, Current Patience: 1\n",
      "Epoch 46, Average Loss: 14.21316373348236, Average MAE: 0.8812102153897285, Learning Rate: 0.008, Current Patience: 2\n",
      "Epoch 47, Average Loss: 14.209136724472046, Average MAE: 0.8811608403921127, Learning Rate: 0.008, Current Patience: 3\n",
      "Epoch 48, Average Loss: 14.209033966064453, Average MAE: 0.8807134851813316, Learning Rate: 0.008, Current Patience: 4\n",
      "Epoch 49, Average Loss: 14.209167718887329, Average MAE: 0.8807505294680595, Learning Rate: 0.008, Current Patience: 5\n",
      "Epoch 50, Average Loss: 14.206926345825195, Average MAE: 0.880757600069046, Learning Rate: 0.0064, Current Patience: 0\n",
      "Epoch 51, Average Loss: 14.198822617530823, Average MAE: 0.879345066845417, Learning Rate: 0.0064, Current Patience: 0\n",
      "Epoch 52, Average Loss: 14.198945879936218, Average MAE: 0.8791946470737457, Learning Rate: 0.0064, Current Patience: 1\n",
      "Epoch 53, Average Loss: 14.197442531585693, Average MAE: 0.8788141012191772, Learning Rate: 0.0064, Current Patience: 2\n",
      "Epoch 54, Average Loss: 14.204340815544128, Average MAE: 0.8788715898990631, Learning Rate: 0.0064, Current Patience: 3\n",
      "Epoch 55, Average Loss: 14.202806234359741, Average MAE: 0.87872364372015, Learning Rate: 0.0064, Current Patience: 4\n",
      "Epoch 56, Average Loss: 14.20033574104309, Average MAE: 0.8786355629563332, Learning Rate: 0.0064, Current Patience: 5\n",
      "Epoch 57, Average Loss: 14.201016902923584, Average MAE: 0.8786015510559082, Learning Rate: 0.00512, Current Patience: 0\n",
      "Epoch 58, Average Loss: 14.19424557685852, Average MAE: 0.8779933303594589, Learning Rate: 0.00512, Current Patience: 0\n",
      "Epoch 59, Average Loss: 14.195339560508728, Average MAE: 0.8779031336307526, Learning Rate: 0.00512, Current Patience: 1\n",
      "Epoch 60, Average Loss: 14.196092009544373, Average MAE: 0.8778425380587578, Learning Rate: 0.00512, Current Patience: 2\n",
      "Epoch 61, Average Loss: 14.192775249481201, Average MAE: 0.8776064589619637, Learning Rate: 0.00512, Current Patience: 0\n",
      "Epoch 62, Average Loss: 14.19291353225708, Average MAE: 0.8775419965386391, Learning Rate: 0.00512, Current Patience: 1\n",
      "Epoch 63, Average Loss: 14.196045637130737, Average MAE: 0.8775815367698669, Learning Rate: 0.00512, Current Patience: 2\n",
      "Epoch 64, Average Loss: 14.193870782852173, Average MAE: 0.8774088770151138, Learning Rate: 0.00512, Current Patience: 3\n",
      "Epoch 65, Average Loss: 14.196681261062622, Average MAE: 0.8772424384951591, Learning Rate: 0.00512, Current Patience: 4\n",
      "Epoch 66, Average Loss: 14.196317911148071, Average MAE: 0.8772912546992302, Learning Rate: 0.00512, Current Patience: 5\n",
      "Epoch 67, Average Loss: 14.194996356964111, Average MAE: 0.8772021308541298, Learning Rate: 0.004096000000000001, Current Patience: 0\n",
      "Epoch 68, Average Loss: 14.187904834747314, Average MAE: 0.8765514194965363, Learning Rate: 0.004096000000000001, Current Patience: 0\n",
      "Epoch 69, Average Loss: 14.190373539924622, Average MAE: 0.8766299560666084, Learning Rate: 0.004096000000000001, Current Patience: 1\n",
      "Epoch 70, Average Loss: 14.190221190452576, Average MAE: 0.8767335340380669, Learning Rate: 0.004096000000000001, Current Patience: 2\n",
      "Epoch 71, Average Loss: 14.191447973251343, Average MAE: 0.8768638670444489, Learning Rate: 0.004096000000000001, Current Patience: 3\n",
      "Epoch 72, Average Loss: 14.191154837608337, Average MAE: 0.8769944831728935, Learning Rate: 0.004096000000000001, Current Patience: 4\n",
      "Epoch 73, Average Loss: 14.188427925109863, Average MAE: 0.8767969161272049, Learning Rate: 0.004096000000000001, Current Patience: 5\n",
      "Epoch 74, Average Loss: 14.187897086143494, Average MAE: 0.8767974227666855, Learning Rate: 0.0032768000000000007, Current Patience: 0\n",
      "Epoch 75, Average Loss: 14.186820149421692, Average MAE: 0.8762102350592613, Learning Rate: 0.0032768000000000007, Current Patience: 1\n",
      "Epoch 76, Average Loss: 14.187801837921143, Average MAE: 0.8762728124856949, Learning Rate: 0.0032768000000000007, Current Patience: 2\n",
      "Epoch 77, Average Loss: 14.186442017555237, Average MAE: 0.8757940828800201, Learning Rate: 0.0032768000000000007, Current Patience: 0\n",
      "Epoch 78, Average Loss: 14.188687920570374, Average MAE: 0.8757367953658104, Learning Rate: 0.0032768000000000007, Current Patience: 1\n",
      "Epoch 79, Average Loss: 14.187650680541992, Average MAE: 0.8755339086055756, Learning Rate: 0.0032768000000000007, Current Patience: 2\n",
      "Epoch 80, Average Loss: 14.188775181770325, Average MAE: 0.8754634857177734, Learning Rate: 0.0032768000000000007, Current Patience: 3\n",
      "Epoch 81, Average Loss: 14.188532829284668, Average MAE: 0.8754488825798035, Learning Rate: 0.0032768000000000007, Current Patience: 4\n",
      "Epoch 82, Average Loss: 14.189162015914917, Average MAE: 0.8752178177237511, Learning Rate: 0.0032768000000000007, Current Patience: 5\n",
      "Epoch 83, Average Loss: 14.187156915664673, Average MAE: 0.875177338719368, Learning Rate: 0.002621440000000001, Current Patience: 0\n",
      "Epoch 84, Average Loss: 14.183563590049744, Average MAE: 0.8748510777950287, Learning Rate: 0.002621440000000001, Current Patience: 0\n",
      "Epoch 85, Average Loss: 14.18290364742279, Average MAE: 0.875066876411438, Learning Rate: 0.002621440000000001, Current Patience: 1\n",
      "Epoch 86, Average Loss: 14.18258512020111, Average MAE: 0.8750083595514297, Learning Rate: 0.002621440000000001, Current Patience: 2\n",
      "Epoch 87, Average Loss: 14.184063911437988, Average MAE: 0.8748332485556602, Learning Rate: 0.002621440000000001, Current Patience: 3\n",
      "Epoch 88, Average Loss: 14.182411789894104, Average MAE: 0.8747467324137688, Learning Rate: 0.002621440000000001, Current Patience: 4\n",
      "Epoch 89, Average Loss: 14.182984590530396, Average MAE: 0.874765045940876, Learning Rate: 0.002621440000000001, Current Patience: 5\n",
      "Epoch 90, Average Loss: 14.181893467903137, Average MAE: 0.8747439607977867, Learning Rate: 0.002621440000000001, Current Patience: 0\n",
      "Epoch 91, Average Loss: 14.183634281158447, Average MAE: 0.8748142868280411, Learning Rate: 0.002621440000000001, Current Patience: 1\n",
      "Epoch 92, Average Loss: 14.181710600852966, Average MAE: 0.874739445745945, Learning Rate: 0.002621440000000001, Current Patience: 2\n",
      "Epoch 93, Average Loss: 14.18268620967865, Average MAE: 0.8746997341513634, Learning Rate: 0.002621440000000001, Current Patience: 3\n",
      "Epoch 94, Average Loss: 14.182549357414246, Average MAE: 0.8747410774230957, Learning Rate: 0.002621440000000001, Current Patience: 4\n",
      "Epoch 95, Average Loss: 14.183705449104309, Average MAE: 0.8745944574475288, Learning Rate: 0.002621440000000001, Current Patience: 5\n",
      "Epoch 96, Average Loss: 14.182279109954834, Average MAE: 0.8743755668401718, Learning Rate: 0.002097152000000001, Current Patience: 0\n",
      "Epoch 97, Average Loss: 14.180273056030273, Average MAE: 0.8743577376008034, Learning Rate: 0.002097152000000001, Current Patience: 0\n",
      "Epoch 98, Average Loss: 14.178852438926697, Average MAE: 0.8745925277471542, Learning Rate: 0.002097152000000001, Current Patience: 0\n",
      "Epoch 99, Average Loss: 14.18027138710022, Average MAE: 0.8746990859508514, Learning Rate: 0.002097152000000001, Current Patience: 1\n",
      "Epoch 100, Average Loss: 14.177950859069824, Average MAE: 0.8746722489595413, Learning Rate: 0.002097152000000001, Current Patience: 2\n",
      "Epoch 101, Average Loss: 14.1809823513031, Average MAE: 0.874832957983017, Learning Rate: 0.002097152000000001, Current Patience: 3\n",
      "Epoch 102, Average Loss: 14.182812690734863, Average MAE: 0.8747817352414131, Learning Rate: 0.002097152000000001, Current Patience: 4\n",
      "Epoch 103, Average Loss: 14.17872679233551, Average MAE: 0.8745891973376274, Learning Rate: 0.002097152000000001, Current Patience: 5\n",
      "Epoch 104, Average Loss: 14.181016087532043, Average MAE: 0.8746571987867355, Learning Rate: 0.001677721600000001, Current Patience: 0\n",
      "Epoch 105, Average Loss: 14.176002264022827, Average MAE: 0.8744056150317192, Learning Rate: 0.001677721600000001, Current Patience: 0\n",
      "Epoch 106, Average Loss: 14.178650617599487, Average MAE: 0.8747333288192749, Learning Rate: 0.001677721600000001, Current Patience: 1\n",
      "Epoch 107, Average Loss: 14.178825616836548, Average MAE: 0.8748797252774239, Learning Rate: 0.001677721600000001, Current Patience: 2\n",
      "Epoch 108, Average Loss: 14.175118684768677, Average MAE: 0.8749001622200012, Learning Rate: 0.001677721600000001, Current Patience: 3\n",
      "Epoch 109, Average Loss: 14.17940354347229, Average MAE: 0.875059612095356, Learning Rate: 0.001677721600000001, Current Patience: 4\n",
      "Epoch 110, Average Loss: 14.179680705070496, Average MAE: 0.8751184865832329, Learning Rate: 0.001677721600000001, Current Patience: 5\n",
      "Epoch 111, Average Loss: 14.178792476654053, Average MAE: 0.875004917383194, Learning Rate: 0.0013421772800000008, Current Patience: 0\n",
      "Epoch 112, Average Loss: 14.177235126495361, Average MAE: 0.8747036904096603, Learning Rate: 0.0013421772800000008, Current Patience: 1\n",
      "Epoch 113, Average Loss: 14.178551077842712, Average MAE: 0.8749585673213005, Learning Rate: 0.0013421772800000008, Current Patience: 2\n",
      "Epoch 114, Average Loss: 14.174350500106812, Average MAE: 0.8746965378522873, Learning Rate: 0.0013421772800000008, Current Patience: 0\n",
      "Epoch 115, Average Loss: 14.173836350440979, Average MAE: 0.8745143711566925, Learning Rate: 0.0013421772800000008, Current Patience: 1\n",
      "Epoch 116, Average Loss: 14.176851153373718, Average MAE: 0.8743970543146133, Learning Rate: 0.0013421772800000008, Current Patience: 2\n",
      "Epoch 117, Average Loss: 14.176820278167725, Average MAE: 0.8743717968463898, Learning Rate: 0.0013421772800000008, Current Patience: 3\n",
      "Epoch 118, Average Loss: 14.175714015960693, Average MAE: 0.8742268085479736, Learning Rate: 0.0013421772800000008, Current Patience: 4\n",
      "Epoch 119, Average Loss: 14.17716133594513, Average MAE: 0.8741875886917114, Learning Rate: 0.0013421772800000008, Current Patience: 5\n",
      "Epoch 120, Average Loss: 14.174996376037598, Average MAE: 0.8741707652807236, Learning Rate: 0.0010737418240000006, Current Patience: 0\n",
      "Epoch 121, Average Loss: 14.17443573474884, Average MAE: 0.873996414244175, Learning Rate: 0.0010737418240000006, Current Patience: 1\n",
      "Epoch 122, Average Loss: 14.176186561584473, Average MAE: 0.8737664073705673, Learning Rate: 0.0010737418240000006, Current Patience: 2\n",
      "Epoch 123, Average Loss: 14.176973819732666, Average MAE: 0.8735775798559189, Learning Rate: 0.0010737418240000006, Current Patience: 3\n",
      "Epoch 124, Average Loss: 14.175299525260925, Average MAE: 0.8734735772013664, Learning Rate: 0.0010737418240000006, Current Patience: 4\n",
      "Epoch 125, Average Loss: 14.17459237575531, Average MAE: 0.8734730929136276, Learning Rate: 0.0010737418240000006, Current Patience: 5\n",
      "Epoch 126, Average Loss: 14.175013184547424, Average MAE: 0.8734415397047997, Learning Rate: 0.0008589934592000006, Current Patience: 0\n",
      "Epoch 127, Average Loss: 14.17477011680603, Average MAE: 0.8732940927147865, Learning Rate: 0.0008589934592000006, Current Patience: 1\n",
      "Epoch 128, Average Loss: 14.172895312309265, Average MAE: 0.8730957135558128, Learning Rate: 0.0008589934592000006, Current Patience: 0\n",
      "Epoch 129, Average Loss: 14.174607276916504, Average MAE: 0.8731162399053574, Learning Rate: 0.0008589934592000006, Current Patience: 1\n",
      "Epoch 130, Average Loss: 14.176396012306213, Average MAE: 0.873168095946312, Learning Rate: 0.0008589934592000006, Current Patience: 2\n",
      "Epoch 131, Average Loss: 14.173051834106445, Average MAE: 0.8730683699250221, Learning Rate: 0.0008589934592000006, Current Patience: 3\n",
      "Epoch 132, Average Loss: 14.17364752292633, Average MAE: 0.8731071799993515, Learning Rate: 0.0008589934592000006, Current Patience: 4\n",
      "Epoch 133, Average Loss: 14.173805594444275, Average MAE: 0.8730143010616302, Learning Rate: 0.0008589934592000006, Current Patience: 5\n",
      "Epoch 134, Average Loss: 14.172008156776428, Average MAE: 0.8728938326239586, Learning Rate: 0.0006871947673600005, Current Patience: 0\n",
      "Epoch 135, Average Loss: 14.17312741279602, Average MAE: 0.872832715511322, Learning Rate: 0.0006871947673600005, Current Patience: 1\n",
      "Epoch 136, Average Loss: 14.173300981521606, Average MAE: 0.872757263481617, Learning Rate: 0.0006871947673600005, Current Patience: 2\n",
      "Epoch 137, Average Loss: 14.1702721118927, Average MAE: 0.8726560100913048, Learning Rate: 0.0006871947673600005, Current Patience: 0\n",
      "Epoch 138, Average Loss: 14.171696662902832, Average MAE: 0.8726333975791931, Learning Rate: 0.0006871947673600005, Current Patience: 1\n",
      "Epoch 139, Average Loss: 14.17362916469574, Average MAE: 0.8728310018777847, Learning Rate: 0.0006871947673600005, Current Patience: 2\n",
      "Epoch 140, Average Loss: 14.172590136528015, Average MAE: 0.8727232739329338, Learning Rate: 0.0006871947673600005, Current Patience: 3\n",
      "Epoch 141, Average Loss: 14.172024011611938, Average MAE: 0.8727218359708786, Learning Rate: 0.0006871947673600005, Current Patience: 4\n",
      "Epoch 142, Average Loss: 14.174071431159973, Average MAE: 0.8728326931595802, Learning Rate: 0.0006871947673600005, Current Patience: 5\n",
      "Epoch 143, Average Loss: 14.173064827919006, Average MAE: 0.8727093264460564, Learning Rate: 0.0005497558138880005, Current Patience: 0\n",
      "Epoch 144, Average Loss: 14.172816514968872, Average MAE: 0.8725810497999191, Learning Rate: 0.0005497558138880005, Current Patience: 1\n",
      "Epoch 145, Average Loss: 14.174221515655518, Average MAE: 0.8726382330060005, Learning Rate: 0.0005497558138880005, Current Patience: 2\n",
      "Epoch 146, Average Loss: 14.171725869178772, Average MAE: 0.872504323720932, Learning Rate: 0.0005497558138880005, Current Patience: 3\n",
      "Epoch 147, Average Loss: 14.171954035758972, Average MAE: 0.8724609464406967, Learning Rate: 0.0005497558138880005, Current Patience: 4\n",
      "Epoch 148, Average Loss: 14.17287790775299, Average MAE: 0.8725489601492882, Learning Rate: 0.0005497558138880005, Current Patience: 5\n",
      "Epoch 149, Average Loss: 14.17190170288086, Average MAE: 0.8725337460637093, Learning Rate: 0.0004398046511104004, Current Patience: 0\n",
      "Epoch 150, Average Loss: 14.171549439430237, Average MAE: 0.8724473938345909, Learning Rate: 0.0004398046511104004, Current Patience: 1\n",
      "Epoch 151, Average Loss: 14.171250820159912, Average MAE: 0.8723435029387474, Learning Rate: 0.0004398046511104004, Current Patience: 2\n",
      "Epoch 152, Average Loss: 14.168989539146423, Average MAE: 0.8723368048667908, Learning Rate: 0.0004398046511104004, Current Patience: 3\n",
      "Epoch 153, Average Loss: 14.171171426773071, Average MAE: 0.8724476620554924, Learning Rate: 0.0004398046511104004, Current Patience: 4\n",
      "Epoch 154, Average Loss: 14.171216249465942, Average MAE: 0.8723491653800011, Learning Rate: 0.0004398046511104004, Current Patience: 5\n",
      "Epoch 155, Average Loss: 14.170992732048035, Average MAE: 0.8723990693688393, Learning Rate: 0.00035184372088832035, Current Patience: 0\n",
      "Epoch 156, Average Loss: 14.170018553733826, Average MAE: 0.8722998276352882, Learning Rate: 0.00035184372088832035, Current Patience: 1\n",
      "Epoch 157, Average Loss: 14.169772386550903, Average MAE: 0.8722810670733452, Learning Rate: 0.00035184372088832035, Current Patience: 2\n",
      "Epoch 158, Average Loss: 14.170967102050781, Average MAE: 0.8723529875278473, Learning Rate: 0.00035184372088832035, Current Patience: 3\n",
      "Epoch 159, Average Loss: 14.171377420425415, Average MAE: 0.8723802343010902, Learning Rate: 0.00035184372088832035, Current Patience: 4\n",
      "Epoch 160, Average Loss: 14.169846534729004, Average MAE: 0.8723360821604729, Learning Rate: 0.00035184372088832035, Current Patience: 5\n",
      "Epoch 161, Average Loss: 14.167519807815552, Average MAE: 0.8722615391016006, Learning Rate: 0.00035184372088832035, Current Patience: 0\n",
      "Epoch 162, Average Loss: 14.1732679605484, Average MAE: 0.8724770694971085, Learning Rate: 0.00035184372088832035, Current Patience: 1\n",
      "Epoch 163, Average Loss: 14.172718286514282, Average MAE: 0.8724429085850716, Learning Rate: 0.00035184372088832035, Current Patience: 2\n",
      "Epoch 164, Average Loss: 14.172616481781006, Average MAE: 0.8724605515599251, Learning Rate: 0.00035184372088832035, Current Patience: 3\n",
      "Epoch 165, Average Loss: 14.171180725097656, Average MAE: 0.8723198622465134, Learning Rate: 0.00035184372088832035, Current Patience: 4\n",
      "Epoch 166, Average Loss: 14.172836184501648, Average MAE: 0.8723464012145996, Learning Rate: 0.00035184372088832035, Current Patience: 5\n",
      "Epoch 167, Average Loss: 14.170745968818665, Average MAE: 0.872294045984745, Learning Rate: 0.0002814749767106563, Current Patience: 0\n",
      "Epoch 168, Average Loss: 14.170490980148315, Average MAE: 0.8723288476467133, Learning Rate: 0.0002814749767106563, Current Patience: 1\n",
      "Epoch 169, Average Loss: 14.169833183288574, Average MAE: 0.8721869811415672, Learning Rate: 0.0002814749767106563, Current Patience: 2\n",
      "Epoch 170, Average Loss: 14.169482707977295, Average MAE: 0.872218407690525, Learning Rate: 0.0002814749767106563, Current Patience: 3\n",
      "Epoch 171, Average Loss: 14.169601321220398, Average MAE: 0.8721923679113388, Learning Rate: 0.0002814749767106563, Current Patience: 4\n",
      "Epoch 172, Average Loss: 14.172053217887878, Average MAE: 0.8722812458872795, Learning Rate: 0.0002814749767106563, Current Patience: 5\n",
      "Epoch 173, Average Loss: 14.171362280845642, Average MAE: 0.8723138049244881, Learning Rate: 0.00022517998136852504, Current Patience: 0\n",
      "Epoch 174, Average Loss: 14.168284893035889, Average MAE: 0.872133694589138, Learning Rate: 0.00022517998136852504, Current Patience: 1\n",
      "Epoch 175, Average Loss: 14.167744517326355, Average MAE: 0.8720745518803596, Learning Rate: 0.00022517998136852504, Current Patience: 2\n",
      "Epoch 176, Average Loss: 14.172216176986694, Average MAE: 0.8721935749053955, Learning Rate: 0.00022517998136852504, Current Patience: 3\n",
      "Epoch 177, Average Loss: 14.171706438064575, Average MAE: 0.8722198009490967, Learning Rate: 0.00022517998136852504, Current Patience: 4\n",
      "Epoch 178, Average Loss: 14.16988217830658, Average MAE: 0.872213825583458, Learning Rate: 0.00022517998136852504, Current Patience: 5\n",
      "Epoch 179, Average Loss: 14.167852997779846, Average MAE: 0.8721338659524918, Learning Rate: 0.00018014398509482005, Current Patience: 0\n",
      "Epoch 180, Average Loss: 14.169773697853088, Average MAE: 0.8721190020442009, Learning Rate: 0.00018014398509482005, Current Patience: 1\n",
      "Epoch 181, Average Loss: 14.17151403427124, Average MAE: 0.8722171187400818, Learning Rate: 0.00018014398509482005, Current Patience: 2\n",
      "Epoch 182, Average Loss: 14.169347763061523, Average MAE: 0.8721711114048958, Learning Rate: 0.00018014398509482005, Current Patience: 3\n",
      "Epoch 183, Average Loss: 14.167536973953247, Average MAE: 0.8721354156732559, Learning Rate: 0.00018014398509482005, Current Patience: 4\n",
      "Epoch 184, Average Loss: 14.169386982917786, Average MAE: 0.8721759766340256, Learning Rate: 0.00018014398509482005, Current Patience: 5\n",
      "Epoch 185, Average Loss: 14.1722913980484, Average MAE: 0.8722511976957321, Learning Rate: 0.00014411518807585605, Current Patience: 0\n",
      "Epoch 186, Average Loss: 14.16904067993164, Average MAE: 0.8721335306763649, Learning Rate: 0.00014411518807585605, Current Patience: 1\n",
      "Epoch 187, Average Loss: 14.170758605003357, Average MAE: 0.8721547946333885, Learning Rate: 0.00014411518807585605, Current Patience: 2\n",
      "Epoch 188, Average Loss: 14.169673204421997, Average MAE: 0.8720908761024475, Learning Rate: 0.00014411518807585605, Current Patience: 3\n",
      "Epoch 189, Average Loss: 14.170025706291199, Average MAE: 0.8721603825688362, Learning Rate: 0.00014411518807585605, Current Patience: 4\n",
      "Epoch 190, Average Loss: 14.16939127445221, Average MAE: 0.8721658959984779, Learning Rate: 0.00014411518807585605, Current Patience: 5\n",
      "Epoch 191, Average Loss: 14.16873025894165, Average MAE: 0.8719932809472084, Learning Rate: 0.00011529215046068484, Current Patience: 0\n",
      "Epoch 192, Average Loss: 14.171947002410889, Average MAE: 0.8721818029880524, Learning Rate: 0.00011529215046068484, Current Patience: 1\n",
      "Epoch 193, Average Loss: 14.168310284614563, Average MAE: 0.8721051588654518, Learning Rate: 0.00011529215046068484, Current Patience: 2\n",
      "Epoch 194, Average Loss: 14.172505259513855, Average MAE: 0.8722267299890518, Learning Rate: 0.00011529215046068484, Current Patience: 3\n",
      "Epoch 195, Average Loss: 14.169273376464844, Average MAE: 0.8721269592642784, Learning Rate: 0.00011529215046068484, Current Patience: 4\n",
      "Epoch 196, Average Loss: 14.166584014892578, Average MAE: 0.8720742389559746, Learning Rate: 0.00011529215046068484, Current Patience: 5\n",
      "Epoch 197, Average Loss: 14.169027924537659, Average MAE: 0.87203498929739, Learning Rate: 9.223372036854788e-05, Current Patience: 0\n",
      "Epoch 198, Average Loss: 14.171030163764954, Average MAE: 0.8722287490963936, Learning Rate: 9.223372036854788e-05, Current Patience: 1\n",
      "Epoch 199, Average Loss: 14.1695796251297, Average MAE: 0.8721237257122993, Learning Rate: 9.223372036854788e-05, Current Patience: 2\n",
      "Epoch 200, Average Loss: 14.167746543884277, Average MAE: 0.8721343949437141, Learning Rate: 9.223372036854788e-05, Current Patience: 3\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "save_model_data() takes 9 positional arguments but 10 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m scheduler_single_transformer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(optimizer_single_transformer, mode\u001b[38;5;241m=\u001b[39mscheduler_mode, factor\u001b[38;5;241m=\u001b[39mscheduler_factor, patience\u001b[38;5;241m=\u001b[39mscheduler_patience, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, min_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m)\n\u001b[1;32m     22\u001b[0m single_transformer_epoch_losses, single_transformer_epoch_maes \u001b[38;5;241m=\u001b[39m train_model(model_single_transformer, optimizer_single_transformer, scheduler_single_transformer, loss_fn, num_epochs, num_batches_train, batched_input_train, batched_output_train, device)\n\u001b[0;32m---> 23\u001b[0m \u001b[43msave_model_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_single_transformer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_single_transformer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler_single_transformer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msingle_transformer_epoch_losses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msingle_transformer_epoch_maes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformer_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnew_iteration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_iteration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_single_transformer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m model_single_transformer\n\u001b[1;32m     25\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "\u001b[0;31mTypeError\u001b[0m: save_model_data() takes 9 positional arguments but 10 were given"
     ]
    }
   ],
   "source": [
    "if run_model_single_transformer:\n",
    "    # Define the parameters\n",
    "    in_channels = batched_input_train.shape[2]\n",
    "    features_channels = batched_input_train.shape[3]\n",
    "    out_channels = batched_output_train.shape[3]  \n",
    "    edge_in_channels = 1\n",
    "\n",
    "    # Print the parameters to verify\n",
    "    print(\"Input Channels:\", in_channels)\n",
    "    print(\"Features Channels:\", features_channels)\n",
    "    print(\"Output Channels:\", out_channels)\n",
    "    print(\"Edge Input Channels:\", edge_in_channels)\n",
    "    print(\"Hidden Channels:\", hidden_channels)\n",
    "    print(\"Transformer Layers:\", transformer_layers)\n",
    "\n",
    "    model_single_transformer = SingleModel_Transformer(in_channels, out_channels, features_channels, edge_in_channels, hidden_channels, transformer_layers).to(device)\n",
    "\n",
    "    # Define the optimizer and loss function for model_transformer\n",
    "    optimizer_single_transformer = torch.optim.Adam(model_single_transformer.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    scheduler_single_transformer = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_single_transformer, mode=scheduler_mode, factor=scheduler_factor, patience=scheduler_patience, verbose=True, min_lr=1e-5)\n",
    "\n",
    "    single_transformer_epoch_losses, single_transformer_epoch_maes = train_model(model_single_transformer, optimizer_single_transformer, scheduler_single_transformer, loss_fn, num_epochs, num_batches_train, batched_input_train, batched_output_train, device)\n",
    "    save_model_data(model_single_transformer, optimizer_single_transformer, scheduler_single_transformer, single_transformer_epoch_losses, single_transformer_epoch_maes, hidden_channels, transformer_layers, new_iteration, 'model_single_transformer')\n",
    "    del model_single_transformer\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model data saved in saved_models_20241212_3/model_single_transformer_20241212_095315\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Channels: 27\n",
      "Features Channels: 69\n",
      "Output Channels: 67\n",
      "Edge Input Channels: 1\n",
      "Hidden Channels: 1024\n",
      "Transformer Layers: 8\n",
      "Epoch 1, Average Loss: 113.63659763336182, Average MAE: 6.514209181070328, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 2, Average Loss: 37.94288778305054, Average MAE: 4.004881650209427, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 3, Average Loss: 24.6147358417511, Average MAE: 2.810287758708, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 4, Average Loss: 18.45980668067932, Average MAE: 2.13160440325737, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 5, Average Loss: 16.09454321861267, Average MAE: 1.7292031198740005, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 6, Average Loss: 14.58388340473175, Average MAE: 1.445575773715973, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 7, Average Loss: 13.72382128238678, Average MAE: 1.2918961644172668, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 8, Average Loss: 13.15995442867279, Average MAE: 1.2043996900320053, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 9, Average Loss: 12.836548089981079, Average MAE: 1.1596797108650208, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 10, Average Loss: 12.461330533027649, Average MAE: 1.1236298978328705, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 11, Average Loss: 12.172452926635742, Average MAE: 1.1024775952100754, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 12, Average Loss: 11.899538397789001, Average MAE: 1.0860071331262589, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 13, Average Loss: 11.71467912197113, Average MAE: 1.0739909559488297, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 14, Average Loss: 11.417396903038025, Average MAE: 1.0574101135134697, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 15, Average Loss: 11.185995817184448, Average MAE: 1.0432641208171844, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 16, Average Loss: 10.997116088867188, Average MAE: 1.0314166247844696, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 17, Average Loss: 10.754485249519348, Average MAE: 1.0170112177729607, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 18, Average Loss: 10.47876489162445, Average MAE: 1.0003119260072708, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 19, Average Loss: 10.196940898895264, Average MAE: 0.9846127778291702, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 20, Average Loss: 9.934211254119873, Average MAE: 0.9703272879123688, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 21, Average Loss: 9.65412437915802, Average MAE: 0.9532047808170319, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 22, Average Loss: 9.40223217010498, Average MAE: 0.9371583461761475, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 23, Average Loss: 9.109077215194702, Average MAE: 0.9222765117883682, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 24, Average Loss: 8.781750500202179, Average MAE: 0.9023995026946068, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 25, Average Loss: 8.455373764038086, Average MAE: 0.8826506435871124, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 26, Average Loss: 8.202915608882904, Average MAE: 0.8664553761482239, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 27, Average Loss: 7.916268885135651, Average MAE: 0.8487718105316162, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 28, Average Loss: 7.670522630214691, Average MAE: 0.8320258855819702, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 29, Average Loss: 7.449697494506836, Average MAE: 0.8155086934566498, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 30, Average Loss: 7.174253344535828, Average MAE: 0.7975235357880592, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 31, Average Loss: 6.974024951457977, Average MAE: 0.7840478345751762, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 32, Average Loss: 6.799420773983002, Average MAE: 0.7712159156799316, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 33, Average Loss: 6.660115838050842, Average MAE: 0.7601245492696762, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 34, Average Loss: 6.5450321435928345, Average MAE: 0.7504402697086334, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 35, Average Loss: 6.498709559440613, Average MAE: 0.7428260520100594, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 36, Average Loss: 6.420063316822052, Average MAE: 0.7345164567232132, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 37, Average Loss: 6.293714702129364, Average MAE: 0.7244114056229591, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 38, Average Loss: 6.199642241001129, Average MAE: 0.7147950157523155, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 39, Average Loss: 6.104792296886444, Average MAE: 0.70775555819273, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 40, Average Loss: 5.970234453678131, Average MAE: 0.6970178708434105, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 41, Average Loss: 5.899169325828552, Average MAE: 0.6905749440193176, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 42, Average Loss: 5.839658498764038, Average MAE: 0.6847758740186691, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 43, Average Loss: 5.75504606962204, Average MAE: 0.6781724318861961, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 44, Average Loss: 5.67695814371109, Average MAE: 0.6703260689973831, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 45, Average Loss: 5.666116774082184, Average MAE: 0.6685711741447449, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 46, Average Loss: 5.57771772146225, Average MAE: 0.6617987602949142, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 47, Average Loss: 5.497901499271393, Average MAE: 0.6560418754816055, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 48, Average Loss: 5.39557147026062, Average MAE: 0.647255502641201, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 49, Average Loss: 5.339623391628265, Average MAE: 0.6400240957736969, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 50, Average Loss: 5.280673623085022, Average MAE: 0.6361038833856583, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 51, Average Loss: 5.219028413295746, Average MAE: 0.6296278014779091, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 52, Average Loss: 5.226345658302307, Average MAE: 0.6281431168317795, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 53, Average Loss: 5.150123953819275, Average MAE: 0.6231330707669258, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 54, Average Loss: 5.1741074323654175, Average MAE: 0.6215540841221809, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 55, Average Loss: 5.144464075565338, Average MAE: 0.6191884651780128, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 56, Average Loss: 5.196187973022461, Average MAE: 0.618782140314579, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 57, Average Loss: 5.245344161987305, Average MAE: 0.6188713014125824, Learning Rate: 0.01, Current Patience: 2\n",
      "Epoch 58, Average Loss: 5.252167165279388, Average MAE: 0.6185074523091316, Learning Rate: 0.01, Current Patience: 3\n",
      "Epoch 59, Average Loss: 5.199466645717621, Average MAE: 0.6159448772668839, Learning Rate: 0.01, Current Patience: 4\n",
      "Epoch 60, Average Loss: 5.233884394168854, Average MAE: 0.6147149130702019, Learning Rate: 0.01, Current Patience: 5\n",
      "Epoch 61, Average Loss: 5.2088282108306885, Average MAE: 0.6126818358898163, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 62, Average Loss: 5.180203020572662, Average MAE: 0.606262318789959, Learning Rate: 0.008, Current Patience: 1\n",
      "Epoch 63, Average Loss: 5.1319276094436646, Average MAE: 0.6030501276254654, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 64, Average Loss: 4.945931971073151, Average MAE: 0.5947653576731682, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 65, Average Loss: 4.783331394195557, Average MAE: 0.5877457782626152, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 66, Average Loss: 4.711307346820831, Average MAE: 0.5793726891279221, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 67, Average Loss: 4.692155063152313, Average MAE: 0.5751577243208885, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 68, Average Loss: 4.694232225418091, Average MAE: 0.5740695968270302, Learning Rate: 0.008, Current Patience: 1\n",
      "Epoch 69, Average Loss: 4.6381995677948, Average MAE: 0.5705588981509209, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 70, Average Loss: 4.541790008544922, Average MAE: 0.5651057288050652, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 71, Average Loss: 4.492733120918274, Average MAE: 0.5613348260521889, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 72, Average Loss: 4.46971195936203, Average MAE: 0.5611288100481033, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 73, Average Loss: 4.445792376995087, Average MAE: 0.5557895675301552, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 74, Average Loss: 4.424998342990875, Average MAE: 0.5544905066490173, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 75, Average Loss: 4.407812237739563, Average MAE: 0.5527736693620682, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 76, Average Loss: 4.428030252456665, Average MAE: 0.553066298365593, Learning Rate: 0.008, Current Patience: 1\n",
      "Epoch 77, Average Loss: 4.358797371387482, Average MAE: 0.5479641705751419, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 78, Average Loss: 4.332903325557709, Average MAE: 0.5469769537448883, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 79, Average Loss: 4.349118173122406, Average MAE: 0.5457431524991989, Learning Rate: 0.008, Current Patience: 1\n",
      "Epoch 80, Average Loss: 4.3188629150390625, Average MAE: 0.5433269441127777, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 81, Average Loss: 4.298601686954498, Average MAE: 0.5417649745941162, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 82, Average Loss: 4.33362203836441, Average MAE: 0.5439969822764397, Learning Rate: 0.008, Current Patience: 1\n",
      "Epoch 83, Average Loss: 4.261899948120117, Average MAE: 0.5384615436196327, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 84, Average Loss: 4.23699015378952, Average MAE: 0.5364044457674026, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 85, Average Loss: 4.192846834659576, Average MAE: 0.5335899814963341, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 86, Average Loss: 4.1546103954315186, Average MAE: 0.530647024512291, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 87, Average Loss: 4.150780558586121, Average MAE: 0.5286881253123283, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 88, Average Loss: 4.1232670545578, Average MAE: 0.5280462950468063, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 89, Average Loss: 4.113469511270523, Average MAE: 0.5271287113428116, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 90, Average Loss: 4.10776287317276, Average MAE: 0.5261610150337219, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 91, Average Loss: 4.079121142625809, Average MAE: 0.5233249664306641, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 92, Average Loss: 4.059699714183807, Average MAE: 0.5231516063213348, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 93, Average Loss: 4.05494886636734, Average MAE: 0.522937998175621, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 94, Average Loss: 4.069887459278107, Average MAE: 0.5208591893315315, Learning Rate: 0.008, Current Patience: 1\n",
      "Epoch 95, Average Loss: 4.089009493589401, Average MAE: 0.5216868221759796, Learning Rate: 0.008, Current Patience: 2\n",
      "Epoch 96, Average Loss: 4.0346289575099945, Average MAE: 0.5198088884353638, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 97, Average Loss: 4.033398449420929, Average MAE: 0.518522210419178, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 98, Average Loss: 4.023839086294174, Average MAE: 0.5165418684482574, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 99, Average Loss: 3.973581701517105, Average MAE: 0.5154913142323494, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 100, Average Loss: 3.9528724253177643, Average MAE: 0.5128396153450012, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 101, Average Loss: 3.9500339329242706, Average MAE: 0.511543270200491, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 102, Average Loss: 3.948702037334442, Average MAE: 0.5109685771167278, Learning Rate: 0.008, Current Patience: 0\n",
      "Epoch 103, Average Loss: 4.016021728515625, Average MAE: 0.5155883096158504, Learning Rate: 0.008, Current Patience: 1\n",
      "Epoch 104, Average Loss: 4.023106396198273, Average MAE: 0.5154401287436485, Learning Rate: 0.008, Current Patience: 2\n",
      "Epoch 105, Average Loss: 4.076261401176453, Average MAE: 0.5158748477697372, Learning Rate: 0.008, Current Patience: 3\n",
      "Epoch 106, Average Loss: 4.108123451471329, Average MAE: 0.5187455452978611, Learning Rate: 0.008, Current Patience: 4\n",
      "Epoch 107, Average Loss: 4.0771691501140594, Average MAE: 0.5141348913311958, Learning Rate: 0.008, Current Patience: 5\n",
      "Epoch 108, Average Loss: 4.179359436035156, Average MAE: 0.5180559270083904, Learning Rate: 0.0064, Current Patience: 0\n",
      "Epoch 109, Average Loss: 4.128062516450882, Average MAE: 0.5142689011991024, Learning Rate: 0.0064, Current Patience: 1\n",
      "Epoch 110, Average Loss: 4.030394017696381, Average MAE: 0.5086522214114666, Learning Rate: 0.0064, Current Patience: 2\n",
      "Epoch 111, Average Loss: 3.9167975187301636, Average MAE: 0.5070400349795818, Learning Rate: 0.0064, Current Patience: 0\n",
      "Epoch 112, Average Loss: 3.8158080875873566, Average MAE: 0.5016881562769413, Learning Rate: 0.0064, Current Patience: 0\n",
      "Epoch 113, Average Loss: 3.7920060455799103, Average MAE: 0.4991767220199108, Learning Rate: 0.0064, Current Patience: 0\n",
      "Epoch 114, Average Loss: 3.823163241147995, Average MAE: 0.4991036430001259, Learning Rate: 0.0064, Current Patience: 1\n",
      "Epoch 115, Average Loss: 3.7843546867370605, Average MAE: 0.49550629034638405, Learning Rate: 0.0064, Current Patience: 0\n",
      "Epoch 116, Average Loss: 3.8251526951789856, Average MAE: 0.4992450475692749, Learning Rate: 0.0064, Current Patience: 1\n",
      "Epoch 117, Average Loss: 3.766388565301895, Average MAE: 0.49753495678305626, Learning Rate: 0.0064, Current Patience: 0\n",
      "Epoch 118, Average Loss: 3.758743941783905, Average MAE: 0.49726948514580727, Learning Rate: 0.0064, Current Patience: 0\n",
      "Epoch 119, Average Loss: 3.747505098581314, Average MAE: 0.4980761855840683, Learning Rate: 0.0064, Current Patience: 0\n",
      "Epoch 120, Average Loss: 3.693780869245529, Average MAE: 0.4922904297709465, Learning Rate: 0.0064, Current Patience: 0\n",
      "Epoch 121, Average Loss: 3.69106587767601, Average MAE: 0.49163879826664925, Learning Rate: 0.0064, Current Patience: 0\n",
      "Epoch 122, Average Loss: 3.683599352836609, Average MAE: 0.48986056074500084, Learning Rate: 0.0064, Current Patience: 0\n",
      "Epoch 123, Average Loss: 3.742245316505432, Average MAE: 0.4914163649082184, Learning Rate: 0.0064, Current Patience: 1\n",
      "Epoch 124, Average Loss: 3.6809073984622955, Average MAE: 0.48791268467903137, Learning Rate: 0.0064, Current Patience: 0\n",
      "Epoch 125, Average Loss: 3.6652769446372986, Average MAE: 0.48893487080931664, Learning Rate: 0.0064, Current Patience: 0\n",
      "Epoch 126, Average Loss: 3.660502016544342, Average MAE: 0.4889003448188305, Learning Rate: 0.0064, Current Patience: 0\n",
      "Epoch 127, Average Loss: 3.6619739830493927, Average MAE: 0.4889642260968685, Learning Rate: 0.0064, Current Patience: 1\n",
      "Epoch 128, Average Loss: 3.6937834322452545, Average MAE: 0.49221525713801384, Learning Rate: 0.0064, Current Patience: 2\n",
      "Epoch 129, Average Loss: 3.7195049822330475, Average MAE: 0.49436237290501595, Learning Rate: 0.0064, Current Patience: 3\n",
      "Epoch 130, Average Loss: 3.7781588435173035, Average MAE: 0.5011976547539234, Learning Rate: 0.0064, Current Patience: 4\n",
      "Epoch 131, Average Loss: 3.809284031391144, Average MAE: 0.5004098191857338, Learning Rate: 0.0064, Current Patience: 5\n",
      "Epoch 132, Average Loss: 3.8098058104515076, Average MAE: 0.5014221742749214, Learning Rate: 0.00512, Current Patience: 0\n",
      "Epoch 133, Average Loss: 3.840163767337799, Average MAE: 0.5026393830776215, Learning Rate: 0.00512, Current Patience: 1\n",
      "Epoch 134, Average Loss: 3.8918960094451904, Average MAE: 0.5078631825745106, Learning Rate: 0.00512, Current Patience: 2\n",
      "Epoch 135, Average Loss: 3.8476055562496185, Average MAE: 0.5022521317005157, Learning Rate: 0.00512, Current Patience: 3\n",
      "Epoch 136, Average Loss: 3.758790075778961, Average MAE: 0.49594853445887566, Learning Rate: 0.00512, Current Patience: 4\n",
      "Epoch 137, Average Loss: 3.6731634438037872, Average MAE: 0.4870644211769104, Learning Rate: 0.00512, Current Patience: 5\n",
      "Epoch 138, Average Loss: 3.6135165989398956, Average MAE: 0.48325277492403984, Learning Rate: 0.00512, Current Patience: 0\n",
      "Epoch 139, Average Loss: 3.596791625022888, Average MAE: 0.4800553284585476, Learning Rate: 0.00512, Current Patience: 0\n",
      "Epoch 140, Average Loss: 3.4899642765522003, Average MAE: 0.4738858565688133, Learning Rate: 0.00512, Current Patience: 0\n",
      "Epoch 141, Average Loss: 3.4613530933856964, Average MAE: 0.472346656024456, Learning Rate: 0.00512, Current Patience: 0\n",
      "Epoch 142, Average Loss: 3.496545433998108, Average MAE: 0.4743828698992729, Learning Rate: 0.00512, Current Patience: 1\n",
      "Epoch 143, Average Loss: 3.4889532327651978, Average MAE: 0.476039569824934, Learning Rate: 0.00512, Current Patience: 2\n",
      "Epoch 144, Average Loss: 3.4598590433597565, Average MAE: 0.4723159968852997, Learning Rate: 0.00512, Current Patience: 0\n",
      "Epoch 145, Average Loss: 3.44938525557518, Average MAE: 0.4703461341559887, Learning Rate: 0.00512, Current Patience: 0\n",
      "Epoch 146, Average Loss: 3.438875287771225, Average MAE: 0.46984605863690376, Learning Rate: 0.00512, Current Patience: 0\n",
      "Epoch 147, Average Loss: 3.384521007537842, Average MAE: 0.4653630554676056, Learning Rate: 0.00512, Current Patience: 0\n",
      "Epoch 148, Average Loss: 3.3376755118370056, Average MAE: 0.46289292722940445, Learning Rate: 0.00512, Current Patience: 0\n",
      "Epoch 149, Average Loss: 3.330370992422104, Average MAE: 0.46275103464722633, Learning Rate: 0.00512, Current Patience: 0\n",
      "Epoch 150, Average Loss: 3.3399902284145355, Average MAE: 0.463053148239851, Learning Rate: 0.00512, Current Patience: 1\n",
      "Epoch 151, Average Loss: 3.337577611207962, Average MAE: 0.46147820726037025, Learning Rate: 0.00512, Current Patience: 2\n",
      "Epoch 152, Average Loss: 3.31825253367424, Average MAE: 0.4607854411005974, Learning Rate: 0.00512, Current Patience: 0\n",
      "Epoch 153, Average Loss: 3.3394519686698914, Average MAE: 0.4625323452055454, Learning Rate: 0.00512, Current Patience: 1\n",
      "Epoch 154, Average Loss: 3.349121540784836, Average MAE: 0.46269259974360466, Learning Rate: 0.00512, Current Patience: 2\n",
      "Epoch 155, Average Loss: 3.338901787996292, Average MAE: 0.46161380037665367, Learning Rate: 0.00512, Current Patience: 3\n",
      "Epoch 156, Average Loss: 3.3339565992355347, Average MAE: 0.461751963943243, Learning Rate: 0.00512, Current Patience: 4\n",
      "Epoch 157, Average Loss: 3.3055404126644135, Average MAE: 0.4604238495230675, Learning Rate: 0.00512, Current Patience: 0\n",
      "Epoch 158, Average Loss: 3.3008992969989777, Average MAE: 0.46136268228292465, Learning Rate: 0.00512, Current Patience: 0\n",
      "Epoch 159, Average Loss: 3.3182361721992493, Average MAE: 0.4624978005886078, Learning Rate: 0.00512, Current Patience: 1\n",
      "Epoch 160, Average Loss: 3.317791223526001, Average MAE: 0.46193618699908257, Learning Rate: 0.00512, Current Patience: 2\n",
      "Epoch 161, Average Loss: 3.3553036749362946, Average MAE: 0.4622791111469269, Learning Rate: 0.00512, Current Patience: 3\n",
      "Epoch 162, Average Loss: 3.3477642834186554, Average MAE: 0.4620622247457504, Learning Rate: 0.00512, Current Patience: 4\n",
      "Epoch 163, Average Loss: 3.3444650769233704, Average MAE: 0.46256962418556213, Learning Rate: 0.00512, Current Patience: 5\n",
      "Epoch 164, Average Loss: 3.2978180050849915, Average MAE: 0.4590515270829201, Learning Rate: 0.00512, Current Patience: 0\n",
      "Epoch 165, Average Loss: 3.2937205731868744, Average MAE: 0.45971497148275375, Learning Rate: 0.00512, Current Patience: 0\n",
      "Epoch 166, Average Loss: 3.3380801677703857, Average MAE: 0.46457812190055847, Learning Rate: 0.00512, Current Patience: 1\n",
      "Epoch 167, Average Loss: 3.3058495223522186, Average MAE: 0.46243420615792274, Learning Rate: 0.00512, Current Patience: 2\n",
      "Epoch 168, Average Loss: 3.306470662355423, Average MAE: 0.4621896371245384, Learning Rate: 0.00512, Current Patience: 3\n",
      "Epoch 169, Average Loss: 3.3467186987400055, Average MAE: 0.4635455161333084, Learning Rate: 0.00512, Current Patience: 4\n",
      "Epoch 170, Average Loss: 3.3395459949970245, Average MAE: 0.4622471034526825, Learning Rate: 0.00512, Current Patience: 5\n",
      "Epoch 171, Average Loss: 3.3125911951065063, Average MAE: 0.46112700179219246, Learning Rate: 0.004096000000000001, Current Patience: 0\n",
      "Epoch 172, Average Loss: 3.2769059240818024, Average MAE: 0.4569697044789791, Learning Rate: 0.004096000000000001, Current Patience: 0\n",
      "Epoch 173, Average Loss: 3.2891487777233124, Average MAE: 0.45973389968276024, Learning Rate: 0.004096000000000001, Current Patience: 1\n",
      "Epoch 174, Average Loss: 3.2925164699554443, Average MAE: 0.4598179906606674, Learning Rate: 0.004096000000000001, Current Patience: 2\n",
      "Epoch 175, Average Loss: 3.2953017354011536, Average MAE: 0.4606354683637619, Learning Rate: 0.004096000000000001, Current Patience: 3\n",
      "Epoch 176, Average Loss: 3.2801786363124847, Average MAE: 0.4581006355583668, Learning Rate: 0.004096000000000001, Current Patience: 4\n",
      "Epoch 177, Average Loss: 3.288949877023697, Average MAE: 0.4579404853284359, Learning Rate: 0.004096000000000001, Current Patience: 5\n",
      "Epoch 178, Average Loss: 3.2288489043712616, Average MAE: 0.45614248886704445, Learning Rate: 0.004096000000000001, Current Patience: 0\n",
      "Epoch 179, Average Loss: 3.2049556374549866, Average MAE: 0.45324281230568886, Learning Rate: 0.004096000000000001, Current Patience: 0\n",
      "Epoch 180, Average Loss: 3.2237628400325775, Average MAE: 0.45461394637823105, Learning Rate: 0.004096000000000001, Current Patience: 1\n",
      "Epoch 181, Average Loss: 3.172933042049408, Average MAE: 0.4516618326306343, Learning Rate: 0.004096000000000001, Current Patience: 0\n",
      "Epoch 182, Average Loss: 3.1635837256908417, Average MAE: 0.44984590634703636, Learning Rate: 0.004096000000000001, Current Patience: 0\n",
      "Epoch 183, Average Loss: 3.1808756291866302, Average MAE: 0.4501315541565418, Learning Rate: 0.004096000000000001, Current Patience: 1\n",
      "Epoch 184, Average Loss: 3.170910984277725, Average MAE: 0.45018522813916206, Learning Rate: 0.004096000000000001, Current Patience: 2\n",
      "Epoch 185, Average Loss: 3.1236886978149414, Average MAE: 0.44838055968284607, Learning Rate: 0.004096000000000001, Current Patience: 0\n",
      "Epoch 186, Average Loss: 3.1350795328617096, Average MAE: 0.44915731996297836, Learning Rate: 0.004096000000000001, Current Patience: 1\n",
      "Epoch 187, Average Loss: 3.104120671749115, Average MAE: 0.44673625752329826, Learning Rate: 0.004096000000000001, Current Patience: 0\n",
      "Epoch 188, Average Loss: 3.1432754695415497, Average MAE: 0.4486067071557045, Learning Rate: 0.004096000000000001, Current Patience: 1\n",
      "Epoch 189, Average Loss: 3.105973482131958, Average MAE: 0.4464900828897953, Learning Rate: 0.004096000000000001, Current Patience: 2\n",
      "Epoch 190, Average Loss: 3.1201359033584595, Average MAE: 0.44705434516072273, Learning Rate: 0.004096000000000001, Current Patience: 3\n",
      "Epoch 191, Average Loss: 3.079999029636383, Average MAE: 0.4451855197548866, Learning Rate: 0.004096000000000001, Current Patience: 0\n",
      "Epoch 192, Average Loss: 3.067631632089615, Average MAE: 0.4444909319281578, Learning Rate: 0.004096000000000001, Current Patience: 0\n",
      "Epoch 193, Average Loss: 3.1162068247795105, Average MAE: 0.4468238651752472, Learning Rate: 0.004096000000000001, Current Patience: 1\n",
      "Epoch 194, Average Loss: 3.0917606949806213, Average MAE: 0.4448649436235428, Learning Rate: 0.004096000000000001, Current Patience: 2\n",
      "Epoch 195, Average Loss: 3.067368119955063, Average MAE: 0.443688977509737, Learning Rate: 0.004096000000000001, Current Patience: 3\n",
      "Epoch 196, Average Loss: 3.0788765847682953, Average MAE: 0.4433619827032089, Learning Rate: 0.004096000000000001, Current Patience: 4\n",
      "Epoch 197, Average Loss: 3.088146299123764, Average MAE: 0.4442903846502304, Learning Rate: 0.004096000000000001, Current Patience: 5\n",
      "Epoch 198, Average Loss: 3.0430913269519806, Average MAE: 0.44224241748452187, Learning Rate: 0.004096000000000001, Current Patience: 0\n",
      "Epoch 199, Average Loss: 3.0627516210079193, Average MAE: 0.44323867931962013, Learning Rate: 0.004096000000000001, Current Patience: 1\n",
      "Epoch 200, Average Loss: 3.0566444993019104, Average MAE: 0.44265141710639, Learning Rate: 0.004096000000000001, Current Patience: 2\n",
      "Model data saved in saved_models_20241212_3/model_single_gnn_20241212_101824\n"
     ]
    }
   ],
   "source": [
    "if run_model_single_gnn:\n",
    "    torch.cuda.empty_cache()\n",
    "    # Define the parameters\n",
    "    in_channels = batched_input_train.shape[2]\n",
    "    features_channels = batched_input_train.shape[3]\n",
    "    out_channels = batched_output_train.shape[3]  \n",
    "    edge_in_channels = 1\n",
    "\n",
    "    # Print the parameters to verify\n",
    "    print(\"Input Channels:\", in_channels)\n",
    "    print(\"Features Channels:\", features_channels)\n",
    "    print(\"Output Channels:\", out_channels)\n",
    "    print(\"Edge Input Channels:\", edge_in_channels)\n",
    "    print(\"Hidden Channels:\", hidden_channels)\n",
    "    print(\"Transformer Layers:\", transformer_layers)\n",
    "\n",
    "    model_single_gnn = SingleModel_GNN(in_channels, out_channels, features_channels, edge_in_channels, hidden_channels, transformer_layers).to(device)\n",
    "    \n",
    "    # Define the optimizer and loss function for model_gnn\n",
    "    optimizer_single_gnn = torch.optim.Adam(model_single_gnn.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    scheduler_single_gnn = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_single_gnn, mode=scheduler_mode, factor=scheduler_factor, patience=scheduler_patience, verbose=True, min_lr=1e-5)\n",
    "\n",
    "    single_gnn_epoch_losses, single_gnn_epoch_maes = train_model(model_single_gnn, optimizer_single_gnn, scheduler_single_gnn, loss_fn, num_epochs, num_batches_train, batched_input_train, batched_output_train, device)\n",
    "    save_model_data(model_single_gnn, optimizer_single_gnn, scheduler_single_gnn, single_gnn_epoch_losses, single_gnn_epoch_maes, hidden_channels, transformer_layers, new_iteration, 'model_single_gnn')\n",
    "    del model_single_gnn\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">**ZE EPIC GRAPHS**</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of gnn_transformer_epoch_losses: (200,)\n",
      "Shape of gnn_transformer_epoch_maes: (200,)\n",
      "Shape of transformer_gnn_epoch_losses: (200,)\n",
      "Shape of transformer_gnn_epoch_maes: (200,)\n",
      "Shape of single_gnn_epoch_losses: (200,)\n",
      "Shape of single_gnn_epoch_maes: (200,)\n",
      "Shape of single_transformer_epoch_losses: (200,)\n",
      "Shape of single_transformer_epoch_maes: (200,)\n",
      "Shape of parallel_epoch_losses: (200,)\n",
      "Shape of parallel_epoch_maes: (200,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "if run_plotting:\n",
    "    # Print the shape of each variable\n",
    "    print(\"Shape of gnn_transformer_epoch_losses:\", np.shape(gnn_transformer_epoch_losses))\n",
    "    print(\"Shape of gnn_transformer_epoch_maes:\", np.shape(gnn_transformer_epoch_maes))\n",
    "    print(\"Shape of transformer_gnn_epoch_losses:\", np.shape(transformer_gnn_epoch_losses))\n",
    "    print(\"Shape of transformer_gnn_epoch_maes:\", np.shape(transformer_gnn_epoch_maes))\n",
    "    print(\"Shape of single_gnn_epoch_losses:\", np.shape(single_gnn_epoch_losses))\n",
    "    print(\"Shape of single_gnn_epoch_maes:\", np.shape(single_gnn_epoch_maes))\n",
    "    print(\"Shape of single_transformer_epoch_losses:\", np.shape(single_transformer_epoch_losses))\n",
    "    print(\"Shape of single_transformer_epoch_maes:\", np.shape(single_transformer_epoch_maes))\n",
    "    print(\"Shape of parallel_epoch_losses:\", np.shape(parallel_epoch_losses))\n",
    "    print(\"Shape of parallel_epoch_maes:\", np.shape(parallel_epoch_maes))\n",
    "\n",
    "    # Flatten all arrays\n",
    "    gnn_transformer_epoch_losses = np.array(gnn_transformer_epoch_losses).flatten()\n",
    "    gnn_transformer_epoch_maes = np.array(gnn_transformer_epoch_maes).flatten()\n",
    "    transformer_gnn_epoch_losses = np.array(transformer_gnn_epoch_losses).flatten()\n",
    "    transformer_gnn_epoch_maes = np.array(transformer_gnn_epoch_maes).flatten()\n",
    "    single_gnn_epoch_losses = np.array(single_gnn_epoch_losses).flatten()\n",
    "    single_gnn_epoch_maes = np.array(single_gnn_epoch_maes).flatten()\n",
    "    single_transformer_epoch_losses = np.array(single_transformer_epoch_losses).flatten()\n",
    "    single_transformer_epoch_maes = np.array(single_transformer_epoch_maes).flatten()\n",
    "    parallel_epoch_losses = np.array(parallel_epoch_losses).flatten()\n",
    "    parallel_epoch_maes = np.array(parallel_epoch_maes).flatten()\n",
    "\n",
    "    # Plotting the epoch losses\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(gnn_transformer_epoch_losses, label='GNN Transformer Loss')\n",
    "    plt.plot(transformer_gnn_epoch_losses, label='Transformer GNN Loss')\n",
    "    plt.plot(single_gnn_epoch_losses, label='GNN Loss')\n",
    "    plt.plot(single_transformer_epoch_losses, label='Transformer Loss')\n",
    "    plt.plot(parallel_epoch_losses, label='Parallel Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Epoch Losses')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plotting the epoch MAE\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(gnn_transformer_epoch_maes, label='GNN Transformer MAE')\n",
    "    plt.plot(transformer_gnn_epoch_maes, label='Transformer GNN MAE')\n",
    "    plt.plot(single_gnn_epoch_maes, label='GNN MAE')\n",
    "    plt.plot(single_transformer_epoch_maes, label='Transformer MAE')\n",
    "    plt.plot(parallel_epoch_maes, label='Parallel MAE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.title('Epoch MAE')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, loss_fn, num_batches_test, batched_input_test, batched_output_test, device):\n",
    "    model.eval()\n",
    "    test_losses = []\n",
    "    test_maes = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for b in range(num_batches_test):\n",
    "            node_features_batch = batched_input_test[b].to(device)\n",
    "            desired_output_batch = batched_output_test[b].to(device)\n",
    "\n",
    "            model_output_batch = model(node_features_batch, edge_index, edge_attr)\n",
    "\n",
    "            loss = loss_fn(model_output_batch, desired_output_batch)\n",
    "            mae = torch.mean(torch.abs(model_output_batch - desired_output_batch))\n",
    "\n",
    "            test_losses.append(loss.item())\n",
    "            test_maes.append(mae.item())\n",
    "\n",
    "    average_test_loss = sum(test_losses) / len(test_losses)\n",
    "    average_test_mae = sum(test_maes) / len(test_maes)\n",
    "    print(f\"Average Test Loss: {average_test_loss}, Average Test MAE: {average_test_mae}\")\n",
    "\n",
    "    return test_losses, test_maes, average_test_loss, average_test_mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_pt_file_from_folder(base_directory, folder_name_start):\n",
    "    pt_files = []\n",
    "    # Iterate over all folders in the base directory\n",
    "    for folder_name in os.listdir(base_directory):\n",
    "        if folder_name.startswith(folder_name_start):\n",
    "            folder_path = os.path.join(base_directory, folder_name)\n",
    "            # Check if it is a directory\n",
    "            if os.path.isdir(folder_path):\n",
    "                # Iterate over all files in the folder\n",
    "                for file_name in os.listdir(folder_path):\n",
    "                    if file_name.endswith('.pt'):\n",
    "                        file_path = os.path.join(folder_path, file_name)\n",
    "                        pt_files.append(file_path)\n",
    "    return pt_files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_losses_map = {}\n",
    "test_maes_map = {}\n",
    "average_test_losses_map = {}\n",
    "average_test_maes_map = {}\n",
    "base_directory = 'saved_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/HDD/RyanFolder/projects/Weather-Predition-Project/.venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_319101/4264184606.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(pt_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Loss: 37163286.625, Average Test MAE: 1159.9265213012695\n",
      "Model loaded from model_parallel\n"
     ]
    }
   ],
   "source": [
    "# Define the model architecture (make sure it matches the saved model)\n",
    "in_channels = batched_input_train.shape[2]\n",
    "features_channels = batched_input_train.shape[3]\n",
    "out_channels = batched_output_train.shape[3]\n",
    "edge_in_channels = 1\n",
    "\n",
    "folder_name_start = 'model_parallel'\n",
    "\n",
    "pt_file_path = get_pt_file_from_folder(base_directory, folder_name_start)\n",
    "\n",
    "# Load the model\n",
    "for pt_file in pt_file_path:\n",
    "    checkpoint = torch.load(pt_file)\n",
    "    \n",
    "    hidden_channels = checkpoint['hidden_channels']\n",
    "    transformer_layers = checkpoint['transformer_layers']\n",
    "    \n",
    "    model = HybridModel_Parallel(in_channels, out_channels, features_channels, edge_in_channels, hidden_channels, transformer_layers).to(device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    test_losses, test_maes, average_test_loss, average_test_mae = evaluate_model(model, loss_fn, num_batches_test, batched_input_test, batched_output_test, device)\n",
    "    pt_file = pt_file.split('/')[-1].split('.')[0]\n",
    "    \n",
    "    test_losses_map[pt_file] = test_losses\n",
    "    test_maes_map[pt_file] = test_maes\n",
    "    average_test_losses_map[pt_file] = average_test_loss\n",
    "    average_test_maes_map[pt_file] = average_test_mae\n",
    "    print(f\"Model loaded from {pt_file}\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_319101/3214045217.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(pt_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Loss: 4552076.25, Average Test MAE: 679.4918212890625\n",
      "Model loaded from model_transformer_gnn\n"
     ]
    }
   ],
   "source": [
    "# Define the model architecture (make sure it matches the saved model)\n",
    "in_channels = batched_input_train.shape[2]\n",
    "features_channels = batched_input_train.shape[3]\n",
    "out_channels = batched_output_train.shape[3]\n",
    "edge_in_channels = 1\n",
    "\n",
    "folder_name_start = 'model_transformer_gnn'\n",
    "\n",
    "pt_file_path = get_pt_file_from_folder(base_directory, folder_name_start)\n",
    "\n",
    "# Load the model\n",
    "for pt_file in pt_file_path:\n",
    "    checkpoint = torch.load(pt_file)\n",
    "\n",
    "\n",
    "    hidden_channels = checkpoint['hidden_channels']\n",
    "    transformer_layers = checkpoint['transformer_layers']\n",
    "    \n",
    "    model = HybridModel_Transformer(in_channels, out_channels, features_channels, edge_in_channels, hidden_channels, transformer_layers).to(device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    test_losses, test_maes, average_test_loss, average_test_mae = evaluate_model(model, loss_fn, num_batches_test, batched_input_test, batched_output_test, device)\n",
    "    pt_file = pt_file.split('/')[-1].split('.')[0]\n",
    "    \n",
    "    test_losses_map[pt_file] = test_losses\n",
    "    test_maes_map[pt_file] = test_maes\n",
    "    average_test_losses_map[pt_file] = average_test_loss\n",
    "    average_test_maes_map[pt_file] = average_test_mae\n",
    "    print(f\"Model loaded from {pt_file}\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_319101/2781377150.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(pt_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Loss: 13.960506439208984, Average Test MAE: 0.8743552714586258\n",
      "Model loaded from model_gnn_transformer\n"
     ]
    }
   ],
   "source": [
    "# Define the model architecture (make sure it matches the saved model)\n",
    "in_channels = batched_input_train.shape[2]\n",
    "features_channels = batched_input_train.shape[3]\n",
    "out_channels = batched_output_train.shape[3]\n",
    "edge_in_channels = 1\n",
    "\n",
    "folder_name_start = 'model_gnn_transformer'\n",
    "\n",
    "pt_file_path = get_pt_file_from_folder(base_directory, folder_name_start)\n",
    "\n",
    "# Load the model\n",
    "for pt_file in pt_file_path:\n",
    "    checkpoint = torch.load(pt_file)\n",
    "\n",
    "    hidden_channels = checkpoint['hidden_channels']\n",
    "    transformer_layers = checkpoint['transformer_layers']\n",
    "\n",
    "    model = HybridModel_GNN(in_channels, out_channels, features_channels, edge_in_channels, hidden_channels, transformer_layers).to(device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    test_losses, test_maes, average_test_loss, average_test_mae = evaluate_model(model, loss_fn, num_batches_test, batched_input_test, batched_output_test, device)\n",
    "    pt_file = pt_file.split('/')[-1].split('.')[0]\n",
    "    \n",
    "    test_losses_map[pt_file] = test_losses\n",
    "    test_maes_map[pt_file] = test_maes\n",
    "    average_test_losses_map[pt_file] = average_test_loss\n",
    "    average_test_maes_map[pt_file] = average_test_mae\n",
    "    print(f\"Model loaded from {pt_file}\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_319101/4158397555.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(pt_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Loss: 44015660.5, Average Test MAE: 1426.585205078125\n",
      "Model loaded from model_single_gnn\n"
     ]
    }
   ],
   "source": [
    "# Define the model architecture (make sure it matches the saved model)\n",
    "in_channels = batched_input_train.shape[2]\n",
    "features_channels = batched_input_train.shape[3]\n",
    "out_channels = batched_output_train.shape[3]\n",
    "edge_in_channels = 1\n",
    "\n",
    "folder_name_start = 'model_single_gnn'\n",
    "\n",
    "pt_file_path = get_pt_file_from_folder(base_directory, folder_name_start)\n",
    "\n",
    "# Load the model\n",
    "for pt_file in pt_file_path:\n",
    "    checkpoint = torch.load(pt_file)\n",
    "\n",
    "\n",
    "    hidden_channels = checkpoint['hidden_channels']\n",
    "    transformer_layers = checkpoint['transformer_layers']\n",
    "\n",
    "    model = SingleModel_GNN(in_channels, out_channels, features_channels, edge_in_channels, hidden_channels, transformer_layers).to(device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    test_losses, test_maes, average_test_loss, average_test_mae = evaluate_model(model, loss_fn, num_batches_test, batched_input_test, batched_output_test, device)\n",
    "    pt_file = pt_file.split('/')[-1].split('.')[0]\n",
    "    \n",
    "    test_losses_map[pt_file] = test_losses\n",
    "    test_maes_map[pt_file] = test_maes\n",
    "    average_test_losses_map[pt_file] = average_test_loss\n",
    "    average_test_maes_map[pt_file] = average_test_mae\n",
    "    print(f\"Model loaded from {pt_file}\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_319101/2629607932.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(pt_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Loss: 13.665408611297607, Average Test MAE: 0.8417310416698456\n",
      "Model loaded from model_single_transformer\n"
     ]
    }
   ],
   "source": [
    "# Define the model architecture (make sure it matches the saved model)\n",
    "in_channels = batched_input_train.shape[2]\n",
    "features_channels = batched_input_train.shape[3]\n",
    "out_channels = batched_output_train.shape[3]\n",
    "edge_in_channels = 1\n",
    "\n",
    "folder_name_start = 'model_single_transformer'\n",
    "\n",
    "pt_file_path = get_pt_file_from_folder(base_directory, folder_name_start)\n",
    "\n",
    "# Load the model\n",
    "for pt_file in pt_file_path:\n",
    "    checkpoint = torch.load(pt_file)\n",
    "    \n",
    "    hidden_channels = checkpoint['hidden_channels']\n",
    "    transformer_layers = checkpoint['transformer_layers']\n",
    "\n",
    "    model = SingleModel_Transformer(in_channels, out_channels, features_channels, edge_in_channels, hidden_channels, transformer_layers).to(device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    pt_file = pt_file.split('/')[-1].split('.')[0]\n",
    "    test_losses, test_maes, average_test_loss, average_test_mae = evaluate_model(model, loss_fn, num_batches_test, batched_input_test, batched_output_test, device)\n",
    "    \n",
    "    test_losses_map[pt_file] = test_losses\n",
    "    test_maes_map[pt_file] = test_maes\n",
    "    average_test_losses_map[pt_file] = average_test_loss\n",
    "    average_test_maes_map[pt_file] = average_test_mae\n",
    "    print(f\"Model loaded from {pt_file}\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_parallel': [2954220.5, 5194994.0, 78326304.0, 62177628.0], 'model_transformer_gnn': [4553480.0, 4555226.0, 4551201.0, 4548398.0], 'model_gnn_transformer': [14.58088493347168, 9.754066467285156, 17.490217208862305, 14.016857147216797], 'model_single_gnn': [8291129.0, 6637457.0, 91999432.0, 69134624.0], 'model_single_transformer': [13.93709945678711, 8.767899513244629, 17.423709869384766, 14.532925605773926]}\n",
      "{'model_parallel': [374.9904479980469, 483.61993408203125, 2026.408935546875, 1754.686767578125], 'model_transformer_gnn': [679.7529296875, 679.7340698242188, 679.2893676757812, 679.19091796875], 'model_gnn_transformer': [0.963783860206604, 0.7682548761367798, 0.8948209881782532, 0.8705613613128662], 'model_single_gnn': [641.1812744140625, 658.1024169921875, 2400.822509765625, 2006.234619140625], 'model_single_transformer': [0.9026488661766052, 0.6916754841804504, 0.8940122127532959, 0.8785876035690308]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Print to verify\n",
    "print(test_losses_map)\n",
    "print(test_maes_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# evaluate_model(model_gnn_transformer, loss_fn, num_batches_test, batched_input_test, batched_output_test, device)\n",
    "# evaluate_model(model_transformer_gnn, loss_fn, num_batches_test, batched_input_test, batched_output_test, device)\n",
    "# evaluate_model(model_parallel, loss_fn, num_batches_test, batched_input_test, batched_output_test, device)\n",
    "# evaluate_model(model_single_transformer, loss_fn, num_batches_test, batched_input_test, batched_output_test, device)\n",
    "# evaluate_model(model_single_gnn, loss_fn, num_batches_test, batched_input_test, batched_output_test, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Plotting average test losses\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "model_names = list(average_test_losses_map.keys())\n",
    "average_losses = list(average_test_losses_map.values())\n",
    "plt.bar(range(len(model_names)), average_losses, tick_label=model_names)\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Average Test Loss')\n",
    "plt.title('Average Test Losses')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Plotting average test MAEs\n",
    "plt.subplot(1, 2, 2)\n",
    "average_maes = list(average_test_maes_map.values())\n",
    "plt.bar(range(len(model_names)), average_maes, tick_label=model_names)\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Average Test MAE')\n",
    "plt.title('Average Test MAEs')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
