{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/HDD/RyanFolder/projects/Weather-Predition-Project/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv, GraphConv\n",
    "from torch_geometric.nn.norm import BatchNorm\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "import numpy as np\n",
    "from torch.optim.swa_utils import AveragedModel, SWALR\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">**ZE EPIC DATA**</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([72790024141, 72785524114, 72789094197, 72793024233, 72785794129, 72788594266, 72797624217, 72785024157, 72797094240, 72798594276, 72792424223, 72792894263, 72781024243, 72781524237, 72788324220, 72698824219, 72793894274, 74206024207, 72782724110, 72793724222, 72792594227, 72782594239, 72794504205, 72792394225, 72784524163, 72792024227, 72785694176])\n"
     ]
    }
   ],
   "source": [
    "# Directory containing the CSV files\n",
    "directory = '../processed-final-data-2'\n",
    "\n",
    "# Dictionary to store the dataframes\n",
    "dataframes = {}\n",
    "\n",
    "# Iterate over all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        # Extract the file name without extension and convert it to int\n",
    "        key = int(os.path.splitext(filename)[0])\n",
    "        \n",
    "        # Read the CSV file into a dataframe\n",
    "        df = pd.read_csv(os.path.join(directory, filename))\n",
    "        dataframes[key] = df\n",
    "\n",
    "\n",
    "# Print the dictionary keys to verify\n",
    "print(dataframes.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72790024141: Sequential split verified.\n",
      "72785524114: Sequential split verified.\n",
      "72789094197: Sequential split verified.\n",
      "72793024233: Sequential split verified.\n",
      "72785794129: Sequential split verified.\n",
      "72788594266: Sequential split verified.\n",
      "72797624217: Sequential split verified.\n",
      "72785024157: Sequential split verified.\n",
      "72797094240: Sequential split verified.\n",
      "72798594276: Sequential split verified.\n",
      "72792424223: Sequential split verified.\n",
      "72792894263: Sequential split verified.\n",
      "72781024243: Sequential split verified.\n",
      "72781524237: Sequential split verified.\n",
      "72788324220: Sequential split verified.\n",
      "72698824219: Sequential split verified.\n",
      "72793894274: Sequential split verified.\n",
      "74206024207: Sequential split verified.\n",
      "72782724110: Sequential split verified.\n",
      "72793724222: Sequential split verified.\n",
      "72792594227: Sequential split verified.\n",
      "72782594239: Sequential split verified.\n",
      "72794504205: Sequential split verified.\n",
      "72792394225: Sequential split verified.\n",
      "72784524163: Sequential split verified.\n",
      "72792024227: Sequential split verified.\n",
      "72785694176: Sequential split verified.\n",
      "dict_keys([72790024141, 72785524114, 72789094197, 72793024233, 72785794129, 72788594266, 72797624217, 72785024157, 72797094240, 72798594276, 72792424223, 72792894263, 72781024243, 72781524237, 72788324220, 72698824219, 72793894274, 74206024207, 72782724110, 72793724222, 72792594227, 72782594239, 72794504205, 72792394225, 72784524163, 72792024227, 72785694176])\n",
      "dict_keys([72790024141, 72785524114, 72789094197, 72793024233, 72785794129, 72788594266, 72797624217, 72785024157, 72797094240, 72798594276, 72792424223, 72792894263, 72781024243, 72781524237, 72788324220, 72698824219, 72793894274, 74206024207, 72782724110, 72793724222, 72792594227, 72782594239, 72794504205, 72792394225, 72784524163, 72792024227, 72785694176])\n"
     ]
    }
   ],
   "source": [
    "# Dictionaries to store the training and testing dataframes\n",
    "train_dataframes = {}\n",
    "test_dataframes = {}\n",
    "\n",
    "# Split each dataframe into training and testing sets\n",
    "for key, df in dataframes.items():\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, shuffle=False)\n",
    "    train_dataframes[key] = train_df\n",
    "    test_dataframes[key] = test_df\n",
    "    # Check if the maximum index of the training set is less than the minimum index of the testing set\n",
    "    if train_df.index.max() < test_df.index.min():\n",
    "        print(f\"{key}: Sequential split verified.\")\n",
    "    else:\n",
    "        print(f\"{key}: Sequential split NOT verified.\")\n",
    "\n",
    "# Print the keys of the training and testing dictionaries to verify\n",
    "print(train_dataframes.keys())\n",
    "print(test_dataframes.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([525, 27, 59])\n",
      "torch.Size([131, 27, 59])\n"
     ]
    }
   ],
   "source": [
    "def create_node_features_sequences(dataframes):\n",
    "    # Create a list to store the node features for each time step for input and desired output\n",
    "    node_features_sequence_input = []\n",
    "    node_features_sequence_output = []\n",
    "\n",
    "    # Iterate over the rows of the dataframes (assuming all dataframes have the same number of rows)\n",
    "    for i in range(len(next(iter(dataframes.values())))):\n",
    "        if i == len(next(iter(dataframes.values()))) - 1:\n",
    "            break\n",
    "        # Create a list to store the features of all nodes at the current time step for input\n",
    "        node_features_input = []\n",
    "        # Create a list to store the features of all nodes at the next time step for output\n",
    "        node_features_output = []\n",
    "\n",
    "        # Iterate over each dataframe and extract the features at the current row for input\n",
    "        # and the next row for output\n",
    "        for key, df in dataframes.items():\n",
    "            node_features_input.append((df.iloc[i].values - df.iloc[i].mean()) / df.iloc[i].std())\n",
    "            node_features_output.append(df.iloc[i + 1].values)\n",
    "\n",
    "        # Stack the features of all nodes to create a 2D array (num_nodes, num_features)\n",
    "        node_features_sequence_input.append(np.stack(node_features_input))\n",
    "        node_features_sequence_output.append(np.stack(node_features_output))\n",
    "\n",
    "    # Convert the lists to numpy arrays (time_steps, num_nodes, num_features)\n",
    "    node_features_sequence_input = np.array(node_features_sequence_input)\n",
    "    node_features_sequence_output = np.array(node_features_sequence_output)\n",
    "\n",
    "    # Convert the numpy arrays to torch tensors\n",
    "    node_features_sequence_input = torch.tensor(node_features_sequence_input, dtype=torch.float)\n",
    "    node_features_sequence_output = torch.tensor(node_features_sequence_output, dtype=torch.float)\n",
    "\n",
    "    return node_features_sequence_input, node_features_sequence_output\n",
    "\n",
    "# Call the function and print the shapes of the resulting tensors\n",
    "node_features_sequence_input_train, node_features_sequence_output_train = create_node_features_sequences(train_dataframes)\n",
    "node_features_sequence_input_test, node_features_sequence_output_test = create_node_features_sequences(test_dataframes)\n",
    "print(node_features_sequence_input_train.shape)\n",
    "print(node_features_sequence_output_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: tensor([[[-0.3513, -0.2447, -0.3517,  0.0810,  4.4760,  1.6202,  2.2340,\n",
      "           2.6113,  2.9742, -0.0595,  1.6904,  1.6897, -0.3517, -0.3467,\n",
      "          -0.3515, -0.3516, -0.3516, -0.3514, -0.3494, -0.3177, -0.3505,\n",
      "          -0.3467, -0.3517, -0.3466, -0.3516, -0.3510, -0.3514, -0.3517,\n",
      "          -0.3517, -0.3515, -0.3515, -0.3502, -0.3516, -0.3516, -0.3465,\n",
      "          -0.3514, -0.3516, -0.3516, -0.3517, -0.3163, -0.3511, -0.3512,\n",
      "          -0.3515, -0.3148, -0.3511, -0.3516, -0.3466, -0.3514, -0.3516,\n",
      "          -0.3512, -0.3365, -0.3515, -0.3514, -0.3513, -0.3513, -0.3516,\n",
      "          -0.3515, -0.3515, -0.3511]]])\n",
      "Standard Deviation: tensor([[[0.0163, 0.1418, 0.0161, 0.2685, 1.1921, 0.2288, 0.4864, 0.5617,\n",
      "          0.8244, 0.1007, 0.2498, 0.2488, 0.0161, 0.0229, 0.0165, 0.0163,\n",
      "          0.0163, 0.0165, 0.0216, 0.0314, 0.0179, 0.0229, 0.0161, 0.0277,\n",
      "          0.0163, 0.0172, 0.0166, 0.0162, 0.0161, 0.0165, 0.0164, 0.0197,\n",
      "          0.0163, 0.0162, 0.0279, 0.0166, 0.0163, 0.0162, 0.0161, 0.0311,\n",
      "          0.0171, 0.0172, 0.0163, 0.0306, 0.0170, 0.0163, 0.0230, 0.0166,\n",
      "          0.0162, 0.0169, 0.0368, 0.0167, 0.0165, 0.0172, 0.0174, 0.0163,\n",
      "          0.0164, 0.0168, 0.0170]]])\n"
     ]
    }
   ],
   "source": [
    "# Calculate mean and standard deviation of the training data\n",
    "mean = node_features_sequence_input_train.mean(dim=(0, 1), keepdim=True)\n",
    "std = node_features_sequence_input_train.std(dim=(0, 1), keepdim=True)\n",
    "\n",
    "# Normalize the training and testing data\n",
    "# node_features_sequence_input_train = (node_features_sequence_input_train - mean) / std\n",
    "# node_features_sequence_input_test = (node_features_sequence_input_test - mean) / std\n",
    "\n",
    "# Print the mean and standard deviation to verify\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Standard Deviation:\", std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">**ZE EPIC EDGE DATA**</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        STATION  LONGITUDE  LATITUDE  ELEVATION\n",
      "0  7.279372e+10 -122.28308  47.92322      167.1\n",
      "1  7.278462e+10 -118.28572  46.09456      356.7\n",
      "2  7.420712e+10 -122.58333  47.08333       91.4\n",
      "3  7.279762e+10 -122.54069  48.79910       45.9\n",
      "4  7.279239e+10 -123.93074  46.97288        4.5\n"
     ]
    }
   ],
   "source": [
    "# Import the location-datamap.csv file as a dataframe\n",
    "location_datamap_df = pd.read_csv('../location-datamap.csv')\n",
    "\n",
    "# Print the first few rows of the dataframe to verify\n",
    "print(location_datamap_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_distance(lat1, lon1, lat2, lon2, el1=0, el2=0):\n",
    "    # Radius of the Earth in kilometers\n",
    "    R = 6371.0\n",
    "\n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "\n",
    "    # Difference in coordinates\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "\n",
    "    # Haversine formula\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "\n",
    "    # Distance in kilometers\n",
    "    distance = R * c\n",
    "\n",
    "    # Elevation difference\n",
    "    height = el2 - el1\n",
    "\n",
    "    # Calculate the total distance considering elevation\n",
    "    total_distance = sqrt(distance**2 + height**2)\n",
    "\n",
    "    return total_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 1): 395.4679457314314, (0, 2): 129.15783117310457, (0, 3): 342.5330553210191, (0, 4): 438.10430559873856, (0, 5): 432.0504921333856, (0, 6): 437.11321302131137, (0, 7): 369.0087247919059, (0, 8): 503.7031148557305, (0, 9): 455.6811328123654, (0, 10): 474.59654599988056, (0, 11): 348.0950737502581, (0, 12): 128.6980383337319, (0, 13): 835.9738346074198, (0, 14): 171.59537540199554, (0, 15): 384.56540064951065, (0, 16): 373.58764051749137, (0, 17): 362.1843030623904, (0, 18): 32.53657446575969, (0, 19): 306.4908073667999, (0, 20): 404.50991220772727, (0, 21): 52.81240627430448, (0, 22): 404.8085057270694, (0, 23): 505.4288197147213, (0, 24): 286.0836462058196, (0, 25): 412.6227567328569, (0, 26): 272.19719043646785, (1, 2): 390.14029117618804, (1, 3): 727.6917541369755, (1, 4): 109.85904921372176, (1, 5): 798.995937782049, (1, 6): 802.4581375032445, (1, 7): 33.690956835057186, (1, 8): 865.1737430163511, (1, 9): 825.6499844120128, (1, 10): 860.6728795779842, (1, 11): 724.9963923066945, (1, 12): 495.92270395996894, (1, 13): 535.6266022869553, (1, 14): 314.54161595295784, (1, 15): 763.6141112163369, (1, 16): 759.3841400612968, (1, 17): 748.5486591666476, (1, 18): 416.12981588895354, (1, 19): 678.7815012654146, (1, 20): 784.7770299622911, (1, 21): 419.79175595055284, (1, 22): 786.18443118649, (1, 23): 886.2658783050533, (1, 24): 655.5895444605051, (1, 25): 798.318691116012, (1, 26): 157.5431429141368, (2, 3): 370.42190175405364, (2, 4): 460.5219705003162, (2, 5): 432.51979230044606, (2, 6): 417.3828379309705, (2, 7): 365.04041085309774, (2, 8): 508.89691489063597, (2, 9): 445.736559884783, (2, 10): 534.5520542530354, (2, 11): 372.6745336577991, (2, 12): 236.9285970548754, (2, 13): 831.0617187024466, (2, 14): 221.69557651346355, (2, 15): 470.9577296531585, (2, 16): 406.24280451088293, (2, 17): 399.71114545506987, (2, 18): 147.68778122386612, (2, 19): 313.8982068438734, (2, 20): 435.3656912848407, (2, 21): 130.20708355028395, (2, 22): 406.9699486305622, (2, 23): 539.0132078425393, (2, 24): 368.91523621650714, (2, 25): 452.3832910640423, (2, 26): 270.2281521689557, (3, 4): 775.0588941479449, (3, 5): 120.26611512741678, (3, 6): 165.52848353942377, (3, 7): 704.0879836514051, (3, 8): 185.28412975619443, (3, 9): 152.43166309928537, (3, 10): 187.27787071696568, (3, 11): 41.159846544112625, (3, 12): 266.9618608270944, (3, 13): 1096.8318771247875, (3, 14): 443.4038293955239, (3, 15): 225.02216925780087, (3, 16): 36.85266364881154, (3, 17): 37.9895765777111, (3, 18): 333.6917238582668, (3, 19): 76.27723241526142, (3, 20): 72.18185980856553, (3, 21): 309.0749255571238, (3, 22): 107.09860018045799, (3, 23): 171.23674671831446, (3, 24): 276.0731537992283, (3, 25): 85.99186556469607, (3, 26): 611.0889227645558, (4, 5): 856.701667766572, (4, 6): 865.4792367557994, (4, 7): 117.74480456011335, (4, 8): 921.4842980890041, (4, 9): 885.9031028312606, (4, 10): 890.5893542480043, (4, 11): 774.366157782777, (4, 12): 524.6337306289196, (4, 13): 540.0809528724242, (4, 14): 357.13279391570444, (4, 15): 780.4774458332262, (4, 16): 804.444912957628, (4, 17): 791.9148221844617, (4, 18): 454.1201925958629, (4, 19): 734.2694292819892, (4, 20): 830.6418814848106, (4, 21): 467.37603414879794, (4, 22): 840.9110561664348, (4, 23): 929.3740741921883, (4, 24): 673.3913688173589, (4, 25): 839.7968409183139, (4, 26): 209.78499150005845, (5, 6): 110.15242913456197, (5, 7): 777.4585937545535, (5, 8): 85.13925057078639, (5, 9): 74.97130998219406, (5, 10): 240.43942234567194, (5, 11): 103.9757703201679, (5, 12): 369.69695959787833, (5, 13): 1139.0521959187856, (5, 14): 516.0490346912795, (5, 15): 330.5216959336131, (5, 16): 117.91278236100351, (5, 17): 133.34207595548997, (5, 18): 428.7153028579556, (5, 19): 125.7282441840991, (5, 20): 101.97987733596389, (5, 21): 392.60154297249403, (5, 22): 108.31645122446824, (5, 23): 153.40108385187867, (5, 24): 392.1898886399527, (5, 25): 137.25092367847324, (5, 26): 690.174750212649, (6, 7): 778.6874797868297, (6, 8): 177.46961023734613, (6, 9): 48.71149879169117, (6, 10): 302.0846527899822, (6, 11): 172.27646695285324, (6, 12): 399.97805818217563, (6, 13): 1176.4852153080844, (6, 14): 545.6427005439592, (6, 15): 369.3977887607593, (6, 16): 175.55537172802033, (6, 17): 190.70446482220075, (6, 18): 431.096161984872, (6, 19): 156.64330089805705, (6, 20): 183.24573042058552, (6, 21): 405.4909815288361, (6, 22): 76.4859126118275, (6, 23): 231.7098185991757, (6, 24): 388.4359336032824, (6, 25): 205.33116784821942, (6, 26): 682.4260680955291, (7, 8): 845.1366146343045, (7, 9): 802.4776134962176, (7, 10): 837.0612376685274, (7, 11): 702.8044715545802, (7, 12): 472.23598917705993, (7, 13): 567.9966156275789, (7, 14): 299.672336109386, (7, 15): 738.0303386253106, (7, 16): 735.9632694864449, (7, 17): 725.1092500944276, (7, 18): 388.53428759844127, (7, 19): 656.1420174916371, (7, 20): 762.4710980022056, (7, 21): 395.81370216605853, (7, 22): 761.3237576215986, (7, 23): 864.2082908552868, (7, 24): 626.0309963149288, (7, 25): 775.1913077729471, (7, 26): 124.27275180830249, (8, 9): 132.5097212349861, (8, 10): 243.7073144688935, (8, 11): 163.47287592761103, (8, 12): 430.6615227573649, (8, 13): 1177.64872286148, (8, 14): 572.9402144031785, (8, 15): 365.1202761234219, (8, 16): 169.20923499165121, (8, 17): 183.39281681071296, (8, 18): 501.4584011725272, (8, 19): 202.41419212127153, (8, 20): 134.47678138943274, (8, 21): 461.61393826175714, (8, 22): 180.6112148305096, (8, 23): 128.08708542091654, (8, 24): 456.1447880245876, (8, 25): 164.101942102309, (8, 26): 763.0321197403028, (9, 10): 268.89465547374556, (9, 11): 154.53742133091865, (9, 12): 405.60664848136616, (9, 13): 1187.3176896736784, (9, 14): 557.1020923327553, (9, 15): 353.92282978101207, (9, 16): 153.1723115228679, (9, 17): 170.09372061722416, (9, 18): 449.65266636740307, (9, 19): 158.4233084920311, (9, 20): 151.2637681156068, (9, 21): 421.5198568724629, (9, 22): 75.9354339763056, (9, 23): 187.20830867381414, (9, 24): 396.00851322484175, (9, 25): 174.23493854899442, (9, 26): 708.5304039928845, (10, 11): 199.69966656538062, (10, 12): 366.6429361022303, (10, 13): 1213.3303112543872, (10, 14): 565.7742092205691, (10, 15): 158.75602862100996, (10, 16): 154.05814985745886, (10, 17): 150.49523188191975, (10, 18): 459.96707604415764, (10, 19): 261.56576121699044, (10, 20): 148.37325199067052, (10, 21): 447.531376407682, (10, 22): 236.6757463314917, (10, 23): 123.91187323342717, (10, 24): 313.3574949442864, (10, 25): 109.93432994721853, (10, 26): 745.579780595948, (11, 12): 271.20003239227077, (11, 13): 1077.0374760399814, (11, 14): 434.420916611982, (11, 15): 249.12813853110475, (11, 16): 54.60340565924095, (11, 17): 56.69074125354086, (11, 18): 342.83747887822733, (11, 19): 68.6532776654429, (11, 20): 64.41876337172579, (11, 21): 309.79708423419913, (11, 22): 128.34321457181807, (11, 23): 167.4998627149666, (11, 24): 308.66359444651886, (11, 25): 94.10375597750865, (11, 26): 614.9675457923169, (12, 13): 891.3237029023409, (12, 14): 217.85555378560997, (12, 15): 275.33106494313824, (12, 16): 290.27386357894756, (12, 17): 275.46064771106325, (12, 18): 121.40670159689395, (12, 19): 252.86361822595595, (12, 20): 317.66456605180196, (12, 21): 111.45482222908821, (12, 22): 352.74223587837065, (12, 23): 411.2440393694844, (12, 24): 228.97116863460516, (12, 25): 319.85603194172165, (12, 26): 386.6621826263452, (13, 14): 677.4867035602812, (13, 15): 1150.4386824494006, (13, 16): 1122.20080799066, (13, 17): 1112.0640517140919, (13, 18): 864.0517998527835, (13, 19): 1044.4930844703567, (13, 20): 1130.7362905229766, (13, 21): 833.7147080759753, (13, 22): 1170.6533446424978, (13, 23): 1218.574885845403, (13, 24): 1103.3829323700534, (13, 25): 1152.414996794818, (13, 26): 683.9489018806388, (14, 15): 489.8134161401565, (14, 16): 470.9676802550459, (14, 17): 459.0274176052015, (14, 18): 199.59384465449898, (14, 19): 400.7840441174383, (14, 20): 490.3741382897129, (14, 21): 162.11769623669784, (14, 22): 520.9373250801362, (14, 23): 587.5949150283049, (14, 24): 432.7632336033511, (14, 25): 504.8208347435879, (14, 26): 259.92185050812566, (15, 16): 213.47199631168857, (15, 17): 199.7906856396986, (15, 18): 363.3873739871944, (15, 19): 286.49766075950384, (15, 20): 235.6189290359573, (15, 21): 371.98475050597, (15, 22): 293.97835633481196, (15, 23): 268.7494270590419, (15, 24): 181.34059793217904, (15, 25): 201.61183866011444, (15, 26): 642.4950004501114, (16, 17): 17.716247962574563, (16, 18): 363.9821600522806, (16, 19): 109.51765254130393, (16, 20): 42.988933127483634, (16, 21): 340.3237239231559, (16, 22): 114.05670021657697, (16, 23): 136.43815241862657, (16, 24): 287.7450730575042, (16, 25): 49.41202473657626, (16, 26): 643.6682517559431, (17, 18): 352.2158347034709, (17, 19): 111.26188855442645, (17, 20): 52.561875466910436, (17, 21): 329.3368670529716, (17, 22): 128.01154758942099, (17, 23): 145.49531716211806, (17, 24): 275.6435250697721, (17, 25): 52.80087090274467, (17, 26): 633.1197418989807, (18, 19): 303.31516235822005, (18, 20): 396.9755284116139, (18, 21): 73.74026824660453, (18, 22): 394.74559894207687, (18, 23): 496.478460291939, (18, 24): 256.1592053257525, (18, 25): 402.1731282608459, (18, 26): 286.80868520838226, (19, 20): 129.67576043022495, (19, 21): 268.31498195846285, (19, 22): 128.38153131965652, (19, 23): 230.11767302669344, (19, 24): 305.30416194483115, (19, 25): 156.88223261226412, (19, 26): 566.3246868369048, (20, 21): 368.04423285388, (20, 22): 133.10933796080906, (20, 23): 103.88553424589651, (20, 24): 327.04970158433304, (20, 25): 41.59043713208587, (20, 26): 674.0342619771884, (21, 22): 375.88513957717953, (21, 23): 469.86938496204897, (21, 24): 296.22180486691445, (21, 25): 379.7335338263519, (21, 26): 308.0303254221726, (22, 23): 191.05105434406818, (22, 24): 321.52952520777865, (22, 25): 144.61575947894278, (22, 26): 662.014427725483, (23, 24): 393.67944122771735, (23, 25): 96.11729883125135, (23, 26): 776.1835969743106, (24, 25): 305.7627599001508, (24, 26): 516.3645983655035, (25, 26): 684.1746652775099}\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary to store the distances between nodes\n",
    "distances = {}\n",
    "\n",
    "# Define the number of nodes\n",
    "num_nodes = len(dataframes)\n",
    "\n",
    "# Iterate over each pair of nodes\n",
    "for i, j in itertools.combinations(range(num_nodes), 2):\n",
    "    # Get the station IDs for the nodes\n",
    "    station_i = list(dataframes.keys())[i]\n",
    "    station_j = list(dataframes.keys())[j]\n",
    "    \n",
    "    # Get the location data for the stations\n",
    "    location_i = location_datamap_df[location_datamap_df['STATION'] == station_i].iloc[0]\n",
    "    location_j = location_datamap_df[location_datamap_df['STATION'] == station_j].iloc[0]\n",
    "    \n",
    "    # Calculate the distance between the stations\n",
    "    distance = haversine_distance(location_i['LATITUDE'], location_i['LONGITUDE'], location_j['LATITUDE'], location_j['LONGITUDE'], location_i['ELEVATION'], location_j['ELEVATION'])\n",
    "    \n",
    "    # Store the distance in the dictionary\n",
    "    distances[(i, j)] = distance\n",
    "\n",
    "# Print the distances to verify\n",
    "print(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 1): 0.314567998140773, (0, 2): 0.09280158354609237, (0, 3): 0.2704871307176606, (0, 4): 0.35007289315695667, (0, 5): 0.34503165576699535, (0, 6): 0.349247573218238, (0, 7): 0.2925344130835223, (0, 8): 0.40469948049003845, (0, 9): 0.3647097760165542, (0, 10): 0.380461349591329, (0, 11): 0.27511883194691195, (0, 12): 0.0924186968141592, (0, 13): 0.6813937634553494, (0, 14): 0.1281409173278857, (0, 15): 0.3054890401868689, (0, 16): 0.2963474478414338, (0, 17): 0.28685146130699407, (0, 18): 0.012341441395062182, (0, 19): 0.2404733998610767, (0, 20): 0.32209758254900717, (0, 21): 0.029225886548649996, (0, 22): 0.3223462325648563, (0, 23): 0.4061365396113629, (0, 24): 0.2234795918330484, (0, 25): 0.32885345228187846, (0, 26): 0.21191581960267647, (1, 2): 0.31013146049414686, (1, 3): 0.5912232162697533, (1, 4): 0.0767307644250279, (1, 5): 0.6506008827124798, (1, 6): 0.6534839861952507, (1, 7): 0.013302738864123758, (1, 8): 0.7057096216985912, (1, 9): 0.672796706424883, (1, 10): 0.7019615840059098, (1, 11): 0.5889786874424193, (1, 12): 0.39822044070107643, (1, 13): 0.4312833650740786, (1, 14): 0.24717760994224985, (1, 15): 0.6211371095009285, (1, 16): 0.6176146539665304, (1, 17): 0.6085915428751594, (1, 18): 0.33177391189757455, (1, 19): 0.5504938153822416, (1, 20): 0.6387602651983096, (1, 21): 0.3348233466487418, (1, 22): 0.6399322609518483, (1, 23): 0.7232738333579158, (1, 24): 0.5311810036379734, (1, 25): 0.6500369140281799, (1, 26): 0.11643909661014204, (2, 3): 0.2937112185105451, (2, 4): 0.3687409230102465, (2, 5): 0.34542245960706086, (2, 6): 0.3328173503194575, (2, 7): 0.2892298493208762, (2, 8): 0.40902455246026, (2, 9): 0.3564285573835974, (2, 10): 0.4303885486485463, (2, 11): 0.295587069533041, (2, 12): 0.18254633990790353, (2, 13): 0.6773032604185943, (2, 14): 0.16986123271804443, (2, 15): 0.37743117082429484, (2, 16): 0.32354062692449737, (2, 17): 0.31810146960009433, (2, 18): 0.108232167518433, (2, 19): 0.24664181905997015, (2, 20): 0.34779234636526585, (2, 21): 0.09367533533009026, (2, 22): 0.3241461470887702, (2, 23): 0.43410351846162026, (2, 24): 0.29245656164252043, (2, 25): 0.36196353957848576, (2, 26): 0.21027612763111883, (3, 4): 0.6306676092371719, (3, 5): 0.08539711830331884, (3, 6): 0.1230887890663378, (3, 7): 0.571567471837432, (3, 8): 0.13954005617934356, (3, 9): 0.11218257577279916, (3, 10): 0.1412003190095184, (3, 11): 0.019522363284050027, (3, 12): 0.20755616439912683, (3, 13): 0.8986200332994612, (3, 14): 0.3544860052665791, (3, 15): 0.17263141118817832, (3, 16): 0.01593561063938, (3, 17): 0.01688236065060862, (3, 18): 0.2631246226056817, (3, 19): 0.0487659268171087, (3, 20): 0.04535555653912475, (3, 21): 0.24262529193963123, (3, 22): 0.07443203504408213, (3, 23): 0.12784227378037094, (3, 24): 0.21514347957903632, (3, 25): 0.05685566597788288, (3, 26): 0.4941236679182537, (4, 5): 0.6986546070761243, (4, 6): 0.7059640177863663, (4, 7): 0.08329752848669511, (4, 8): 0.7526015316172568, (4, 9): 0.7229717366228394, (4, 10): 0.7268741538340823, (4, 11): 0.6300907416997995, (4, 12): 0.4221291887944986, (4, 13): 0.4349926697706931, (4, 14): 0.28264488029293566, (4, 15): 0.6351798403311119, (4, 16): 0.6551384485871659, (4, 17): 0.6447041723302557, (4, 18): 0.36340992258896465, (4, 19): 0.5967006929164721, (4, 20): 0.676953646230553, (4, 21): 0.37444855872377714, (4, 22): 0.6855051729112698, (4, 23): 0.7591716439138168, (4, 24): 0.5460052500523883, (4, 25): 0.6845773241096111, (4, 26): 0.15994284212845428, (5, 6): 0.07697507288198122, (5, 7): 0.6326659290484358, (5, 8): 0.0561456615135665, (5, 9): 0.04767843625671288, (5, 10): 0.18546993572513168, (5, 11): 0.07183153756521501, (5, 12): 0.29310753200381906, (5, 13): 0.9337784753192767, (5, 14): 0.41498039070384635, (5, 15): 0.2604848215294762, (5, 16): 0.08343740989787128, (5, 17): 0.0962859610160021, (5, 18): 0.34225431864319356, (5, 19): 0.08994563790785139, (5, 20): 0.07016948266446262, (5, 21): 0.3121810371209556, (5, 22): 0.07544618525759716, (5, 23): 0.11298984877064547, (5, 24): 0.3118382371280546, (5, 25): 0.09954100503173634, (5, 26): 0.5599814008380295, (6, 7): 0.6336892685103085, (6, 8): 0.1330326128614309, (6, 9): 0.025810907172023778, (6, 10): 0.23680422978741508, (6, 11): 0.1287080877918951, (6, 12): 0.3183237378327452, (6, 13): 0.9649503536806591, (6, 14): 0.43962414552985546, (6, 15): 0.2928584012338173, (6, 16): 0.1314385547025949, (6, 17): 0.14405377236126038, (6, 18): 0.344236949280813, (6, 19): 0.11568976443423645, (6, 20): 0.13784260464649475, (6, 21): 0.322914555746484, (6, 22): 0.048939702638827395, (6, 23): 0.17820046747041107, (6, 24): 0.30871217805811385, (6, 25): 0.15623397622922458, (6, 26): 0.5535287827923442, (7, 8): 0.6890239538356585, (7, 9): 0.6535002045846253, (7, 10): 0.6822992847438707, (7, 11): 0.5704986432040403, (7, 12): 0.378495625443329, (7, 13): 0.45823908852017337, (7, 14): 0.2347954032657112, (7, 15): 0.5998325430982321, (7, 16): 0.5981112171455704, (7, 17): 0.5890726683525555, (7, 18): 0.30879408111652235, (7, 19): 0.531641068639551, (7, 20): 0.6201852795535281, (7, 21): 0.3148559224840052, (7, 22): 0.6192298462124066, (7, 23): 0.7049056534956674, (7, 24): 0.5065664926429994, (7, 25): 0.6307778750260209, (7, 26): 0.08873359484975862, (8, 9): 0.09559282804077418, (8, 10): 0.18819123198776505, (8, 11): 0.12137700755686981, (8, 12): 0.34387500890432254, (8, 13): 0.965919250032562, (8, 14): 0.46235580852338326, (8, 15): 0.289296356125095, (8, 16): 0.12615388876759553, (8, 17): 0.13796508899684823, (8, 18): 0.4028302232665897, (8, 19): 0.15380490120329926, (8, 20): 0.09723087276343585, (8, 21): 0.3696502454958358, (8, 22): 0.13564874476410202, (8, 23): 0.09190993342308056, (8, 24): 0.365095879091134, (8, 25): 0.12190085454006432, (8, 26): 0.6206524633838301, (9, 10): 0.20916567494906027, (9, 11): 0.11393611958320622, (9, 12): 0.3230108759534436, (9, 13): 0.973970961122591, (9, 14): 0.44916681060906877, (9, 15): 0.27997182283768707, (9, 16): 0.11279934147711913, (9, 17): 0.12689043310151676, (9, 18): 0.359689646040564, (9, 19): 0.11717204347843128, (9, 20): 0.11121002584324408, (9, 21): 0.3362624010614717, (9, 22): 0.04848129844523104, (9, 23): 0.14114239208876594, (9, 24): 0.31501814895483, (9, 25): 0.13033898050012765, (9, 26): 0.5752668417726906, (10, 11): 0.15154441402332883, (10, 12): 0.29056433216388416, (10, 13): 0.9956326461537036, (10, 14): 0.45638840740176295, (10, 15): 0.11744911200130542, (10, 16): 0.11353701226254376, (10, 17): 0.11057003691412082, (10, 18): 0.36827884159728613, (10, 19): 0.20306262998976654, (10, 20): 0.108802984719709, (10, 21): 0.3579231683779967, (10, 22): 0.18233578163284334, (10, 23): 0.08843307773351292, (10, 24): 0.24619154799347698, (10, 25): 0.07679345351358663, (10, 26): 0.6061192464057483, (11, 12): 0.21108544872242438, (11, 13): 0.882136493555179, (11, 14): 0.3470055970818329, (11, 15): 0.19270535537515096, (11, 16): 0.030717318869188645, (11, 17): 0.03245552145894557, (11, 18): 0.27074063562457035, (11, 19): 0.0424171739253779, (11, 20): 0.03889093514911131, (11, 21): 0.24322666053897657, (11, 22): 0.09212322176762135, (11, 23): 0.12473043039975776, (11, 24): 0.24228276110575214, (11, 25): 0.0636107411856644, (11, 26): 0.49735354269735277, (12, 13): 0.7274856734844146, (12, 14): 0.16666350185554782, (12, 15): 0.21452551437257505, (12, 16): 0.22696894290314232, (12, 17): 0.21463342280062572, (12, 18): 0.08634692740948313, (12, 19): 0.19581602933544076, (12, 20): 0.24977820754826785, (12, 21): 0.0780596244298823, (12, 22): 0.27898869804231324, (12, 23): 0.32770534265441786, (12, 24): 0.17591989099106883, (12, 25): 0.25160312333834234, (12, 26): 0.3072351091334447, (13, 14): 0.5494155887997888, (13, 15): 0.9432604294572675, (13, 16): 0.919745692944629, (13, 17): 0.9113044360332913, (13, 18): 0.7047753375720718, (13, 19): 0.8550355588215105, (13, 20): 0.9268535091879839, (13, 21): 0.6795125041129281, (13, 22): 0.9600939363792285, (13, 23): 1.0, (13, 24): 0.9040753425578563, (13, 25): 0.9449061804916289, (13, 26): 0.5547969035661553, (14, 15): 0.39313300773679066, (14, 16): 0.3774394570634686, (14, 17): 0.3674963527935975, (14, 18): 0.15145629215157527, (14, 19): 0.3189949125321117, (14, 20): 0.39359994208848503, (14, 21): 0.12024849863153748, (14, 22): 0.41905105334027043, (14, 23): 0.47455932704156895, (14, 24): 0.3456251823049916, (14, 25): 0.4056302477365713, (14, 26): 0.20169368392317283, (15, 16): 0.16301314923648366, (15, 17): 0.15162020901821552, (15, 18): 0.2878533035612373, (15, 19): 0.22382435727056416, (15, 20): 0.1814557302577726, (15, 21): 0.2950126612471121, (15, 22): 0.23005381287784274, (15, 23): 0.20904473780448538, (15, 24): 0.13625612941260268, (15, 25): 0.15313675140127786, (15, 26): 0.5202766860128114, (16, 17): 0.0, (16, 18): 0.2883486042122239, (16, 19): 0.07644647061920598, (16, 20): 0.021045512242361872, (16, 21): 0.268647337649462, (16, 22): 0.0802263057572332, (16, 23): 0.09886417993825188, (16, 24): 0.22486312424833235, (16, 25): 0.026394261384406467, (16, 26): 0.5212536963525964, (17, 18): 0.2785503440526814, (17, 19): 0.07789896132718613, (17, 20): 0.029017260154592706, (17, 21): 0.2594981701091805, (17, 22): 0.09184703023938133, (17, 23): 0.10640642051326218, (17, 24): 0.2147857116320833, (17, 25): 0.02921628061236748, (17, 26): 0.5124695568009504, (18, 19): 0.23782892122853866, (18, 20): 0.3158234187478483, (18, 21): 0.04665330165988813, (18, 22): 0.3139664728932817, (18, 23): 0.39868323983033105, (18, 24): 0.19856038824316935, (18, 25): 0.32015165496588943, (18, 26): 0.22408335898739135, (19, 20): 0.09323288265223323, (19, 21): 0.20868295908476445, (19, 22): 0.09215512955978745, (19, 23): 0.17687462817321511, (19, 24): 0.23948523573872763, (19, 25): 0.11588873182862379, (19, 26): 0.45684681074664474, (20, 21): 0.29173124449431487, (20, 22): 0.09609215136402571, (20, 23): 0.0717563946038166, (20, 24): 0.2575935616927636, (20, 25): 0.019880932206643945, (20, 26): 0.5465406112843839, (21, 22): 0.2982606614264556, (21, 23): 0.3765248654051755, (21, 24): 0.2319220165625477, (21, 25): 0.30146536356854725, (21, 26): 0.24175541425211852, (22, 23): 0.1443423904474645, (22, 24): 0.25299670390916407, (22, 25): 0.10567397986169143, (22, 26): 0.5365312447590309, (23, 24): 0.31307864340134484, (23, 25): 0.06528749379435833, (23, 26): 0.6316041914383448, (24, 25): 0.23986712744592156, (24, 26): 0.4152431724037643, (25, 26): 0.5549849052091039}\n"
     ]
    }
   ],
   "source": [
    "# Extract the distance values from the dictionary\n",
    "distance_values = np.array(list(distances.values()))\n",
    "\n",
    "# Perform Min-Max normalization\n",
    "min_distance = distance_values.min()\n",
    "max_distance = distance_values.max()\n",
    "normalized_distances = (distance_values - min_distance) / (max_distance - min_distance)\n",
    "\n",
    "# Update the distances dictionary with normalized values\n",
    "normalized_distances_dict = {key: normalized_distances[i] for i, key in enumerate(distances.keys())}\n",
    "\n",
    "# Print the normalized distances to verify\n",
    "print(normalized_distances_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create edge index and edge attributes from distances dictionary\n",
    "edge_index = []\n",
    "edge_attr = []\n",
    "\n",
    "for (i, j), distance in distances.items():\n",
    "    edge_index.append([i, j])\n",
    "    edge_index.append([j, i])  # Assuming undirected graph\n",
    "    edge_attr.append([distance])\n",
    "    edge_attr.append([distance])  # Assuming undirected graph\n",
    "\n",
    "edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "edge_attr = torch.tensor(edge_attr, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">**ZE EPIC GPU IMPORT**</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move data to device\n",
    "node_features_sequence_input_train = node_features_sequence_input_train.to(device)\n",
    "node_features_sequence_output_test = node_features_sequence_output_test.to(device)\n",
    "edge_index = edge_index.to(device)\n",
    "edge_attr = edge_attr.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">**ZE EPIC BATCHING**</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 525\n",
      "Number of full batches: 525\n"
     ]
    }
   ],
   "source": [
    "batch_size = node_features_sequence_input_train.shape[0] # Adjust this value based on your GPU memory capacity\n",
    "# Calculate the number of full batches\n",
    "num_full_batches_train = (node_features_sequence_input_train.size(0) // batch_size) * batch_size\n",
    "\n",
    "# Trim the input and output tensors to have a size divisible by the batch size\n",
    "trimmed_input_train = node_features_sequence_input_train[:num_full_batches_train]\n",
    "trimmed_output_train = node_features_sequence_output_train[:num_full_batches_train]\n",
    "\n",
    "# Create batches of data\n",
    "batched_input_train = trimmed_input_train.view(-1, batch_size, trimmed_input_train.size(1), trimmed_input_train.size(2))\n",
    "batched_output_train = trimmed_output_train.view(-1, batch_size, trimmed_output_train.size(1), trimmed_output_train.size(2))\n",
    "\n",
    "# Adjust the number of batches\n",
    "num_batches_train = batched_input_train.size(0)\n",
    "\n",
    "# Print the batch size and number of full batches to verify\n",
    "print(\"Batch size:\", batch_size)\n",
    "print(\"Number of full batches:\", num_full_batches_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 131\n",
      "Number of full batches: 525\n"
     ]
    }
   ],
   "source": [
    "batch_size = node_features_sequence_input_test.shape[0]  # Adjust this value based on your GPU memory capacity\n",
    "# Calculate the number of full batches\n",
    "num_full_batches_test = (node_features_sequence_input_test.size(0) // batch_size) * batch_size\n",
    "\n",
    "# Trim the input and output tensors to have a size divisible by the batch size\n",
    "trimmed_input_test = node_features_sequence_input_test[:num_full_batches_test]\n",
    "trimmed_output_test = node_features_sequence_output_test[:num_full_batches_test]\n",
    "\n",
    "# Create batches of data\n",
    "batched_input_test = trimmed_input_test.view(-1, batch_size, trimmed_input_test.size(1), trimmed_input_test.size(2))\n",
    "batched_output_test = trimmed_output_test.view(-1, batch_size, trimmed_output_test.size(1), trimmed_output_test.size(2))\n",
    "\n",
    "# Adjust the number of batches\n",
    "num_batches_test = batched_input_test.size(0)\n",
    "print(\"Batch size:\", batch_size)\n",
    "print(\"Number of full batches:\", num_full_batches_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">**ZE EPIC MODEL**</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNWithTransformer(torch.nn.Module):\n",
    "    def __init__(self, in_channels, edge_in_channels, hidden_channels, out_channels, num_transformer_layers, lstm_hidden_size, lstm_num_layers):\n",
    "        super(GNNWithTransformer, self).__init__()\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels, edge_dim=edge_in_channels)\n",
    "        self.conv2 = GATConv(hidden_channels, hidden_channels, edge_dim=edge_in_channels)\n",
    "        self.conv3 = GATConv(hidden_channels, hidden_channels, edge_dim=edge_in_channels)\n",
    "        self.batch_norm1 = BatchNorm(hidden_channels)\n",
    "        self.batch_norm2 = BatchNorm(hidden_channels)\n",
    "        self.batch_norm3 = BatchNorm(hidden_channels)\n",
    "        self.dropout = torch.nn.Dropout(p=0.5)\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(hidden_channels, lstm_hidden_size, lstm_num_layers, batch_first=True)\n",
    "        \n",
    "        # Transformer encoder layer\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=hidden_channels, nhead=16),\n",
    "            num_layers=num_transformer_layers\n",
    "        )\n",
    "        \n",
    "        self.output_layer = torch.nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # Calculate edge weights as the inverse of edge attributes\n",
    "        edge_weight = 1.0 / edge_attr\n",
    "\n",
    "        # Apply the first GAT convolution with edge weights\n",
    "        x1 = self.conv1(x, edge_index, edge_weight)\n",
    "        x1 = self.batch_norm1(x1)\n",
    "        x1 = F.relu(x1)\n",
    "        x1 = self.dropout(x1)\n",
    "        \n",
    "        # Apply the second GAT convolution with edge weights\n",
    "        x2 = self.conv2(x1, edge_index, edge_weight)\n",
    "        x2 = self.batch_norm2(x2)\n",
    "        x2 = F.relu(x2)\n",
    "        x2 = self.dropout(x2)\n",
    "        \n",
    "        # Apply the third GAT convolution with edge weights\n",
    "        x3 = self.conv3(x2, edge_index, edge_weight)\n",
    "        x3 = self.batch_norm3(x3)\n",
    "        x3 = F.relu(x3)\n",
    "        x3 = self.dropout(x3)\n",
    "        \n",
    "        # Add residual connections\n",
    "        x = x1 + x2 + x3 \n",
    "        \n",
    "        # Prepare data for Transformer\n",
    "        x = x.unsqueeze(0)  # Add batch dimension\n",
    "        \n",
    "        # Apply LSTM\n",
    "        x, _ = self.lstm(x)\n",
    "        \n",
    "        # Apply Transformer encoder\n",
    "        x = self.transformer_encoder(x)\n",
    "        \n",
    "        x = x.squeeze(0)  # Remove batch dimension\n",
    "        \n",
    "        # Apply the output layer\n",
    "        x = self.output_layer(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">**ZE EPIC MODEL SETUP**</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input channels: 59\n",
      "Number of output channels: 59\n",
      "Number of hidden channels: 1024\n",
      "Number of transformer layers: 2\n",
      "LSTM hidden size: 1024\n",
      "LSTM number of layers: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/HDD/RyanFolder/projects/Weather-Predition-Project/.venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters\n",
    "in_channels = node_features_sequence_input_train.shape[2]\n",
    "out_channels = in_channels  \n",
    "edge_in_channels = 1\n",
    "hidden_channels = 1024\n",
    "num_transformer_layers = 2\n",
    "lstm_hidden_size = 1024\n",
    "lstm_num_layers = 1\n",
    "\n",
    "print(\"Number of input channels:\", in_channels)\n",
    "print(\"Number of output channels:\", out_channels)\n",
    "print(\"Number of hidden channels:\", hidden_channels)\n",
    "print(\"Number of transformer layers:\", num_transformer_layers)\n",
    "print(\"LSTM hidden size:\", lstm_hidden_size)\n",
    "print(\"LSTM number of layers:\", lstm_num_layers)\n",
    "\n",
    "model = GNNWithTransformer(in_channels, edge_in_channels, hidden_channels, out_channels, num_transformer_layers, lstm_hidden_size, lstm_num_layers).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of epochs: 1000\n",
      "Learning rate: 0.8\n",
      "Scheduler mode: min\n",
      "Scheduler factor: 0.8\n",
      "Scheduler patience: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/HDD/RyanFolder/projects/Weather-Predition-Project/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters\n",
    "num_epochs = 1000  # Adjust the number of epochs as needed\n",
    "learning_rate = 0.005\n",
    "scheduler_mode = 'min'\n",
    "scheduler_factor = 0.8\n",
    "scheduler_patience = 10\n",
    "swa_start = 100  # Epoch to start SWA\n",
    "# num_warmpup_steps = 10 \n",
    "# num_training_steps = num_epochs * num_batches_train\n",
    "\n",
    "#Print all parameter \n",
    "print(\"Number of epochs:\", num_epochs)\n",
    "print(\"Learning rate:\", learning_rate)\n",
    "print(\"Scheduler mode:\", scheduler_mode)\n",
    "print(\"Scheduler factor:\", scheduler_factor)\n",
    "print(\"Scheduler patience:\", scheduler_patience)\n",
    "# print(\"Number of warmup steps:\", num_warmpup_steps)\n",
    "# print(\"Number of training steps:\", num_training_steps)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "swa_model = AveragedModel(model)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=scheduler_mode, factor=scheduler_factor, patience=scheduler_patience, verbose=True,min_lr=1e-5)\n",
    "swa_scheduler = SWALR(optimizer, swa_lr=0.005)\n",
    "swa_model = swa_model.to(device)\n",
    "loss_fn = torch.nn.SmoothL1Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">**ZE EPIC MODEL TRAIN**</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 5.267284870147705, Average MAE: 5.595034599304199, Learning Rate: 0.8, Current Patience: 0\n",
      "Epoch 2, Average Loss: 294.6548767089844, Average MAE: 295.15484619140625, Learning Rate: 0.8, Current Patience: 1\n",
      "Epoch 3, Average Loss: 1637.05078125, Average MAE: 1637.5506591796875, Learning Rate: 0.8, Current Patience: 2\n",
      "Epoch 4, Average Loss: 879.1227416992188, Average MAE: 879.622802734375, Learning Rate: 0.8, Current Patience: 3\n",
      "Epoch 5, Average Loss: 213.731689453125, Average MAE: 214.2307891845703, Learning Rate: 0.8, Current Patience: 4\n",
      "Epoch 6, Average Loss: 460.1806945800781, Average MAE: 460.68072509765625, Learning Rate: 0.8, Current Patience: 5\n",
      "Epoch 7, Average Loss: 227.9022674560547, Average MAE: 228.40225219726562, Learning Rate: 0.8, Current Patience: 6\n",
      "Epoch 8, Average Loss: 194.68511962890625, Average MAE: 195.18411254882812, Learning Rate: 0.8, Current Patience: 7\n",
      "Epoch 9, Average Loss: 309.774169921875, Average MAE: 310.2741394042969, Learning Rate: 0.8, Current Patience: 8\n",
      "Epoch 10, Average Loss: 204.24334716796875, Average MAE: 204.74334716796875, Learning Rate: 0.8, Current Patience: 9\n",
      "Epoch 11, Average Loss: 64.2659683227539, Average MAE: 64.76467895507812, Learning Rate: 0.8, Current Patience: 10\n",
      "Epoch 12, Average Loss: 123.16011810302734, Average MAE: 123.66011047363281, Learning Rate: 0.6400000000000001, Current Patience: 0\n",
      "Epoch 13, Average Loss: 65.01519775390625, Average MAE: 65.51374816894531, Learning Rate: 0.6400000000000001, Current Patience: 1\n",
      "Epoch 14, Average Loss: 71.19226837158203, Average MAE: 71.69041442871094, Learning Rate: 0.6400000000000001, Current Patience: 2\n",
      "Epoch 15, Average Loss: 78.16645812988281, Average MAE: 78.66645812988281, Learning Rate: 0.6400000000000001, Current Patience: 3\n",
      "Epoch 16, Average Loss: 64.21896362304688, Average MAE: 64.7186279296875, Learning Rate: 0.6400000000000001, Current Patience: 4\n",
      "Epoch 17, Average Loss: 64.70562744140625, Average MAE: 65.20429992675781, Learning Rate: 0.6400000000000001, Current Patience: 5\n",
      "Epoch 18, Average Loss: 60.34947967529297, Average MAE: 60.849266052246094, Learning Rate: 0.6400000000000001, Current Patience: 6\n",
      "Epoch 19, Average Loss: 31.992639541625977, Average MAE: 32.49103927612305, Learning Rate: 0.6400000000000001, Current Patience: 7\n",
      "Epoch 20, Average Loss: 49.550697326660156, Average MAE: 50.049224853515625, Learning Rate: 0.6400000000000001, Current Patience: 8\n",
      "Epoch 21, Average Loss: 61.76700973510742, Average MAE: 62.266761779785156, Learning Rate: 0.6400000000000001, Current Patience: 9\n",
      "Epoch 22, Average Loss: 48.0513916015625, Average MAE: 48.54859161376953, Learning Rate: 0.6400000000000001, Current Patience: 10\n",
      "Epoch 23, Average Loss: 36.40334701538086, Average MAE: 36.90271759033203, Learning Rate: 0.5120000000000001, Current Patience: 0\n",
      "Epoch 24, Average Loss: 54.66264724731445, Average MAE: 55.15951156616211, Learning Rate: 0.5120000000000001, Current Patience: 1\n",
      "Epoch 25, Average Loss: 60.13397979736328, Average MAE: 60.63322448730469, Learning Rate: 0.5120000000000001, Current Patience: 2\n",
      "Epoch 26, Average Loss: 40.94694900512695, Average MAE: 41.44617462158203, Learning Rate: 0.5120000000000001, Current Patience: 3\n",
      "Epoch 27, Average Loss: 18.817655563354492, Average MAE: 19.310226440429688, Learning Rate: 0.5120000000000001, Current Patience: 4\n",
      "Epoch 28, Average Loss: 34.624969482421875, Average MAE: 35.12057113647461, Learning Rate: 0.5120000000000001, Current Patience: 5\n",
      "Epoch 29, Average Loss: 38.793540954589844, Average MAE: 39.2919921875, Learning Rate: 0.5120000000000001, Current Patience: 6\n",
      "Epoch 30, Average Loss: 26.649709701538086, Average MAE: 27.14518165588379, Learning Rate: 0.5120000000000001, Current Patience: 7\n",
      "Epoch 31, Average Loss: 23.18355941772461, Average MAE: 23.680648803710938, Learning Rate: 0.5120000000000001, Current Patience: 8\n",
      "Epoch 32, Average Loss: 27.158878326416016, Average MAE: 27.655881881713867, Learning Rate: 0.5120000000000001, Current Patience: 9\n",
      "Epoch 33, Average Loss: 34.87451934814453, Average MAE: 35.373069763183594, Learning Rate: 0.5120000000000001, Current Patience: 10\n",
      "Epoch 34, Average Loss: 15.833855628967285, Average MAE: 16.328798294067383, Learning Rate: 0.40960000000000013, Current Patience: 0\n",
      "Epoch 35, Average Loss: 30.532073974609375, Average MAE: 31.026079177856445, Learning Rate: 0.40960000000000013, Current Patience: 1\n",
      "Epoch 36, Average Loss: 27.198368072509766, Average MAE: 27.697078704833984, Learning Rate: 0.40960000000000013, Current Patience: 2\n",
      "Epoch 37, Average Loss: 27.718713760375977, Average MAE: 28.21764373779297, Learning Rate: 0.40960000000000013, Current Patience: 3\n",
      "Epoch 38, Average Loss: 25.475528717041016, Average MAE: 25.97167205810547, Learning Rate: 0.40960000000000013, Current Patience: 4\n",
      "Epoch 39, Average Loss: 25.062219619750977, Average MAE: 25.559114456176758, Learning Rate: 0.40960000000000013, Current Patience: 5\n",
      "Epoch 40, Average Loss: 18.2834415435791, Average MAE: 18.767675399780273, Learning Rate: 0.40960000000000013, Current Patience: 6\n",
      "Epoch 41, Average Loss: 10.311964988708496, Average MAE: 10.799442291259766, Learning Rate: 0.40960000000000013, Current Patience: 7\n",
      "Epoch 42, Average Loss: 20.97203826904297, Average MAE: 21.465185165405273, Learning Rate: 0.40960000000000013, Current Patience: 8\n",
      "Epoch 43, Average Loss: 11.307262420654297, Average MAE: 11.796432495117188, Learning Rate: 0.40960000000000013, Current Patience: 9\n",
      "Epoch 44, Average Loss: 12.633837699890137, Average MAE: 13.116860389709473, Learning Rate: 0.40960000000000013, Current Patience: 10\n",
      "Epoch 45, Average Loss: 17.175304412841797, Average MAE: 17.669818878173828, Learning Rate: 0.32768000000000014, Current Patience: 0\n",
      "Epoch 46, Average Loss: 12.14061164855957, Average MAE: 12.630250930786133, Learning Rate: 0.32768000000000014, Current Patience: 1\n",
      "Epoch 47, Average Loss: 10.535523414611816, Average MAE: 11.026100158691406, Learning Rate: 0.32768000000000014, Current Patience: 2\n",
      "Epoch 48, Average Loss: 11.100326538085938, Average MAE: 11.59093189239502, Learning Rate: 0.32768000000000014, Current Patience: 3\n",
      "Epoch 49, Average Loss: 9.494460105895996, Average MAE: 9.979662895202637, Learning Rate: 0.32768000000000014, Current Patience: 4\n",
      "Epoch 50, Average Loss: 8.344156265258789, Average MAE: 8.825897216796875, Learning Rate: 0.32768000000000014, Current Patience: 5\n",
      "Epoch 51, Average Loss: 9.41270637512207, Average MAE: 9.904436111450195, Learning Rate: 0.32768000000000014, Current Patience: 6\n",
      "Epoch 52, Average Loss: 7.119825839996338, Average MAE: 7.607773780822754, Learning Rate: 0.32768000000000014, Current Patience: 7\n",
      "Epoch 53, Average Loss: 6.860301971435547, Average MAE: 7.34328031539917, Learning Rate: 0.32768000000000014, Current Patience: 8\n",
      "Epoch 54, Average Loss: 9.352360725402832, Average MAE: 9.836335182189941, Learning Rate: 0.32768000000000014, Current Patience: 9\n",
      "Epoch 55, Average Loss: 6.98665714263916, Average MAE: 7.47611141204834, Learning Rate: 0.32768000000000014, Current Patience: 10\n",
      "Epoch 56, Average Loss: 9.208305358886719, Average MAE: 9.696931838989258, Learning Rate: 0.2621440000000001, Current Patience: 0\n",
      "Epoch 57, Average Loss: 9.200660705566406, Average MAE: 9.687485694885254, Learning Rate: 0.2621440000000001, Current Patience: 1\n",
      "Epoch 58, Average Loss: 7.775983810424805, Average MAE: 8.263202667236328, Learning Rate: 0.2621440000000001, Current Patience: 2\n",
      "Epoch 59, Average Loss: 8.186777114868164, Average MAE: 8.673089027404785, Learning Rate: 0.2621440000000001, Current Patience: 3\n",
      "Epoch 60, Average Loss: 6.527817726135254, Average MAE: 7.006962299346924, Learning Rate: 0.2621440000000001, Current Patience: 4\n",
      "Epoch 61, Average Loss: 6.825080394744873, Average MAE: 7.306107521057129, Learning Rate: 0.2621440000000001, Current Patience: 5\n",
      "Epoch 62, Average Loss: 5.299886226654053, Average MAE: 5.782619476318359, Learning Rate: 0.2621440000000001, Current Patience: 6\n",
      "Epoch 63, Average Loss: 5.997507572174072, Average MAE: 6.4851837158203125, Learning Rate: 0.2621440000000001, Current Patience: 7\n",
      "Epoch 64, Average Loss: 6.303822994232178, Average MAE: 6.785281181335449, Learning Rate: 0.2621440000000001, Current Patience: 8\n",
      "Epoch 65, Average Loss: 4.8715057373046875, Average MAE: 5.34993314743042, Learning Rate: 0.2621440000000001, Current Patience: 0\n",
      "Epoch 66, Average Loss: 4.881227493286133, Average MAE: 5.360682487487793, Learning Rate: 0.2621440000000001, Current Patience: 1\n",
      "Epoch 67, Average Loss: 5.039183616638184, Average MAE: 5.5129313468933105, Learning Rate: 0.2621440000000001, Current Patience: 2\n",
      "Epoch 68, Average Loss: 4.9301228523254395, Average MAE: 5.412613868713379, Learning Rate: 0.2621440000000001, Current Patience: 3\n",
      "Epoch 69, Average Loss: 4.041981220245361, Average MAE: 4.515292644500732, Learning Rate: 0.2621440000000001, Current Patience: 0\n",
      "Epoch 70, Average Loss: 3.6550893783569336, Average MAE: 4.119396686553955, Learning Rate: 0.2621440000000001, Current Patience: 0\n",
      "Epoch 71, Average Loss: 3.891159772872925, Average MAE: 4.357442378997803, Learning Rate: 0.2621440000000001, Current Patience: 1\n",
      "Epoch 72, Average Loss: 3.722200870513916, Average MAE: 4.189392566680908, Learning Rate: 0.2621440000000001, Current Patience: 2\n",
      "Epoch 73, Average Loss: 4.225563049316406, Average MAE: 4.7001261711120605, Learning Rate: 0.2621440000000001, Current Patience: 3\n",
      "Epoch 74, Average Loss: 4.068749904632568, Average MAE: 4.536802768707275, Learning Rate: 0.2621440000000001, Current Patience: 4\n",
      "Epoch 75, Average Loss: 3.2881195545196533, Average MAE: 3.748692274093628, Learning Rate: 0.2621440000000001, Current Patience: 0\n",
      "Epoch 76, Average Loss: 3.2336602210998535, Average MAE: 3.697410821914673, Learning Rate: 0.2621440000000001, Current Patience: 0\n",
      "Epoch 77, Average Loss: 2.992417573928833, Average MAE: 3.4499223232269287, Learning Rate: 0.2621440000000001, Current Patience: 0\n",
      "Epoch 78, Average Loss: 3.685023784637451, Average MAE: 4.1443190574646, Learning Rate: 0.2621440000000001, Current Patience: 1\n",
      "Epoch 79, Average Loss: 3.9797472953796387, Average MAE: 4.445655822753906, Learning Rate: 0.2621440000000001, Current Patience: 2\n",
      "Epoch 80, Average Loss: 3.204845666885376, Average MAE: 3.669079065322876, Learning Rate: 0.2621440000000001, Current Patience: 3\n",
      "Epoch 81, Average Loss: 3.0538642406463623, Average MAE: 3.5124518871307373, Learning Rate: 0.2621440000000001, Current Patience: 4\n",
      "Epoch 82, Average Loss: 3.0158348083496094, Average MAE: 3.470503330230713, Learning Rate: 0.2621440000000001, Current Patience: 5\n",
      "Epoch 83, Average Loss: 3.234557867050171, Average MAE: 3.700922727584839, Learning Rate: 0.2621440000000001, Current Patience: 6\n",
      "Epoch 84, Average Loss: 3.9292256832122803, Average MAE: 4.388111114501953, Learning Rate: 0.2621440000000001, Current Patience: 7\n",
      "Epoch 85, Average Loss: 2.703472375869751, Average MAE: 3.154134511947632, Learning Rate: 0.2621440000000001, Current Patience: 0\n",
      "Epoch 86, Average Loss: 2.4170451164245605, Average MAE: 2.8603250980377197, Learning Rate: 0.2621440000000001, Current Patience: 0\n",
      "Epoch 87, Average Loss: 3.4332330226898193, Average MAE: 3.8900301456451416, Learning Rate: 0.2621440000000001, Current Patience: 1\n",
      "Epoch 88, Average Loss: 3.4538650512695312, Average MAE: 3.9147109985351562, Learning Rate: 0.2621440000000001, Current Patience: 2\n",
      "Epoch 89, Average Loss: 2.627716541290283, Average MAE: 3.077033281326294, Learning Rate: 0.2621440000000001, Current Patience: 3\n",
      "Epoch 90, Average Loss: 2.5178110599517822, Average MAE: 2.9627163410186768, Learning Rate: 0.2621440000000001, Current Patience: 4\n",
      "Epoch 91, Average Loss: 2.1298916339874268, Average MAE: 2.5622739791870117, Learning Rate: 0.2621440000000001, Current Patience: 0\n",
      "Epoch 92, Average Loss: 2.340435028076172, Average MAE: 2.7762339115142822, Learning Rate: 0.2621440000000001, Current Patience: 1\n",
      "Epoch 93, Average Loss: 3.8241426944732666, Average MAE: 4.285165786743164, Learning Rate: 0.2621440000000001, Current Patience: 2\n",
      "Epoch 94, Average Loss: 2.8643014430999756, Average MAE: 3.3211958408355713, Learning Rate: 0.2621440000000001, Current Patience: 3\n",
      "Epoch 95, Average Loss: 2.3196916580200195, Average MAE: 2.753934860229492, Learning Rate: 0.2621440000000001, Current Patience: 4\n",
      "Epoch 96, Average Loss: 3.3839869499206543, Average MAE: 3.8478751182556152, Learning Rate: 0.2621440000000001, Current Patience: 5\n",
      "Epoch 97, Average Loss: 2.2956440448760986, Average MAE: 2.733614921569824, Learning Rate: 0.2621440000000001, Current Patience: 6\n",
      "Epoch 98, Average Loss: 2.092796802520752, Average MAE: 2.5202159881591797, Learning Rate: 0.2621440000000001, Current Patience: 0\n",
      "Epoch 99, Average Loss: 2.9479894638061523, Average MAE: 3.397181510925293, Learning Rate: 0.2621440000000001, Current Patience: 1\n",
      "Epoch 100, Average Loss: 2.2468693256378174, Average MAE: 2.6784911155700684, Learning Rate: 0.2621440000000001, Current Patience: 2\n",
      "Epoch 101, Average Loss: 3.0360395908355713, Average MAE: 3.491070032119751, Learning Rate: 0.2621440000000001, Current Patience: 3\n",
      "Epoch 102, Average Loss: 1.8528064489364624, Average MAE: 2.2695369720458984, Learning Rate: 0.2621440000000001, Current Patience: 0\n",
      "Epoch 103, Average Loss: 3.1075687408447266, Average MAE: 3.5631377696990967, Learning Rate: 0.2621440000000001, Current Patience: 1\n",
      "Epoch 104, Average Loss: 2.5557072162628174, Average MAE: 2.9975366592407227, Learning Rate: 0.2621440000000001, Current Patience: 2\n",
      "Epoch 105, Average Loss: 3.4528112411499023, Average MAE: 3.9074630737304688, Learning Rate: 0.2621440000000001, Current Patience: 3\n",
      "Epoch 106, Average Loss: 3.3682973384857178, Average MAE: 3.836395502090454, Learning Rate: 0.2621440000000001, Current Patience: 4\n",
      "Epoch 107, Average Loss: 1.965861439704895, Average MAE: 2.399489641189575, Learning Rate: 0.2621440000000001, Current Patience: 5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(model_output_batch, desired_output_batch)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Update the optimizer\u001b[39;00m\n\u001b[1;32m     32\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/mnt/HDD/RyanFolder/projects/Weather-Predition-Project/.venv/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/HDD/RyanFolder/projects/Weather-Predition-Project/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/HDD/RyanFolder/projects/Weather-Predition-Project/.venv/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epoch_losses = []\n",
    "epoch_maes = []\n",
    "\n",
    "# Set the model to training mode\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    total_mae = 0\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for b in range(num_batches_train):\n",
    "        # Get the batched node features and desired output\n",
    "        node_features_batch = batched_input_train[b].to(device)\n",
    "        desired_output_batch = batched_output_train[b].to(device)\n",
    "        \n",
    "        # Reshape node_features_batch to match the expected input shape for the model\n",
    "        node_features_batch = node_features_batch.view(-1, node_features_batch.size(2)).to(device)\n",
    "        \n",
    "        model_output_batch = model(node_features_batch, edge_index, edge_attr)\n",
    "        \n",
    "        # Reshape model_output_batch back to the original shape\n",
    "        model_output_batch = model_output_batch.view(batched_input_train.size(1), -1, model_output_batch.size(1))\n",
    "        loss = loss_fn(model_output_batch, desired_output_batch)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the optimizer\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute MAE for debugging\n",
    "        mae = torch.mean(torch.abs(model_output_batch - desired_output_batch))\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_mae += mae.item()\n",
    "\n",
    "    average_loss = total_loss / num_batches_train\n",
    "    average_mae = total_mae / num_batches_train\n",
    "    epoch_losses.append(average_loss)\n",
    "    epoch_maes.append(average_mae)\n",
    "\n",
    "    scheduler.step(average_loss)\n",
    "\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    current_patience = scheduler.num_bad_epochs\n",
    "    print(f\"Epoch {epoch+1}, Average Loss: {average_loss}, Average MAE: {average_mae}, Learning Rate: {current_lr}, Current Patience: {current_patience}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">**ZE EPIC GRAPHS**</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the epoch losses and MAEs\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(epoch_losses, label='Loss')\n",
    "plt.plot(epoch_maes, label='MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Training Loss and MAE over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
