{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/HDD/RyanFolder/projects/Weather-Predition-Project/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv, GraphConv\n",
    "from torch_geometric.nn.norm import BatchNorm\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "import numpy as np\n",
    "from torch.optim.swa_utils import AveragedModel, SWALR\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">**ZE EPIC DATA**</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([72790024141, 72785524114, 72789094197, 72793024233, 72785794129, 72788594266, 72797624217, 72785024157, 72797094240, 72798594276, 72792424223, 72792894263, 72781024243, 72781524237, 72788324220, 72698824219, 72793894274, 74206024207, 72782724110, 72793724222, 72792594227, 72782594239, 72794504205, 72792394225, 72784524163, 72792024227, 72785694176])\n"
     ]
    }
   ],
   "source": [
    "# Directory containing the CSV files\n",
    "directory = '../processed-final-data-2'\n",
    "\n",
    "# Dictionary to store the dataframes\n",
    "dataframes = {}\n",
    "\n",
    "# Iterate over all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        # Extract the file name without extension and convert it to int\n",
    "        key = int(os.path.splitext(filename)[0])\n",
    "        \n",
    "        # Read the CSV file into a dataframe\n",
    "        df = pd.read_csv(os.path.join(directory, filename))\n",
    "        dataframes[key] = df\n",
    "\n",
    "\n",
    "# Print the dictionary keys to verify\n",
    "print(dataframes.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72790024141: Sequential split verified.\n",
      "72785524114: Sequential split verified.\n",
      "72789094197: Sequential split verified.\n",
      "72793024233: Sequential split verified.\n",
      "72785794129: Sequential split verified.\n",
      "72788594266: Sequential split verified.\n",
      "72797624217: Sequential split verified.\n",
      "72785024157: Sequential split verified.\n",
      "72797094240: Sequential split verified.\n",
      "72798594276: Sequential split verified.\n",
      "72792424223: Sequential split verified.\n",
      "72792894263: Sequential split verified.\n",
      "72781024243: Sequential split verified.\n",
      "72781524237: Sequential split verified.\n",
      "72788324220: Sequential split verified.\n",
      "72698824219: Sequential split verified.\n",
      "72793894274: Sequential split verified.\n",
      "74206024207: Sequential split verified.\n",
      "72782724110: Sequential split verified.\n",
      "72793724222: Sequential split verified.\n",
      "72792594227: Sequential split verified.\n",
      "72782594239: Sequential split verified.\n",
      "72794504205: Sequential split verified.\n",
      "72792394225: Sequential split verified.\n",
      "72784524163: Sequential split verified.\n",
      "72792024227: Sequential split verified.\n",
      "72785694176: Sequential split verified.\n",
      "dict_keys([72790024141, 72785524114, 72789094197, 72793024233, 72785794129, 72788594266, 72797624217, 72785024157, 72797094240, 72798594276, 72792424223, 72792894263, 72781024243, 72781524237, 72788324220, 72698824219, 72793894274, 74206024207, 72782724110, 72793724222, 72792594227, 72782594239, 72794504205, 72792394225, 72784524163, 72792024227, 72785694176])\n",
      "dict_keys([72790024141, 72785524114, 72789094197, 72793024233, 72785794129, 72788594266, 72797624217, 72785024157, 72797094240, 72798594276, 72792424223, 72792894263, 72781024243, 72781524237, 72788324220, 72698824219, 72793894274, 74206024207, 72782724110, 72793724222, 72792594227, 72782594239, 72794504205, 72792394225, 72784524163, 72792024227, 72785694176])\n"
     ]
    }
   ],
   "source": [
    "# Dictionaries to store the training and testing dataframes\n",
    "train_dataframes = {}\n",
    "test_dataframes = {}\n",
    "\n",
    "# Split each dataframe into training and testing sets\n",
    "for key, df in dataframes.items():\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, shuffle=False)\n",
    "    train_dataframes[key] = train_df\n",
    "    test_dataframes[key] = test_df\n",
    "    # Check if the maximum index of the training set is less than the minimum index of the testing set\n",
    "    if train_df.index.max() < test_df.index.min():\n",
    "        print(f\"{key}: Sequential split verified.\")\n",
    "    else:\n",
    "        print(f\"{key}: Sequential split NOT verified.\")\n",
    "\n",
    "# Print the keys of the training and testing dictionaries to verify\n",
    "print(train_dataframes.keys())\n",
    "print(test_dataframes.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([823, 27, 67])\n",
      "torch.Size([206, 27, 67])\n"
     ]
    }
   ],
   "source": [
    "def create_node_features_sequences(dataframes):\n",
    "    # Create a list to store the node features for each time step for input and desired output\n",
    "    node_features_sequence_input = []\n",
    "    node_features_sequence_output = []\n",
    "\n",
    "    # Iterate over the rows of the dataframes (assuming all dataframes have the same number of rows)\n",
    "    for i in range(len(next(iter(dataframes.values())))):\n",
    "        if i == len(next(iter(dataframes.values()))) - 1:\n",
    "            break\n",
    "        # Create a list to store the features of all nodes at the current time step for input\n",
    "        node_features_input = []\n",
    "        # Create a list to store the features of all nodes at the next time step for output\n",
    "        node_features_output = []\n",
    "\n",
    "        # Iterate over each dataframe and extract the features at the current row for input\n",
    "        # and the next row for output\n",
    "        for key, df in dataframes.items():\n",
    "            node_features_input.append((df.iloc[i].values - df.iloc[i].mean()) / df.iloc[i].std())\n",
    "            node_features_output.append(df.iloc[i + 1].values)\n",
    "\n",
    "        # Stack the features of all nodes to create a 2D array (num_nodes, num_features)\n",
    "        node_features_sequence_input.append(np.stack(node_features_input))\n",
    "        node_features_sequence_output.append(np.stack(node_features_output))\n",
    "\n",
    "    # Convert the lists to numpy arrays (time_steps, num_nodes, num_features)\n",
    "    node_features_sequence_input = np.array(node_features_sequence_input)\n",
    "    node_features_sequence_output = np.array(node_features_sequence_output)\n",
    "\n",
    "    # Convert the numpy arrays to torch tensors\n",
    "    node_features_sequence_input = torch.tensor(node_features_sequence_input, dtype=torch.float)\n",
    "    node_features_sequence_output = torch.tensor(node_features_sequence_output, dtype=torch.float)\n",
    "\n",
    "    return node_features_sequence_input, node_features_sequence_output\n",
    "\n",
    "# Call the function and print the shapes of the resulting tensors\n",
    "node_features_sequence_input_train, node_features_sequence_output_train = create_node_features_sequences(train_dataframes)\n",
    "node_features_sequence_input_test, node_features_sequence_output_test = create_node_features_sequences(test_dataframes)\n",
    "print(node_features_sequence_input_train.shape)\n",
    "print(node_features_sequence_output_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: tensor([[[-0.3252,  4.8230,  1.8191,  1.7459,  0.1171,  1.8200,  2.3915,\n",
      "          -0.3256,  3.1427, -0.0194, -0.2180,  2.7711, -0.3193, -0.3254,\n",
      "          -0.3255, -0.3194, -0.3254, -0.3230, -0.3250, -0.3254, -0.3250,\n",
      "          -0.3255, -0.3256, -0.3255, -0.3254, -0.3255, -0.2910, -0.3076,\n",
      "          -0.3255, -0.3255, -0.3240, -0.3256, -0.3256, -0.3254, -0.3254,\n",
      "          -0.3254, -0.3256, -0.3255, -0.3255, -0.3256, -0.2925, -0.3256,\n",
      "          -0.3255, -0.3248, -0.3249, -0.3194, -0.3256, -0.3255, -0.3250,\n",
      "          -0.3254, -0.3255, -0.3256, -0.3255, -0.3244, -0.3192, -0.3256,\n",
      "          -0.3194, -0.3251, -0.3255, -0.3256, -0.3255, -0.3255, -0.3256,\n",
      "          -0.3252, -0.3248, -0.2906, -0.3253]]])\n",
      "Standard Deviation: tensor([[[0.0155, 1.3561, 0.2738, 0.2514, 0.2861, 0.2749, 0.5699, 0.0153,\n",
      "          0.9273, 0.1064, 0.1481, 0.6489, 0.0241, 0.0161, 0.0154, 0.0290,\n",
      "          0.0156, 0.0218, 0.0166, 0.0158, 0.0175, 0.0155, 0.0153, 0.0155,\n",
      "          0.0156, 0.0155, 0.0324, 0.0387, 0.0155, 0.0156, 0.0192, 0.0154,\n",
      "          0.0153, 0.0158, 0.0159, 0.0157, 0.0153, 0.0155, 0.0155, 0.0153,\n",
      "          0.0326, 0.0153, 0.0155, 0.0167, 0.0165, 0.0291, 0.0153, 0.0156,\n",
      "          0.0164, 0.0156, 0.0154, 0.0154, 0.0156, 0.0173, 0.0242, 0.0153,\n",
      "          0.0240, 0.0162, 0.0156, 0.0153, 0.0155, 0.0156, 0.0153, 0.0165,\n",
      "          0.0166, 0.0324, 0.0158]]])\n"
     ]
    }
   ],
   "source": [
    "# Calculate mean and standard deviation of the training data\n",
    "mean = node_features_sequence_input_train.mean(dim=(0, 1), keepdim=True)\n",
    "std = node_features_sequence_input_train.std(dim=(0, 1), keepdim=True)\n",
    "\n",
    "# Normalize the training and testing data\n",
    "# node_features_sequence_input_train = (node_features_sequence_input_train - mean) / std\n",
    "# node_features_sequence_input_test = (node_features_sequence_input_test - mean) / std\n",
    "\n",
    "# Print the mean and standard deviation to verify\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Standard Deviation:\", std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">**ZE EPIC EDGE DATA**</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        STATION  LONGITUDE  LATITUDE  ELEVATION\n",
      "0  7.279002e+10 -119.51551  47.30777      382.1\n",
      "1  7.278552e+10 -117.65000  47.63333      750.1\n",
      "2  7.278909e+10 -119.52091  48.46113      397.4\n",
      "3  7.279302e+10 -122.31442  47.44467      112.5\n",
      "4  7.278579e+10 -117.11581  46.74376      775.7\n"
     ]
    }
   ],
   "source": [
    "# Import the location-datamap.csv file as a dataframe\n",
    "location_datamap_df = pd.read_csv('../location-datamap.csv')\n",
    "\n",
    "# Print the first few rows of the dataframe to verify\n",
    "print(location_datamap_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_distance(lat1, lon1, lat2, lon2, el1=0, el2=0):\n",
    "    # Radius of the Earth in kilometers\n",
    "    R = 6371.0\n",
    "\n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "\n",
    "    # Difference in coordinates\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "\n",
    "    # Haversine formula\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "\n",
    "    # Distance in kilometers\n",
    "    distance = R * c\n",
    "\n",
    "    # Elevation difference\n",
    "    height = el2 - el1\n",
    "\n",
    "    # Calculate the total distance considering elevation\n",
    "    total_distance = sqrt(distance**2 + height**2)\n",
    "\n",
    "    return total_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 1): 395.4679457314314, (0, 2): 129.15783117310457, (0, 3): 342.5330553210191, (0, 4): 438.10430559873856, (0, 5): 432.0504921333856, (0, 6): 437.11321302131137, (0, 7): 369.0087247919059, (0, 8): 503.7031148557305, (0, 9): 455.6811328123654, (0, 10): 474.59654599988056, (0, 11): 348.0950737502581, (0, 12): 128.6980383337319, (0, 13): 835.9738346074198, (0, 14): 171.59537540199554, (0, 15): 384.56540064951065, (0, 16): 373.58764051749137, (0, 17): 362.1843030623904, (0, 18): 32.53657446575969, (0, 19): 306.4908073667999, (0, 20): 404.50991220772727, (0, 21): 52.81240627430448, (0, 22): 404.8085057270694, (0, 23): 505.4288197147213, (0, 24): 286.0836462058196, (0, 25): 412.6227567328569, (0, 26): 272.19719043646785, (1, 2): 390.14029117618804, (1, 3): 727.6917541369755, (1, 4): 109.85904921372176, (1, 5): 798.995937782049, (1, 6): 802.4581375032445, (1, 7): 33.690956835057186, (1, 8): 865.1737430163511, (1, 9): 825.6499844120128, (1, 10): 860.6728795779842, (1, 11): 724.9963923066945, (1, 12): 495.92270395996894, (1, 13): 535.6266022869553, (1, 14): 314.54161595295784, (1, 15): 763.6141112163369, (1, 16): 759.3841400612968, (1, 17): 748.5486591666476, (1, 18): 416.12981588895354, (1, 19): 678.7815012654146, (1, 20): 784.7770299622911, (1, 21): 419.79175595055284, (1, 22): 786.18443118649, (1, 23): 886.2658783050533, (1, 24): 655.5895444605051, (1, 25): 798.318691116012, (1, 26): 157.5431429141368, (2, 3): 370.42190175405364, (2, 4): 460.5219705003162, (2, 5): 432.51979230044606, (2, 6): 417.3828379309705, (2, 7): 365.04041085309774, (2, 8): 508.89691489063597, (2, 9): 445.736559884783, (2, 10): 534.5520542530354, (2, 11): 372.6745336577991, (2, 12): 236.9285970548754, (2, 13): 831.0617187024466, (2, 14): 221.69557651346355, (2, 15): 470.9577296531585, (2, 16): 406.24280451088293, (2, 17): 399.71114545506987, (2, 18): 147.68778122386612, (2, 19): 313.8982068438734, (2, 20): 435.3656912848407, (2, 21): 130.20708355028395, (2, 22): 406.9699486305622, (2, 23): 539.0132078425393, (2, 24): 368.91523621650714, (2, 25): 452.3832910640423, (2, 26): 270.2281521689557, (3, 4): 775.0588941479449, (3, 5): 120.26611512741678, (3, 6): 165.52848353942377, (3, 7): 704.0879836514051, (3, 8): 185.28412975619443, (3, 9): 152.43166309928537, (3, 10): 187.27787071696568, (3, 11): 41.159846544112625, (3, 12): 266.9618608270944, (3, 13): 1096.8318771247875, (3, 14): 443.4038293955239, (3, 15): 225.02216925780087, (3, 16): 36.85266364881154, (3, 17): 37.9895765777111, (3, 18): 333.6917238582668, (3, 19): 76.27723241526142, (3, 20): 72.18185980856553, (3, 21): 309.0749255571238, (3, 22): 107.09860018045799, (3, 23): 171.23674671831446, (3, 24): 276.0731537992283, (3, 25): 85.99186556469607, (3, 26): 611.0889227645558, (4, 5): 856.701667766572, (4, 6): 865.4792367557994, (4, 7): 117.74480456011335, (4, 8): 921.4842980890041, (4, 9): 885.9031028312606, (4, 10): 890.5893542480043, (4, 11): 774.366157782777, (4, 12): 524.6337306289196, (4, 13): 540.0809528724242, (4, 14): 357.13279391570444, (4, 15): 780.4774458332262, (4, 16): 804.444912957628, (4, 17): 791.9148221844617, (4, 18): 454.1201925958629, (4, 19): 734.2694292819892, (4, 20): 830.6418814848106, (4, 21): 467.37603414879794, (4, 22): 840.9110561664348, (4, 23): 929.3740741921883, (4, 24): 673.3913688173589, (4, 25): 839.7968409183139, (4, 26): 209.78499150005845, (5, 6): 110.15242913456197, (5, 7): 777.4585937545535, (5, 8): 85.13925057078639, (5, 9): 74.97130998219406, (5, 10): 240.43942234567194, (5, 11): 103.9757703201679, (5, 12): 369.69695959787833, (5, 13): 1139.0521959187856, (5, 14): 516.0490346912795, (5, 15): 330.5216959336131, (5, 16): 117.91278236100351, (5, 17): 133.34207595548997, (5, 18): 428.7153028579556, (5, 19): 125.7282441840991, (5, 20): 101.97987733596389, (5, 21): 392.60154297249403, (5, 22): 108.31645122446824, (5, 23): 153.40108385187867, (5, 24): 392.1898886399527, (5, 25): 137.25092367847324, (5, 26): 690.174750212649, (6, 7): 778.6874797868297, (6, 8): 177.46961023734613, (6, 9): 48.71149879169117, (6, 10): 302.0846527899822, (6, 11): 172.27646695285324, (6, 12): 399.97805818217563, (6, 13): 1176.4852153080844, (6, 14): 545.6427005439592, (6, 15): 369.3977887607593, (6, 16): 175.55537172802033, (6, 17): 190.70446482220075, (6, 18): 431.096161984872, (6, 19): 156.64330089805705, (6, 20): 183.24573042058552, (6, 21): 405.4909815288361, (6, 22): 76.4859126118275, (6, 23): 231.7098185991757, (6, 24): 388.4359336032824, (6, 25): 205.33116784821942, (6, 26): 682.4260680955291, (7, 8): 845.1366146343045, (7, 9): 802.4776134962176, (7, 10): 837.0612376685274, (7, 11): 702.8044715545802, (7, 12): 472.23598917705993, (7, 13): 567.9966156275789, (7, 14): 299.672336109386, (7, 15): 738.0303386253106, (7, 16): 735.9632694864449, (7, 17): 725.1092500944276, (7, 18): 388.53428759844127, (7, 19): 656.1420174916371, (7, 20): 762.4710980022056, (7, 21): 395.81370216605853, (7, 22): 761.3237576215986, (7, 23): 864.2082908552868, (7, 24): 626.0309963149288, (7, 25): 775.1913077729471, (7, 26): 124.27275180830249, (8, 9): 132.5097212349861, (8, 10): 243.7073144688935, (8, 11): 163.47287592761103, (8, 12): 430.6615227573649, (8, 13): 1177.64872286148, (8, 14): 572.9402144031785, (8, 15): 365.1202761234219, (8, 16): 169.20923499165121, (8, 17): 183.39281681071296, (8, 18): 501.4584011725272, (8, 19): 202.41419212127153, (8, 20): 134.47678138943274, (8, 21): 461.61393826175714, (8, 22): 180.6112148305096, (8, 23): 128.08708542091654, (8, 24): 456.1447880245876, (8, 25): 164.101942102309, (8, 26): 763.0321197403028, (9, 10): 268.89465547374556, (9, 11): 154.53742133091865, (9, 12): 405.60664848136616, (9, 13): 1187.3176896736784, (9, 14): 557.1020923327553, (9, 15): 353.92282978101207, (9, 16): 153.1723115228679, (9, 17): 170.09372061722416, (9, 18): 449.65266636740307, (9, 19): 158.4233084920311, (9, 20): 151.2637681156068, (9, 21): 421.5198568724629, (9, 22): 75.9354339763056, (9, 23): 187.20830867381414, (9, 24): 396.00851322484175, (9, 25): 174.23493854899442, (9, 26): 708.5304039928845, (10, 11): 199.69966656538062, (10, 12): 366.6429361022303, (10, 13): 1213.3303112543872, (10, 14): 565.7742092205691, (10, 15): 158.75602862100996, (10, 16): 154.05814985745886, (10, 17): 150.49523188191975, (10, 18): 459.96707604415764, (10, 19): 261.56576121699044, (10, 20): 148.37325199067052, (10, 21): 447.531376407682, (10, 22): 236.6757463314917, (10, 23): 123.91187323342717, (10, 24): 313.3574949442864, (10, 25): 109.93432994721853, (10, 26): 745.579780595948, (11, 12): 271.20003239227077, (11, 13): 1077.0374760399814, (11, 14): 434.420916611982, (11, 15): 249.12813853110475, (11, 16): 54.60340565924095, (11, 17): 56.69074125354086, (11, 18): 342.83747887822733, (11, 19): 68.6532776654429, (11, 20): 64.41876337172579, (11, 21): 309.79708423419913, (11, 22): 128.34321457181807, (11, 23): 167.4998627149666, (11, 24): 308.66359444651886, (11, 25): 94.10375597750865, (11, 26): 614.9675457923169, (12, 13): 891.3237029023409, (12, 14): 217.85555378560997, (12, 15): 275.33106494313824, (12, 16): 290.27386357894756, (12, 17): 275.46064771106325, (12, 18): 121.40670159689395, (12, 19): 252.86361822595595, (12, 20): 317.66456605180196, (12, 21): 111.45482222908821, (12, 22): 352.74223587837065, (12, 23): 411.2440393694844, (12, 24): 228.97116863460516, (12, 25): 319.85603194172165, (12, 26): 386.6621826263452, (13, 14): 677.4867035602812, (13, 15): 1150.4386824494006, (13, 16): 1122.20080799066, (13, 17): 1112.0640517140919, (13, 18): 864.0517998527835, (13, 19): 1044.4930844703567, (13, 20): 1130.7362905229766, (13, 21): 833.7147080759753, (13, 22): 1170.6533446424978, (13, 23): 1218.574885845403, (13, 24): 1103.3829323700534, (13, 25): 1152.414996794818, (13, 26): 683.9489018806388, (14, 15): 489.8134161401565, (14, 16): 470.9676802550459, (14, 17): 459.0274176052015, (14, 18): 199.59384465449898, (14, 19): 400.7840441174383, (14, 20): 490.3741382897129, (14, 21): 162.11769623669784, (14, 22): 520.9373250801362, (14, 23): 587.5949150283049, (14, 24): 432.7632336033511, (14, 25): 504.8208347435879, (14, 26): 259.92185050812566, (15, 16): 213.47199631168857, (15, 17): 199.7906856396986, (15, 18): 363.3873739871944, (15, 19): 286.49766075950384, (15, 20): 235.6189290359573, (15, 21): 371.98475050597, (15, 22): 293.97835633481196, (15, 23): 268.7494270590419, (15, 24): 181.34059793217904, (15, 25): 201.61183866011444, (15, 26): 642.4950004501114, (16, 17): 17.716247962574563, (16, 18): 363.9821600522806, (16, 19): 109.51765254130393, (16, 20): 42.988933127483634, (16, 21): 340.3237239231559, (16, 22): 114.05670021657697, (16, 23): 136.43815241862657, (16, 24): 287.7450730575042, (16, 25): 49.41202473657626, (16, 26): 643.6682517559431, (17, 18): 352.2158347034709, (17, 19): 111.26188855442645, (17, 20): 52.561875466910436, (17, 21): 329.3368670529716, (17, 22): 128.01154758942099, (17, 23): 145.49531716211806, (17, 24): 275.6435250697721, (17, 25): 52.80087090274467, (17, 26): 633.1197418989807, (18, 19): 303.31516235822005, (18, 20): 396.9755284116139, (18, 21): 73.74026824660453, (18, 22): 394.74559894207687, (18, 23): 496.478460291939, (18, 24): 256.1592053257525, (18, 25): 402.1731282608459, (18, 26): 286.80868520838226, (19, 20): 129.67576043022495, (19, 21): 268.31498195846285, (19, 22): 128.38153131965652, (19, 23): 230.11767302669344, (19, 24): 305.30416194483115, (19, 25): 156.88223261226412, (19, 26): 566.3246868369048, (20, 21): 368.04423285388, (20, 22): 133.10933796080906, (20, 23): 103.88553424589651, (20, 24): 327.04970158433304, (20, 25): 41.59043713208587, (20, 26): 674.0342619771884, (21, 22): 375.88513957717953, (21, 23): 469.86938496204897, (21, 24): 296.22180486691445, (21, 25): 379.7335338263519, (21, 26): 308.0303254221726, (22, 23): 191.05105434406818, (22, 24): 321.52952520777865, (22, 25): 144.61575947894278, (22, 26): 662.014427725483, (23, 24): 393.67944122771735, (23, 25): 96.11729883125135, (23, 26): 776.1835969743106, (24, 25): 305.7627599001508, (24, 26): 516.3645983655035, (25, 26): 684.1746652775099}\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary to store the distances between nodes\n",
    "distances = {}\n",
    "\n",
    "# Define the number of nodes\n",
    "num_nodes = len(dataframes)\n",
    "\n",
    "# Iterate over each pair of nodes\n",
    "for i, j in itertools.combinations(range(num_nodes), 2):\n",
    "    # Get the station IDs for the nodes\n",
    "    station_i = list(dataframes.keys())[i]\n",
    "    station_j = list(dataframes.keys())[j]\n",
    "    \n",
    "    # Get the location data for the stations\n",
    "    location_i = location_datamap_df[location_datamap_df['STATION'] == station_i].iloc[0]\n",
    "    location_j = location_datamap_df[location_datamap_df['STATION'] == station_j].iloc[0]\n",
    "    \n",
    "    # Calculate the distance between the stations\n",
    "    distance = haversine_distance(location_i['LATITUDE'], location_i['LONGITUDE'], location_j['LATITUDE'], location_j['LONGITUDE'], location_i['ELEVATION'], location_j['ELEVATION'])\n",
    "    \n",
    "    # Store the distance in the dictionary\n",
    "    distances[(i, j)] = distance\n",
    "\n",
    "# Print the distances to verify\n",
    "print(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 1): 0.314567998140773, (0, 2): 0.09280158354609237, (0, 3): 0.2704871307176606, (0, 4): 0.35007289315695667, (0, 5): 0.34503165576699535, (0, 6): 0.349247573218238, (0, 7): 0.2925344130835223, (0, 8): 0.40469948049003845, (0, 9): 0.3647097760165542, (0, 10): 0.380461349591329, (0, 11): 0.27511883194691195, (0, 12): 0.0924186968141592, (0, 13): 0.6813937634553494, (0, 14): 0.1281409173278857, (0, 15): 0.3054890401868689, (0, 16): 0.2963474478414338, (0, 17): 0.28685146130699407, (0, 18): 0.012341441395062182, (0, 19): 0.2404733998610767, (0, 20): 0.32209758254900717, (0, 21): 0.029225886548649996, (0, 22): 0.3223462325648563, (0, 23): 0.4061365396113629, (0, 24): 0.2234795918330484, (0, 25): 0.32885345228187846, (0, 26): 0.21191581960267647, (1, 2): 0.31013146049414686, (1, 3): 0.5912232162697533, (1, 4): 0.0767307644250279, (1, 5): 0.6506008827124798, (1, 6): 0.6534839861952507, (1, 7): 0.013302738864123758, (1, 8): 0.7057096216985912, (1, 9): 0.672796706424883, (1, 10): 0.7019615840059098, (1, 11): 0.5889786874424193, (1, 12): 0.39822044070107643, (1, 13): 0.4312833650740786, (1, 14): 0.24717760994224985, (1, 15): 0.6211371095009285, (1, 16): 0.6176146539665304, (1, 17): 0.6085915428751594, (1, 18): 0.33177391189757455, (1, 19): 0.5504938153822416, (1, 20): 0.6387602651983096, (1, 21): 0.3348233466487418, (1, 22): 0.6399322609518483, (1, 23): 0.7232738333579158, (1, 24): 0.5311810036379734, (1, 25): 0.6500369140281799, (1, 26): 0.11643909661014204, (2, 3): 0.2937112185105451, (2, 4): 0.3687409230102465, (2, 5): 0.34542245960706086, (2, 6): 0.3328173503194575, (2, 7): 0.2892298493208762, (2, 8): 0.40902455246026, (2, 9): 0.3564285573835974, (2, 10): 0.4303885486485463, (2, 11): 0.295587069533041, (2, 12): 0.18254633990790353, (2, 13): 0.6773032604185943, (2, 14): 0.16986123271804443, (2, 15): 0.37743117082429484, (2, 16): 0.32354062692449737, (2, 17): 0.31810146960009433, (2, 18): 0.108232167518433, (2, 19): 0.24664181905997015, (2, 20): 0.34779234636526585, (2, 21): 0.09367533533009026, (2, 22): 0.3241461470887702, (2, 23): 0.43410351846162026, (2, 24): 0.29245656164252043, (2, 25): 0.36196353957848576, (2, 26): 0.21027612763111883, (3, 4): 0.6306676092371719, (3, 5): 0.08539711830331884, (3, 6): 0.1230887890663378, (3, 7): 0.571567471837432, (3, 8): 0.13954005617934356, (3, 9): 0.11218257577279916, (3, 10): 0.1412003190095184, (3, 11): 0.019522363284050027, (3, 12): 0.20755616439912683, (3, 13): 0.8986200332994612, (3, 14): 0.3544860052665791, (3, 15): 0.17263141118817832, (3, 16): 0.01593561063938, (3, 17): 0.01688236065060862, (3, 18): 0.2631246226056817, (3, 19): 0.0487659268171087, (3, 20): 0.04535555653912475, (3, 21): 0.24262529193963123, (3, 22): 0.07443203504408213, (3, 23): 0.12784227378037094, (3, 24): 0.21514347957903632, (3, 25): 0.05685566597788288, (3, 26): 0.4941236679182537, (4, 5): 0.6986546070761243, (4, 6): 0.7059640177863663, (4, 7): 0.08329752848669511, (4, 8): 0.7526015316172568, (4, 9): 0.7229717366228394, (4, 10): 0.7268741538340823, (4, 11): 0.6300907416997995, (4, 12): 0.4221291887944986, (4, 13): 0.4349926697706931, (4, 14): 0.28264488029293566, (4, 15): 0.6351798403311119, (4, 16): 0.6551384485871659, (4, 17): 0.6447041723302557, (4, 18): 0.36340992258896465, (4, 19): 0.5967006929164721, (4, 20): 0.676953646230553, (4, 21): 0.37444855872377714, (4, 22): 0.6855051729112698, (4, 23): 0.7591716439138168, (4, 24): 0.5460052500523883, (4, 25): 0.6845773241096111, (4, 26): 0.15994284212845428, (5, 6): 0.07697507288198122, (5, 7): 0.6326659290484358, (5, 8): 0.0561456615135665, (5, 9): 0.04767843625671288, (5, 10): 0.18546993572513168, (5, 11): 0.07183153756521501, (5, 12): 0.29310753200381906, (5, 13): 0.9337784753192767, (5, 14): 0.41498039070384635, (5, 15): 0.2604848215294762, (5, 16): 0.08343740989787128, (5, 17): 0.0962859610160021, (5, 18): 0.34225431864319356, (5, 19): 0.08994563790785139, (5, 20): 0.07016948266446262, (5, 21): 0.3121810371209556, (5, 22): 0.07544618525759716, (5, 23): 0.11298984877064547, (5, 24): 0.3118382371280546, (5, 25): 0.09954100503173634, (5, 26): 0.5599814008380295, (6, 7): 0.6336892685103085, (6, 8): 0.1330326128614309, (6, 9): 0.025810907172023778, (6, 10): 0.23680422978741508, (6, 11): 0.1287080877918951, (6, 12): 0.3183237378327452, (6, 13): 0.9649503536806591, (6, 14): 0.43962414552985546, (6, 15): 0.2928584012338173, (6, 16): 0.1314385547025949, (6, 17): 0.14405377236126038, (6, 18): 0.344236949280813, (6, 19): 0.11568976443423645, (6, 20): 0.13784260464649475, (6, 21): 0.322914555746484, (6, 22): 0.048939702638827395, (6, 23): 0.17820046747041107, (6, 24): 0.30871217805811385, (6, 25): 0.15623397622922458, (6, 26): 0.5535287827923442, (7, 8): 0.6890239538356585, (7, 9): 0.6535002045846253, (7, 10): 0.6822992847438707, (7, 11): 0.5704986432040403, (7, 12): 0.378495625443329, (7, 13): 0.45823908852017337, (7, 14): 0.2347954032657112, (7, 15): 0.5998325430982321, (7, 16): 0.5981112171455704, (7, 17): 0.5890726683525555, (7, 18): 0.30879408111652235, (7, 19): 0.531641068639551, (7, 20): 0.6201852795535281, (7, 21): 0.3148559224840052, (7, 22): 0.6192298462124066, (7, 23): 0.7049056534956674, (7, 24): 0.5065664926429994, (7, 25): 0.6307778750260209, (7, 26): 0.08873359484975862, (8, 9): 0.09559282804077418, (8, 10): 0.18819123198776505, (8, 11): 0.12137700755686981, (8, 12): 0.34387500890432254, (8, 13): 0.965919250032562, (8, 14): 0.46235580852338326, (8, 15): 0.289296356125095, (8, 16): 0.12615388876759553, (8, 17): 0.13796508899684823, (8, 18): 0.4028302232665897, (8, 19): 0.15380490120329926, (8, 20): 0.09723087276343585, (8, 21): 0.3696502454958358, (8, 22): 0.13564874476410202, (8, 23): 0.09190993342308056, (8, 24): 0.365095879091134, (8, 25): 0.12190085454006432, (8, 26): 0.6206524633838301, (9, 10): 0.20916567494906027, (9, 11): 0.11393611958320622, (9, 12): 0.3230108759534436, (9, 13): 0.973970961122591, (9, 14): 0.44916681060906877, (9, 15): 0.27997182283768707, (9, 16): 0.11279934147711913, (9, 17): 0.12689043310151676, (9, 18): 0.359689646040564, (9, 19): 0.11717204347843128, (9, 20): 0.11121002584324408, (9, 21): 0.3362624010614717, (9, 22): 0.04848129844523104, (9, 23): 0.14114239208876594, (9, 24): 0.31501814895483, (9, 25): 0.13033898050012765, (9, 26): 0.5752668417726906, (10, 11): 0.15154441402332883, (10, 12): 0.29056433216388416, (10, 13): 0.9956326461537036, (10, 14): 0.45638840740176295, (10, 15): 0.11744911200130542, (10, 16): 0.11353701226254376, (10, 17): 0.11057003691412082, (10, 18): 0.36827884159728613, (10, 19): 0.20306262998976654, (10, 20): 0.108802984719709, (10, 21): 0.3579231683779967, (10, 22): 0.18233578163284334, (10, 23): 0.08843307773351292, (10, 24): 0.24619154799347698, (10, 25): 0.07679345351358663, (10, 26): 0.6061192464057483, (11, 12): 0.21108544872242438, (11, 13): 0.882136493555179, (11, 14): 0.3470055970818329, (11, 15): 0.19270535537515096, (11, 16): 0.030717318869188645, (11, 17): 0.03245552145894557, (11, 18): 0.27074063562457035, (11, 19): 0.0424171739253779, (11, 20): 0.03889093514911131, (11, 21): 0.24322666053897657, (11, 22): 0.09212322176762135, (11, 23): 0.12473043039975776, (11, 24): 0.24228276110575214, (11, 25): 0.0636107411856644, (11, 26): 0.49735354269735277, (12, 13): 0.7274856734844146, (12, 14): 0.16666350185554782, (12, 15): 0.21452551437257505, (12, 16): 0.22696894290314232, (12, 17): 0.21463342280062572, (12, 18): 0.08634692740948313, (12, 19): 0.19581602933544076, (12, 20): 0.24977820754826785, (12, 21): 0.0780596244298823, (12, 22): 0.27898869804231324, (12, 23): 0.32770534265441786, (12, 24): 0.17591989099106883, (12, 25): 0.25160312333834234, (12, 26): 0.3072351091334447, (13, 14): 0.5494155887997888, (13, 15): 0.9432604294572675, (13, 16): 0.919745692944629, (13, 17): 0.9113044360332913, (13, 18): 0.7047753375720718, (13, 19): 0.8550355588215105, (13, 20): 0.9268535091879839, (13, 21): 0.6795125041129281, (13, 22): 0.9600939363792285, (13, 23): 1.0, (13, 24): 0.9040753425578563, (13, 25): 0.9449061804916289, (13, 26): 0.5547969035661553, (14, 15): 0.39313300773679066, (14, 16): 0.3774394570634686, (14, 17): 0.3674963527935975, (14, 18): 0.15145629215157527, (14, 19): 0.3189949125321117, (14, 20): 0.39359994208848503, (14, 21): 0.12024849863153748, (14, 22): 0.41905105334027043, (14, 23): 0.47455932704156895, (14, 24): 0.3456251823049916, (14, 25): 0.4056302477365713, (14, 26): 0.20169368392317283, (15, 16): 0.16301314923648366, (15, 17): 0.15162020901821552, (15, 18): 0.2878533035612373, (15, 19): 0.22382435727056416, (15, 20): 0.1814557302577726, (15, 21): 0.2950126612471121, (15, 22): 0.23005381287784274, (15, 23): 0.20904473780448538, (15, 24): 0.13625612941260268, (15, 25): 0.15313675140127786, (15, 26): 0.5202766860128114, (16, 17): 0.0, (16, 18): 0.2883486042122239, (16, 19): 0.07644647061920598, (16, 20): 0.021045512242361872, (16, 21): 0.268647337649462, (16, 22): 0.0802263057572332, (16, 23): 0.09886417993825188, (16, 24): 0.22486312424833235, (16, 25): 0.026394261384406467, (16, 26): 0.5212536963525964, (17, 18): 0.2785503440526814, (17, 19): 0.07789896132718613, (17, 20): 0.029017260154592706, (17, 21): 0.2594981701091805, (17, 22): 0.09184703023938133, (17, 23): 0.10640642051326218, (17, 24): 0.2147857116320833, (17, 25): 0.02921628061236748, (17, 26): 0.5124695568009504, (18, 19): 0.23782892122853866, (18, 20): 0.3158234187478483, (18, 21): 0.04665330165988813, (18, 22): 0.3139664728932817, (18, 23): 0.39868323983033105, (18, 24): 0.19856038824316935, (18, 25): 0.32015165496588943, (18, 26): 0.22408335898739135, (19, 20): 0.09323288265223323, (19, 21): 0.20868295908476445, (19, 22): 0.09215512955978745, (19, 23): 0.17687462817321511, (19, 24): 0.23948523573872763, (19, 25): 0.11588873182862379, (19, 26): 0.45684681074664474, (20, 21): 0.29173124449431487, (20, 22): 0.09609215136402571, (20, 23): 0.0717563946038166, (20, 24): 0.2575935616927636, (20, 25): 0.019880932206643945, (20, 26): 0.5465406112843839, (21, 22): 0.2982606614264556, (21, 23): 0.3765248654051755, (21, 24): 0.2319220165625477, (21, 25): 0.30146536356854725, (21, 26): 0.24175541425211852, (22, 23): 0.1443423904474645, (22, 24): 0.25299670390916407, (22, 25): 0.10567397986169143, (22, 26): 0.5365312447590309, (23, 24): 0.31307864340134484, (23, 25): 0.06528749379435833, (23, 26): 0.6316041914383448, (24, 25): 0.23986712744592156, (24, 26): 0.4152431724037643, (25, 26): 0.5549849052091039}\n"
     ]
    }
   ],
   "source": [
    "# Extract the distance values from the dictionary\n",
    "distance_values = np.array(list(distances.values()))\n",
    "\n",
    "# Perform Min-Max normalization\n",
    "min_distance = distance_values.min()\n",
    "max_distance = distance_values.max()\n",
    "normalized_distances = (distance_values - min_distance) / (max_distance - min_distance)\n",
    "\n",
    "# Update the distances dictionary with normalized values\n",
    "normalized_distances_dict = {key: normalized_distances[i] for i, key in enumerate(distances.keys())}\n",
    "\n",
    "# Print the normalized distances to verify\n",
    "print(normalized_distances_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create edge index and edge attributes from distances dictionary\n",
    "edge_index = []\n",
    "edge_attr = []\n",
    "\n",
    "for (i, j), distance in distances.items():\n",
    "    edge_index.append([i, j])\n",
    "    edge_index.append([j, i])  # Assuming undirected graph\n",
    "    edge_attr.append([distance])\n",
    "    edge_attr.append([distance])  # Assuming undirected graph\n",
    "\n",
    "edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "edge_attr = torch.tensor(edge_attr, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">**ZE EPIC GPU IMPORT**</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move data to device\n",
    "node_features_sequence_input_train = node_features_sequence_input_train.to(device)\n",
    "node_features_sequence_output_test = node_features_sequence_output_test.to(device)\n",
    "edge_index = edge_index.to(device)\n",
    "edge_attr = edge_attr.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">**ZE EPIC BATCHING**</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 823\n",
      "Number of full batches: 823\n"
     ]
    }
   ],
   "source": [
    "batch_size = node_features_sequence_input_train.shape[0] # Adjust this value based on your GPU memory capacity\n",
    "# Calculate the number of full batches\n",
    "num_full_batches_train = (node_features_sequence_input_train.size(0) // batch_size) * batch_size\n",
    "\n",
    "# Trim the input and output tensors to have a size divisible by the batch size\n",
    "trimmed_input_train = node_features_sequence_input_train[:num_full_batches_train]\n",
    "trimmed_output_train = node_features_sequence_output_train[:num_full_batches_train]\n",
    "\n",
    "# Create batches of data\n",
    "batched_input_train = trimmed_input_train.view(-1, batch_size, trimmed_input_train.size(1), trimmed_input_train.size(2))\n",
    "batched_output_train = trimmed_output_train.view(-1, batch_size, trimmed_output_train.size(1), trimmed_output_train.size(2))\n",
    "\n",
    "# Adjust the number of batches\n",
    "num_batches_train = batched_input_train.size(0)\n",
    "\n",
    "# Print the batch size and number of full batches to verify\n",
    "print(\"Batch size:\", batch_size)\n",
    "print(\"Number of full batches:\", num_full_batches_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 206\n",
      "Number of full batches: 823\n"
     ]
    }
   ],
   "source": [
    "batch_size = node_features_sequence_input_test.shape[0]  # Adjust this value based on your GPU memory capacity\n",
    "# Calculate the number of full batches\n",
    "num_full_batches_test = (node_features_sequence_input_test.size(0) // batch_size) * batch_size\n",
    "\n",
    "# Trim the input and output tensors to have a size divisible by the batch size\n",
    "trimmed_input_test = node_features_sequence_input_test[:num_full_batches_test]\n",
    "trimmed_output_test = node_features_sequence_output_test[:num_full_batches_test]\n",
    "\n",
    "# Create batches of data\n",
    "batched_input_test = trimmed_input_test.view(-1, batch_size, trimmed_input_test.size(1), trimmed_input_test.size(2))\n",
    "batched_output_test = trimmed_output_test.view(-1, batch_size, trimmed_output_test.size(1), trimmed_output_test.size(2))\n",
    "\n",
    "# Adjust the number of batches\n",
    "num_batches_test = batched_input_test.size(0)\n",
    "print(\"Batch size:\", batch_size)\n",
    "print(\"Number of full batches:\", num_full_batches_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">**ZE EPIC MODEL**</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNWithTransformer(torch.nn.Module):\n",
    "    def __init__(self, in_channels, edge_in_channels, hidden_channels, out_channels, num_transformer_layers, lstm_hidden_size, lstm_num_layers):\n",
    "        super(GNNWithTransformer, self).__init__()\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels, edge_dim=edge_in_channels)\n",
    "        self.conv2 = GATConv(hidden_channels, hidden_channels, edge_dim=edge_in_channels)\n",
    "        self.conv3 = GATConv(hidden_channels, hidden_channels, edge_dim=edge_in_channels)\n",
    "        self.batch_norm1 = BatchNorm(hidden_channels)\n",
    "        self.batch_norm2 = BatchNorm(hidden_channels)\n",
    "        self.batch_norm3 = BatchNorm(hidden_channels)\n",
    "        self.dropout = torch.nn.Dropout(p=0.5)\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(hidden_channels, lstm_hidden_size, lstm_num_layers, batch_first=True)\n",
    "        \n",
    "        # Transformer encoder layer\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=hidden_channels, nhead=16),\n",
    "            num_layers=num_transformer_layers\n",
    "        )\n",
    "        \n",
    "        self.output_layer = torch.nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # Calculate edge weights as the inverse of edge attributes\n",
    "        edge_weight = 1.0 / edge_attr\n",
    "\n",
    "        # Apply the first GAT convolution with edge weights\n",
    "        x1 = self.conv1(x, edge_index, edge_weight)\n",
    "        x1 = self.batch_norm1(x1)\n",
    "        x1 = F.relu(x1)\n",
    "        x1 = self.dropout(x1)\n",
    "        \n",
    "        # Apply the second GAT convolution with edge weights\n",
    "        x2 = self.conv2(x1, edge_index, edge_weight)\n",
    "        x2 = self.batch_norm2(x2)\n",
    "        x2 = F.relu(x2)\n",
    "        x2 = self.dropout(x2)\n",
    "        \n",
    "        # Apply the third GAT convolution with edge weights\n",
    "        x3 = self.conv3(x2, edge_index, edge_weight)\n",
    "        x3 = self.batch_norm3(x3)\n",
    "        x3 = F.relu(x3)\n",
    "        x3 = self.dropout(x3)\n",
    "        \n",
    "        # Add residual connections\n",
    "        x = x1 + x2 + x3 \n",
    "        \n",
    "        # Prepare data for Transformer\n",
    "        x = x.unsqueeze(0)  # Add batch dimension\n",
    "        \n",
    "        # Apply LSTM\n",
    "        x, _ = self.lstm(x)\n",
    "        \n",
    "        # Apply Transformer encoder\n",
    "        x = self.transformer_encoder(x)\n",
    "        \n",
    "        x = x.squeeze(0)  # Remove batch dimension\n",
    "        \n",
    "        # Apply the output layer\n",
    "        x = self.output_layer(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">**ZE EPIC MODEL SETUP**</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input channels: 67\n",
      "Number of output channels: 67\n",
      "Number of hidden channels: 256\n",
      "Number of transformer layers: 4\n",
      "LSTM hidden size: 256\n",
      "LSTM number of layers: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/HDD/RyanFolder/projects/Weather-Predition-Project/.venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters\n",
    "in_channels = node_features_sequence_input_train.shape[2]\n",
    "out_channels = in_channels  \n",
    "edge_in_channels = 1\n",
    "hidden_channels = 256\n",
    "num_transformer_layers = 4\n",
    "lstm_hidden_size = 256\n",
    "lstm_num_layers = 1\n",
    "\n",
    "print(\"Number of input channels:\", in_channels)\n",
    "print(\"Number of output channels:\", out_channels)\n",
    "print(\"Number of hidden channels:\", hidden_channels)\n",
    "print(\"Number of transformer layers:\", num_transformer_layers)\n",
    "print(\"LSTM hidden size:\", lstm_hidden_size)\n",
    "print(\"LSTM number of layers:\", lstm_num_layers)\n",
    "\n",
    "model = GNNWithTransformer(in_channels, edge_in_channels, hidden_channels, out_channels, num_transformer_layers, lstm_hidden_size, lstm_num_layers).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of epochs: 1000\n",
      "Learning rate: 0.005\n",
      "Scheduler mode: min\n",
      "Scheduler factor: 0.8\n",
      "Scheduler patience: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/HDD/RyanFolder/projects/Weather-Predition-Project/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters\n",
    "num_epochs = 500  # Adjust the number of epochs as needed\n",
    "learning_rate = 0.01\n",
    "scheduler_mode = 'min'\n",
    "scheduler_factor = 0.8\n",
    "scheduler_patience = 10\n",
    "swa_start = 100  # Epoch to start SWA\n",
    "# num_warmpup_steps = 10 \n",
    "# num_training_steps = num_epochs * num_batches_train\n",
    "\n",
    "#Print all parameter \n",
    "print(\"Number of epochs:\", num_epochs)\n",
    "print(\"Learning rate:\", learning_rate)\n",
    "print(\"Scheduler mode:\", scheduler_mode)\n",
    "print(\"Scheduler factor:\", scheduler_factor)\n",
    "print(\"Scheduler patience:\", scheduler_patience)\n",
    "# print(\"Number of warmup steps:\", num_warmpup_steps)\n",
    "# print(\"Number of training steps:\", num_training_steps)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "swa_model = AveragedModel(model)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=scheduler_mode, factor=scheduler_factor, patience=scheduler_patience, verbose=True,min_lr=1e-5)\n",
    "swa_scheduler = SWALR(optimizer, swa_lr=0.005)\n",
    "swa_model = swa_model.to(device)\n",
    "loss_fn = torch.nn.SmoothL1Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">**ZE EPIC MODEL TRAIN**</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 4.696568965911865, Average MAE: 5.0319437980651855, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 2, Average Loss: 4.551151752471924, Average MAE: 4.944742679595947, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 3, Average Loss: 4.29141902923584, Average MAE: 4.648752689361572, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 4, Average Loss: 4.071187496185303, Average MAE: 4.358545303344727, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 5, Average Loss: 3.939687967300415, Average MAE: 4.208992958068848, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 6, Average Loss: 3.8078174591064453, Average MAE: 4.048708915710449, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 7, Average Loss: 3.6864356994628906, Average MAE: 3.9123764038085938, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 8, Average Loss: 3.5711522102355957, Average MAE: 3.797283172607422, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 9, Average Loss: 3.45291805267334, Average MAE: 3.6775317192077637, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 10, Average Loss: 3.3299286365509033, Average MAE: 3.54669189453125, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 11, Average Loss: 3.206416606903076, Average MAE: 3.4203035831451416, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 12, Average Loss: 3.081758737564087, Average MAE: 3.2996647357940674, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 13, Average Loss: 2.948613166809082, Average MAE: 3.166778326034546, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 14, Average Loss: 2.808499336242676, Average MAE: 3.0183041095733643, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 15, Average Loss: 2.6687636375427246, Average MAE: 2.8747711181640625, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 16, Average Loss: 2.5290887355804443, Average MAE: 2.740030527114868, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 17, Average Loss: 2.3852524757385254, Average MAE: 2.596381664276123, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 18, Average Loss: 2.2354135513305664, Average MAE: 2.440992593765259, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 19, Average Loss: 2.084591865539551, Average MAE: 2.2914059162139893, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 20, Average Loss: 1.9333176612854004, Average MAE: 2.1438345909118652, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 21, Average Loss: 1.780547022819519, Average MAE: 1.9882756471633911, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 22, Average Loss: 1.629284143447876, Average MAE: 1.8338500261306763, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 23, Average Loss: 1.5056281089782715, Average MAE: 1.7070457935333252, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 24, Average Loss: 1.4477962255477905, Average MAE: 1.6503016948699951, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 25, Average Loss: 1.4237698316574097, Average MAE: 1.6287943124771118, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 26, Average Loss: 1.3917720317840576, Average MAE: 1.59565269947052, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 27, Average Loss: 1.3522645235061646, Average MAE: 1.556415319442749, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 28, Average Loss: 1.3054945468902588, Average MAE: 1.5104167461395264, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 29, Average Loss: 1.2510074377059937, Average MAE: 1.4563623666763306, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 30, Average Loss: 1.1873764991760254, Average MAE: 1.3910702466964722, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 31, Average Loss: 1.1145174503326416, Average MAE: 1.3147035837173462, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 32, Average Loss: 1.0750054121017456, Average MAE: 1.2718183994293213, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 33, Average Loss: 1.0786235332489014, Average MAE: 1.2783352136611938, Learning Rate: 0.005, Current Patience: 1\n",
      "Epoch 34, Average Loss: 1.0449320077896118, Average MAE: 1.250014305114746, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 35, Average Loss: 0.9937268495559692, Average MAE: 1.1932624578475952, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 36, Average Loss: 0.9688146114349365, Average MAE: 1.16350519657135, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 37, Average Loss: 0.9670702815055847, Average MAE: 1.1676996946334839, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 38, Average Loss: 0.9579823017120361, Average MAE: 1.1562386751174927, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 39, Average Loss: 0.9389128088951111, Average MAE: 1.1323761940002441, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 40, Average Loss: 0.9212032556533813, Average MAE: 1.1150565147399902, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 41, Average Loss: 0.9122625589370728, Average MAE: 1.1067641973495483, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 42, Average Loss: 0.9077000617980957, Average MAE: 1.1038113832473755, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 43, Average Loss: 0.8972613215446472, Average MAE: 1.0941356420516968, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 44, Average Loss: 0.8801944851875305, Average MAE: 1.0738221406936646, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 45, Average Loss: 0.8653373122215271, Average MAE: 1.0551071166992188, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 46, Average Loss: 0.8563064336776733, Average MAE: 1.0455764532089233, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 47, Average Loss: 0.8471291065216064, Average MAE: 1.036711573600769, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 48, Average Loss: 0.8337578773498535, Average MAE: 1.0229908227920532, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 49, Average Loss: 0.8188136219978333, Average MAE: 1.0065367221832275, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 50, Average Loss: 0.8080993294715881, Average MAE: 0.9951035976409912, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 51, Average Loss: 0.8017430305480957, Average MAE: 0.9903078079223633, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 52, Average Loss: 0.794273853302002, Average MAE: 0.981591522693634, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 53, Average Loss: 0.7853559851646423, Average MAE: 0.9711282849311829, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 54, Average Loss: 0.7780328989028931, Average MAE: 0.9622638821601868, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 55, Average Loss: 0.7740850448608398, Average MAE: 0.9581397771835327, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 56, Average Loss: 0.7710950374603271, Average MAE: 0.9557268023490906, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 57, Average Loss: 0.7667465209960938, Average MAE: 0.9493355751037598, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 58, Average Loss: 0.7637637853622437, Average MAE: 0.9445511102676392, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 59, Average Loss: 0.7631798386573792, Average MAE: 0.9440733790397644, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 60, Average Loss: 0.7629279494285583, Average MAE: 0.9440621733665466, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 61, Average Loss: 0.7611570954322815, Average MAE: 0.9410356283187866, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 62, Average Loss: 0.7591800093650818, Average MAE: 0.9371883869171143, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 63, Average Loss: 0.7593885660171509, Average MAE: 0.937262773513794, Learning Rate: 0.005, Current Patience: 1\n",
      "Epoch 64, Average Loss: 0.7602867484092712, Average MAE: 0.9380210041999817, Learning Rate: 0.005, Current Patience: 2\n",
      "Epoch 65, Average Loss: 0.759905219078064, Average MAE: 0.9370061159133911, Learning Rate: 0.005, Current Patience: 3\n",
      "Epoch 66, Average Loss: 0.7591919302940369, Average MAE: 0.9349911212921143, Learning Rate: 0.005, Current Patience: 4\n",
      "Epoch 67, Average Loss: 0.7590797543525696, Average MAE: 0.9339733123779297, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 68, Average Loss: 0.7597381472587585, Average MAE: 0.934296190738678, Learning Rate: 0.005, Current Patience: 1\n",
      "Epoch 69, Average Loss: 0.7596972584724426, Average MAE: 0.9338034987449646, Learning Rate: 0.005, Current Patience: 2\n",
      "Epoch 70, Average Loss: 0.7592771053314209, Average MAE: 0.932600736618042, Learning Rate: 0.005, Current Patience: 3\n",
      "Epoch 71, Average Loss: 0.7593246698379517, Average MAE: 0.9312395453453064, Learning Rate: 0.005, Current Patience: 4\n",
      "Epoch 72, Average Loss: 0.759535551071167, Average MAE: 0.9308185577392578, Learning Rate: 0.005, Current Patience: 5\n",
      "Epoch 73, Average Loss: 0.7592616081237793, Average MAE: 0.929866373538971, Learning Rate: 0.005, Current Patience: 6\n",
      "Epoch 74, Average Loss: 0.7584345936775208, Average MAE: 0.9290152788162231, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 75, Average Loss: 0.7583527565002441, Average MAE: 0.927717924118042, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 76, Average Loss: 0.757895827293396, Average MAE: 0.9263571500778198, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 77, Average Loss: 0.7570671439170837, Average MAE: 0.9246092438697815, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 78, Average Loss: 0.756008505821228, Average MAE: 0.9227626919746399, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 79, Average Loss: 0.755418598651886, Average MAE: 0.921264111995697, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 80, Average Loss: 0.7549581527709961, Average MAE: 0.9202877283096313, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 81, Average Loss: 0.7546049952507019, Average MAE: 0.918906033039093, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 82, Average Loss: 0.7538909316062927, Average MAE: 0.9174807071685791, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 83, Average Loss: 0.7533979415893555, Average MAE: 0.9163622260093689, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 84, Average Loss: 0.7529120445251465, Average MAE: 0.9151855707168579, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 85, Average Loss: 0.7524766325950623, Average MAE: 0.9135591983795166, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 86, Average Loss: 0.7520774602890015, Average MAE: 0.9126938581466675, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 87, Average Loss: 0.7521352767944336, Average MAE: 0.9123347997665405, Learning Rate: 0.005, Current Patience: 1\n",
      "Epoch 88, Average Loss: 0.7520554661750793, Average MAE: 0.911533534526825, Learning Rate: 0.005, Current Patience: 2\n",
      "Epoch 89, Average Loss: 0.751532256603241, Average MAE: 0.9098544120788574, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 90, Average Loss: 0.7516847252845764, Average MAE: 0.9092680811882019, Learning Rate: 0.005, Current Patience: 1\n",
      "Epoch 91, Average Loss: 0.7514092326164246, Average MAE: 0.9085839986801147, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 92, Average Loss: 0.7514521479606628, Average MAE: 0.9082835912704468, Learning Rate: 0.005, Current Patience: 1\n",
      "Epoch 93, Average Loss: 0.7511001825332642, Average MAE: 0.9071543216705322, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 94, Average Loss: 0.7513771057128906, Average MAE: 0.90678870677948, Learning Rate: 0.005, Current Patience: 1\n",
      "Epoch 95, Average Loss: 0.7511638402938843, Average MAE: 0.905832827091217, Learning Rate: 0.005, Current Patience: 2\n",
      "Epoch 96, Average Loss: 0.7510371208190918, Average MAE: 0.9052351713180542, Learning Rate: 0.005, Current Patience: 3\n",
      "Epoch 97, Average Loss: 0.7510321140289307, Average MAE: 0.9048821330070496, Learning Rate: 0.005, Current Patience: 4\n",
      "Epoch 98, Average Loss: 0.7506791949272156, Average MAE: 0.9038163423538208, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 99, Average Loss: 0.7507976293563843, Average MAE: 0.9032387733459473, Learning Rate: 0.005, Current Patience: 1\n",
      "Epoch 100, Average Loss: 0.7504757046699524, Average MAE: 0.9024228453636169, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 101, Average Loss: 0.7502647042274475, Average MAE: 0.9015635251998901, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 102, Average Loss: 0.7502772212028503, Average MAE: 0.901314914226532, Learning Rate: 0.005, Current Patience: 1\n",
      "Epoch 103, Average Loss: 0.7502619624137878, Average MAE: 0.9005839824676514, Learning Rate: 0.005, Current Patience: 2\n",
      "Epoch 104, Average Loss: 0.749849796295166, Average MAE: 0.8996286988258362, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 105, Average Loss: 0.7498955130577087, Average MAE: 0.8991800546646118, Learning Rate: 0.005, Current Patience: 1\n",
      "Epoch 106, Average Loss: 0.7495447397232056, Average MAE: 0.8982754349708557, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 107, Average Loss: 0.7494125962257385, Average MAE: 0.8975261449813843, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 108, Average Loss: 0.7493797540664673, Average MAE: 0.8971973657608032, Learning Rate: 0.005, Current Patience: 1\n",
      "Epoch 109, Average Loss: 0.7492604851722717, Average MAE: 0.8964175581932068, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 110, Average Loss: 0.749045193195343, Average MAE: 0.8956803679466248, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 111, Average Loss: 0.7491409182548523, Average MAE: 0.8952707648277283, Learning Rate: 0.005, Current Patience: 1\n",
      "Epoch 112, Average Loss: 0.7489838004112244, Average MAE: 0.8946099877357483, Learning Rate: 0.005, Current Patience: 2\n",
      "Epoch 113, Average Loss: 0.7489985823631287, Average MAE: 0.8941481709480286, Learning Rate: 0.005, Current Patience: 3\n",
      "Epoch 114, Average Loss: 0.7489609122276306, Average MAE: 0.8937543630599976, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 115, Average Loss: 0.7488725781440735, Average MAE: 0.8931847810745239, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 116, Average Loss: 0.7486948370933533, Average MAE: 0.8924899101257324, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 117, Average Loss: 0.7483281493186951, Average MAE: 0.8916429877281189, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 118, Average Loss: 0.7485742568969727, Average MAE: 0.8916524648666382, Learning Rate: 0.005, Current Patience: 1\n",
      "Epoch 119, Average Loss: 0.7486247420310974, Average MAE: 0.8911654353141785, Learning Rate: 0.005, Current Patience: 2\n",
      "Epoch 120, Average Loss: 0.748444139957428, Average MAE: 0.8904545307159424, Learning Rate: 0.005, Current Patience: 3\n",
      "Epoch 121, Average Loss: 0.7485507726669312, Average MAE: 0.8901309967041016, Learning Rate: 0.005, Current Patience: 4\n",
      "Epoch 122, Average Loss: 0.7479982376098633, Average MAE: 0.8891388773918152, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 123, Average Loss: 0.7482306361198425, Average MAE: 0.8890149593353271, Learning Rate: 0.005, Current Patience: 1\n",
      "Epoch 124, Average Loss: 0.7481871843338013, Average MAE: 0.8885669708251953, Learning Rate: 0.005, Current Patience: 2\n",
      "Epoch 125, Average Loss: 0.7481558918952942, Average MAE: 0.8879965543746948, Learning Rate: 0.005, Current Patience: 3\n",
      "Epoch 126, Average Loss: 0.748006284236908, Average MAE: 0.88729327917099, Learning Rate: 0.005, Current Patience: 4\n",
      "Epoch 127, Average Loss: 0.7477700710296631, Average MAE: 0.8868971467018127, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 128, Average Loss: 0.747749924659729, Average MAE: 0.8864765167236328, Learning Rate: 0.005, Current Patience: 1\n",
      "Epoch 129, Average Loss: 0.7476918697357178, Average MAE: 0.885852575302124, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 130, Average Loss: 0.7477946281433105, Average MAE: 0.885635256767273, Learning Rate: 0.005, Current Patience: 1\n",
      "Epoch 131, Average Loss: 0.7478204965591431, Average MAE: 0.8853208422660828, Learning Rate: 0.005, Current Patience: 2\n",
      "Epoch 132, Average Loss: 0.7475472092628479, Average MAE: 0.8847619295120239, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 133, Average Loss: 0.7475029230117798, Average MAE: 0.8845513463020325, Learning Rate: 0.005, Current Patience: 1\n",
      "Epoch 134, Average Loss: 0.747539222240448, Average MAE: 0.8839454054832458, Learning Rate: 0.005, Current Patience: 2\n",
      "Epoch 135, Average Loss: 0.7474260330200195, Average MAE: 0.883669912815094, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 136, Average Loss: 0.747455358505249, Average MAE: 0.8836197257041931, Learning Rate: 0.005, Current Patience: 1\n",
      "Epoch 137, Average Loss: 0.7474864721298218, Average MAE: 0.8833528161048889, Learning Rate: 0.005, Current Patience: 2\n",
      "Epoch 138, Average Loss: 0.7474834322929382, Average MAE: 0.8829759359359741, Learning Rate: 0.005, Current Patience: 3\n",
      "Epoch 139, Average Loss: 0.7471812963485718, Average MAE: 0.8823114633560181, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 140, Average Loss: 0.7471983432769775, Average MAE: 0.8813998103141785, Learning Rate: 0.005, Current Patience: 1\n",
      "Epoch 141, Average Loss: 0.7472076416015625, Average MAE: 0.8811026811599731, Learning Rate: 0.005, Current Patience: 2\n",
      "Epoch 142, Average Loss: 0.7471862435340881, Average MAE: 0.8808345198631287, Learning Rate: 0.005, Current Patience: 3\n",
      "Epoch 143, Average Loss: 0.7470254302024841, Average MAE: 0.8807496428489685, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 144, Average Loss: 0.7470399737358093, Average MAE: 0.8802548050880432, Learning Rate: 0.005, Current Patience: 1\n",
      "Epoch 145, Average Loss: 0.7470018863677979, Average MAE: 0.8797565698623657, Learning Rate: 0.005, Current Patience: 2\n",
      "Epoch 146, Average Loss: 0.7467249035835266, Average MAE: 0.8787649273872375, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 147, Average Loss: 0.746818482875824, Average MAE: 0.8787562847137451, Learning Rate: 0.005, Current Patience: 1\n",
      "Epoch 148, Average Loss: 0.746799111366272, Average MAE: 0.8786102533340454, Learning Rate: 0.005, Current Patience: 2\n",
      "Epoch 149, Average Loss: 0.7465648651123047, Average MAE: 0.878034234046936, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 150, Average Loss: 0.7466037273406982, Average MAE: 0.8776620030403137, Learning Rate: 0.005, Current Patience: 1\n",
      "Epoch 151, Average Loss: 0.746856689453125, Average MAE: 0.8774986863136292, Learning Rate: 0.005, Current Patience: 2\n",
      "Epoch 152, Average Loss: 0.746494472026825, Average MAE: 0.8769276142120361, Learning Rate: 0.005, Current Patience: 3\n",
      "Epoch 153, Average Loss: 0.7466228604316711, Average MAE: 0.876779317855835, Learning Rate: 0.005, Current Patience: 4\n",
      "Epoch 154, Average Loss: 0.7465913891792297, Average MAE: 0.876807451248169, Learning Rate: 0.005, Current Patience: 5\n",
      "Epoch 155, Average Loss: 0.746579110622406, Average MAE: 0.876018226146698, Learning Rate: 0.005, Current Patience: 6\n",
      "Epoch 156, Average Loss: 0.7465722560882568, Average MAE: 0.8761687278747559, Learning Rate: 0.005, Current Patience: 7\n",
      "Epoch 157, Average Loss: 0.7464938163757324, Average MAE: 0.8753327131271362, Learning Rate: 0.005, Current Patience: 8\n",
      "Epoch 158, Average Loss: 0.7464666366577148, Average MAE: 0.8754145503044128, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 159, Average Loss: 0.7463343143463135, Average MAE: 0.8747367858886719, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 160, Average Loss: 0.7463423013687134, Average MAE: 0.8747623562812805, Learning Rate: 0.005, Current Patience: 1\n",
      "Epoch 161, Average Loss: 0.7461678981781006, Average MAE: 0.8741216659545898, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 162, Average Loss: 0.7460559606552124, Average MAE: 0.873896598815918, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 163, Average Loss: 0.7460047602653503, Average MAE: 0.8733057379722595, Learning Rate: 0.005, Current Patience: 1\n",
      "Epoch 164, Average Loss: 0.7461773157119751, Average MAE: 0.8735759258270264, Learning Rate: 0.005, Current Patience: 2\n",
      "Epoch 165, Average Loss: 0.7462909817695618, Average MAE: 0.8731203079223633, Learning Rate: 0.005, Current Patience: 3\n",
      "Epoch 166, Average Loss: 0.746189534664154, Average MAE: 0.8731693029403687, Learning Rate: 0.005, Current Patience: 4\n",
      "Epoch 167, Average Loss: 0.7460678219795227, Average MAE: 0.8724632263183594, Learning Rate: 0.005, Current Patience: 5\n",
      "Epoch 168, Average Loss: 0.7460120320320129, Average MAE: 0.8725704550743103, Learning Rate: 0.005, Current Patience: 6\n",
      "Epoch 169, Average Loss: 0.7460203170776367, Average MAE: 0.87211674451828, Learning Rate: 0.005, Current Patience: 7\n",
      "Epoch 170, Average Loss: 0.7460085153579712, Average MAE: 0.8719882369041443, Learning Rate: 0.005, Current Patience: 8\n",
      "Epoch 171, Average Loss: 0.7460312247276306, Average MAE: 0.8718895316123962, Learning Rate: 0.005, Current Patience: 9\n",
      "Epoch 172, Average Loss: 0.7458797693252563, Average MAE: 0.8717748522758484, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 173, Average Loss: 0.745954155921936, Average MAE: 0.8715658783912659, Learning Rate: 0.005, Current Patience: 1\n",
      "Epoch 174, Average Loss: 0.7459850311279297, Average MAE: 0.8716074824333191, Learning Rate: 0.005, Current Patience: 2\n",
      "Epoch 175, Average Loss: 0.7458819150924683, Average MAE: 0.8710383176803589, Learning Rate: 0.005, Current Patience: 3\n",
      "Epoch 176, Average Loss: 0.7458256483078003, Average MAE: 0.8709673285484314, Learning Rate: 0.005, Current Patience: 4\n",
      "Epoch 177, Average Loss: 0.7459167838096619, Average MAE: 0.8701927065849304, Learning Rate: 0.005, Current Patience: 5\n",
      "Epoch 178, Average Loss: 0.7456424832344055, Average MAE: 0.8704504370689392, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 179, Average Loss: 0.7457212209701538, Average MAE: 0.8690406084060669, Learning Rate: 0.005, Current Patience: 1\n",
      "Epoch 180, Average Loss: 0.7456865310668945, Average MAE: 0.870711088180542, Learning Rate: 0.005, Current Patience: 2\n",
      "Epoch 181, Average Loss: 0.7457643747329712, Average MAE: 0.8689730763435364, Learning Rate: 0.005, Current Patience: 3\n",
      "Epoch 182, Average Loss: 0.7458767890930176, Average MAE: 0.8708646893501282, Learning Rate: 0.005, Current Patience: 4\n",
      "Epoch 183, Average Loss: 0.7456942796707153, Average MAE: 0.8683566451072693, Learning Rate: 0.005, Current Patience: 5\n",
      "Epoch 184, Average Loss: 0.7457789182662964, Average MAE: 0.8693909049034119, Learning Rate: 0.005, Current Patience: 6\n",
      "Epoch 185, Average Loss: 0.7459074258804321, Average MAE: 0.86893630027771, Learning Rate: 0.005, Current Patience: 7\n",
      "Epoch 186, Average Loss: 0.7456156611442566, Average MAE: 0.8679288625717163, Learning Rate: 0.005, Current Patience: 8\n",
      "Epoch 187, Average Loss: 0.745590329170227, Average MAE: 0.8690288662910461, Learning Rate: 0.005, Current Patience: 9\n",
      "Epoch 188, Average Loss: 0.7455896735191345, Average MAE: 0.8675432801246643, Learning Rate: 0.005, Current Patience: 10\n",
      "Epoch 189, Average Loss: 0.7457658052444458, Average MAE: 0.868606448173523, Learning Rate: 0.004, Current Patience: 0\n",
      "Epoch 190, Average Loss: 0.745367169380188, Average MAE: 0.8672581315040588, Learning Rate: 0.004, Current Patience: 0\n",
      "Epoch 191, Average Loss: 0.7455095648765564, Average MAE: 0.8672119379043579, Learning Rate: 0.004, Current Patience: 1\n",
      "Epoch 192, Average Loss: 0.745479166507721, Average MAE: 0.8675429224967957, Learning Rate: 0.004, Current Patience: 2\n",
      "Epoch 193, Average Loss: 0.7452959418296814, Average MAE: 0.866715669631958, Learning Rate: 0.004, Current Patience: 3\n",
      "Epoch 194, Average Loss: 0.7455820441246033, Average MAE: 0.8668205142021179, Learning Rate: 0.004, Current Patience: 4\n",
      "Epoch 195, Average Loss: 0.7453352808952332, Average MAE: 0.8670518398284912, Learning Rate: 0.004, Current Patience: 5\n",
      "Epoch 196, Average Loss: 0.7454109787940979, Average MAE: 0.866603434085846, Learning Rate: 0.004, Current Patience: 6\n",
      "Epoch 197, Average Loss: 0.7454934120178223, Average MAE: 0.8662847876548767, Learning Rate: 0.004, Current Patience: 7\n",
      "Epoch 198, Average Loss: 0.7453826069831848, Average MAE: 0.866575300693512, Learning Rate: 0.004, Current Patience: 8\n",
      "Epoch 199, Average Loss: 0.7452976703643799, Average MAE: 0.8660534620285034, Learning Rate: 0.004, Current Patience: 9\n",
      "Epoch 200, Average Loss: 0.7453741431236267, Average MAE: 0.8659067153930664, Learning Rate: 0.004, Current Patience: 10\n",
      "Epoch 201, Average Loss: 0.7452619075775146, Average MAE: 0.8658484220504761, Learning Rate: 0.004, Current Patience: 0\n",
      "Epoch 202, Average Loss: 0.7455415725708008, Average MAE: 0.8659019470214844, Learning Rate: 0.004, Current Patience: 1\n",
      "Epoch 203, Average Loss: 0.7454240918159485, Average MAE: 0.8656460046768188, Learning Rate: 0.004, Current Patience: 2\n",
      "Epoch 204, Average Loss: 0.7451711297035217, Average MAE: 0.8656300902366638, Learning Rate: 0.004, Current Patience: 0\n",
      "Epoch 205, Average Loss: 0.7451956272125244, Average MAE: 0.8651036620140076, Learning Rate: 0.004, Current Patience: 1\n",
      "Epoch 206, Average Loss: 0.7452619671821594, Average MAE: 0.865119993686676, Learning Rate: 0.004, Current Patience: 2\n",
      "Epoch 207, Average Loss: 0.7453533411026001, Average MAE: 0.865415632724762, Learning Rate: 0.004, Current Patience: 3\n",
      "Epoch 208, Average Loss: 0.7453306317329407, Average MAE: 0.8649235963821411, Learning Rate: 0.004, Current Patience: 4\n",
      "Epoch 209, Average Loss: 0.7451575994491577, Average MAE: 0.864762008190155, Learning Rate: 0.004, Current Patience: 5\n",
      "Epoch 210, Average Loss: 0.7452577948570251, Average MAE: 0.8649141192436218, Learning Rate: 0.004, Current Patience: 6\n",
      "Epoch 211, Average Loss: 0.7453190684318542, Average MAE: 0.8647307753562927, Learning Rate: 0.004, Current Patience: 7\n",
      "Epoch 212, Average Loss: 0.7453034520149231, Average MAE: 0.864629864692688, Learning Rate: 0.004, Current Patience: 8\n",
      "Epoch 213, Average Loss: 0.7452398538589478, Average MAE: 0.8643516302108765, Learning Rate: 0.004, Current Patience: 9\n",
      "Epoch 214, Average Loss: 0.745376467704773, Average MAE: 0.8644724488258362, Learning Rate: 0.004, Current Patience: 10\n",
      "Epoch 215, Average Loss: 0.7451285719871521, Average MAE: 0.8640923500061035, Learning Rate: 0.0032, Current Patience: 0\n",
      "Epoch 216, Average Loss: 0.7452647089958191, Average MAE: 0.8640967011451721, Learning Rate: 0.0032, Current Patience: 1\n",
      "Epoch 217, Average Loss: 0.7451966404914856, Average MAE: 0.8638838529586792, Learning Rate: 0.0032, Current Patience: 2\n",
      "Epoch 218, Average Loss: 0.7452093362808228, Average MAE: 0.8637081980705261, Learning Rate: 0.0032, Current Patience: 3\n",
      "Epoch 219, Average Loss: 0.7452640533447266, Average MAE: 0.8638861179351807, Learning Rate: 0.0032, Current Patience: 4\n",
      "Epoch 220, Average Loss: 0.7451450824737549, Average MAE: 0.8635963797569275, Learning Rate: 0.0032, Current Patience: 5\n",
      "Epoch 221, Average Loss: 0.7450977563858032, Average MAE: 0.8634018301963806, Learning Rate: 0.0032, Current Patience: 6\n",
      "Epoch 222, Average Loss: 0.7451027035713196, Average MAE: 0.8634247183799744, Learning Rate: 0.0032, Current Patience: 7\n",
      "Epoch 223, Average Loss: 0.7449796199798584, Average MAE: 0.8632189035415649, Learning Rate: 0.0032, Current Patience: 0\n",
      "Epoch 224, Average Loss: 0.745093047618866, Average MAE: 0.8632032871246338, Learning Rate: 0.0032, Current Patience: 1\n",
      "Epoch 225, Average Loss: 0.7450202703475952, Average MAE: 0.8628936409950256, Learning Rate: 0.0032, Current Patience: 2\n",
      "Epoch 226, Average Loss: 0.7451686263084412, Average MAE: 0.8632031083106995, Learning Rate: 0.0032, Current Patience: 3\n",
      "Epoch 227, Average Loss: 0.7452611327171326, Average MAE: 0.8632655739784241, Learning Rate: 0.0032, Current Patience: 4\n",
      "Epoch 228, Average Loss: 0.7449648380279541, Average MAE: 0.8628152012825012, Learning Rate: 0.0032, Current Patience: 5\n",
      "Epoch 229, Average Loss: 0.7452561855316162, Average MAE: 0.8629145622253418, Learning Rate: 0.0032, Current Patience: 6\n",
      "Epoch 230, Average Loss: 0.7449145317077637, Average MAE: 0.8626511693000793, Learning Rate: 0.0032, Current Patience: 7\n",
      "Epoch 231, Average Loss: 0.7450492978096008, Average MAE: 0.8625537157058716, Learning Rate: 0.0032, Current Patience: 8\n",
      "Epoch 232, Average Loss: 0.745026707649231, Average MAE: 0.8624905347824097, Learning Rate: 0.0032, Current Patience: 9\n",
      "Epoch 233, Average Loss: 0.7447974681854248, Average MAE: 0.8621984720230103, Learning Rate: 0.0032, Current Patience: 0\n",
      "Epoch 234, Average Loss: 0.7449280619621277, Average MAE: 0.8622463345527649, Learning Rate: 0.0032, Current Patience: 1\n",
      "Epoch 235, Average Loss: 0.7450337409973145, Average MAE: 0.8623045682907104, Learning Rate: 0.0032, Current Patience: 2\n",
      "Epoch 236, Average Loss: 0.7448421716690063, Average MAE: 0.8620418906211853, Learning Rate: 0.0032, Current Patience: 3\n",
      "Epoch 237, Average Loss: 0.7451021075248718, Average MAE: 0.8621804118156433, Learning Rate: 0.0032, Current Patience: 4\n",
      "Epoch 238, Average Loss: 0.7448002696037292, Average MAE: 0.8617193698883057, Learning Rate: 0.0032, Current Patience: 5\n",
      "Epoch 239, Average Loss: 0.7451027631759644, Average MAE: 0.862048327922821, Learning Rate: 0.0032, Current Patience: 6\n",
      "Epoch 240, Average Loss: 0.7448393702507019, Average MAE: 0.8618150353431702, Learning Rate: 0.0032, Current Patience: 7\n",
      "Epoch 241, Average Loss: 0.7447585463523865, Average MAE: 0.8613842725753784, Learning Rate: 0.0032, Current Patience: 8\n",
      "Epoch 242, Average Loss: 0.7449412941932678, Average MAE: 0.861668586730957, Learning Rate: 0.0032, Current Patience: 9\n",
      "Epoch 243, Average Loss: 0.7449613213539124, Average MAE: 0.861599326133728, Learning Rate: 0.0032, Current Patience: 10\n",
      "Epoch 244, Average Loss: 0.7447074055671692, Average MAE: 0.8612475395202637, Learning Rate: 0.0032, Current Patience: 0\n",
      "Epoch 245, Average Loss: 0.7447529435157776, Average MAE: 0.8612014055252075, Learning Rate: 0.0032, Current Patience: 1\n",
      "Epoch 246, Average Loss: 0.7447504997253418, Average MAE: 0.8610795140266418, Learning Rate: 0.0032, Current Patience: 2\n",
      "Epoch 247, Average Loss: 0.7448269128799438, Average MAE: 0.8611577153205872, Learning Rate: 0.0032, Current Patience: 3\n",
      "Epoch 248, Average Loss: 0.7448000907897949, Average MAE: 0.8611419200897217, Learning Rate: 0.0032, Current Patience: 4\n",
      "Epoch 249, Average Loss: 0.7450012564659119, Average MAE: 0.8610886931419373, Learning Rate: 0.0032, Current Patience: 5\n",
      "Epoch 250, Average Loss: 0.7449568510055542, Average MAE: 0.8610591888427734, Learning Rate: 0.0032, Current Patience: 6\n",
      "Epoch 251, Average Loss: 0.7447167038917542, Average MAE: 0.8607712984085083, Learning Rate: 0.0032, Current Patience: 7\n",
      "Epoch 252, Average Loss: 0.7447299957275391, Average MAE: 0.8607050776481628, Learning Rate: 0.0032, Current Patience: 8\n",
      "Epoch 253, Average Loss: 0.7447614669799805, Average MAE: 0.8606391549110413, Learning Rate: 0.0032, Current Patience: 9\n",
      "Epoch 254, Average Loss: 0.7448365092277527, Average MAE: 0.8606228828430176, Learning Rate: 0.0032, Current Patience: 10\n",
      "Epoch 255, Average Loss: 0.7446208596229553, Average MAE: 0.8602690696716309, Learning Rate: 0.0032, Current Patience: 0\n",
      "Epoch 256, Average Loss: 0.7448188066482544, Average MAE: 0.8604285717010498, Learning Rate: 0.0032, Current Patience: 1\n",
      "Epoch 257, Average Loss: 0.7447716593742371, Average MAE: 0.8604410886764526, Learning Rate: 0.0032, Current Patience: 2\n",
      "Epoch 258, Average Loss: 0.7448098659515381, Average MAE: 0.860146701335907, Learning Rate: 0.0032, Current Patience: 3\n",
      "Epoch 259, Average Loss: 0.7448020577430725, Average MAE: 0.86102694272995, Learning Rate: 0.0032, Current Patience: 4\n",
      "Epoch 260, Average Loss: 0.7447413206100464, Average MAE: 0.8598853349685669, Learning Rate: 0.0032, Current Patience: 5\n",
      "Epoch 261, Average Loss: 0.7447532415390015, Average MAE: 0.8601180911064148, Learning Rate: 0.0032, Current Patience: 6\n",
      "Epoch 262, Average Loss: 0.7446942329406738, Average MAE: 0.8606401681900024, Learning Rate: 0.0032, Current Patience: 7\n",
      "Epoch 263, Average Loss: 0.7447607517242432, Average MAE: 0.8596529364585876, Learning Rate: 0.0032, Current Patience: 8\n",
      "Epoch 264, Average Loss: 0.744713306427002, Average MAE: 0.8600319027900696, Learning Rate: 0.0032, Current Patience: 9\n",
      "Epoch 265, Average Loss: 0.7447905540466309, Average MAE: 0.8600081205368042, Learning Rate: 0.0032, Current Patience: 10\n",
      "Epoch 266, Average Loss: 0.7446033954620361, Average MAE: 0.859409749507904, Learning Rate: 0.00256, Current Patience: 0\n",
      "Epoch 267, Average Loss: 0.7448517084121704, Average MAE: 0.860176146030426, Learning Rate: 0.00256, Current Patience: 1\n",
      "Epoch 268, Average Loss: 0.7446675300598145, Average MAE: 0.859258770942688, Learning Rate: 0.00256, Current Patience: 2\n",
      "Epoch 269, Average Loss: 0.7446976900100708, Average MAE: 0.8594488501548767, Learning Rate: 0.00256, Current Patience: 3\n",
      "Epoch 270, Average Loss: 0.744376003742218, Average MAE: 0.8597217202186584, Learning Rate: 0.00256, Current Patience: 0\n",
      "Epoch 271, Average Loss: 0.7447630763053894, Average MAE: 0.8592517971992493, Learning Rate: 0.00256, Current Patience: 1\n",
      "Epoch 272, Average Loss: 0.7446823120117188, Average MAE: 0.8591397404670715, Learning Rate: 0.00256, Current Patience: 2\n",
      "Epoch 273, Average Loss: 0.7447258234024048, Average MAE: 0.8595783114433289, Learning Rate: 0.00256, Current Patience: 3\n",
      "Epoch 274, Average Loss: 0.7449514865875244, Average MAE: 0.8595601916313171, Learning Rate: 0.00256, Current Patience: 4\n",
      "Epoch 275, Average Loss: 0.7444764375686646, Average MAE: 0.858671247959137, Learning Rate: 0.00256, Current Patience: 5\n",
      "Epoch 276, Average Loss: 0.7445551753044128, Average MAE: 0.8591600060462952, Learning Rate: 0.00256, Current Patience: 6\n",
      "Epoch 277, Average Loss: 0.7446644902229309, Average MAE: 0.8589029312133789, Learning Rate: 0.00256, Current Patience: 7\n",
      "Epoch 278, Average Loss: 0.7445977330207825, Average MAE: 0.8588942289352417, Learning Rate: 0.00256, Current Patience: 8\n",
      "Epoch 279, Average Loss: 0.7445402145385742, Average MAE: 0.8589150309562683, Learning Rate: 0.00256, Current Patience: 9\n",
      "Epoch 280, Average Loss: 0.7445406913757324, Average MAE: 0.8587337136268616, Learning Rate: 0.00256, Current Patience: 10\n",
      "Epoch 281, Average Loss: 0.7446299195289612, Average MAE: 0.8586487174034119, Learning Rate: 0.0020480000000000003, Current Patience: 0\n",
      "Epoch 282, Average Loss: 0.7447472214698792, Average MAE: 0.8589076399803162, Learning Rate: 0.0020480000000000003, Current Patience: 1\n",
      "Epoch 283, Average Loss: 0.7446821928024292, Average MAE: 0.8587649464607239, Learning Rate: 0.0020480000000000003, Current Patience: 2\n",
      "Epoch 284, Average Loss: 0.7444357872009277, Average MAE: 0.8584609627723694, Learning Rate: 0.0020480000000000003, Current Patience: 3\n",
      "Epoch 285, Average Loss: 0.7444404363632202, Average MAE: 0.8584112524986267, Learning Rate: 0.0020480000000000003, Current Patience: 4\n",
      "Epoch 286, Average Loss: 0.7446116209030151, Average MAE: 0.8583347797393799, Learning Rate: 0.0020480000000000003, Current Patience: 5\n",
      "Epoch 287, Average Loss: 0.7445870041847229, Average MAE: 0.858418345451355, Learning Rate: 0.0020480000000000003, Current Patience: 6\n",
      "Epoch 288, Average Loss: 0.744757354259491, Average MAE: 0.8589797616004944, Learning Rate: 0.0020480000000000003, Current Patience: 7\n",
      "Epoch 289, Average Loss: 0.7447229623794556, Average MAE: 0.8582621812820435, Learning Rate: 0.0020480000000000003, Current Patience: 8\n",
      "Epoch 290, Average Loss: 0.7446843385696411, Average MAE: 0.8584974408149719, Learning Rate: 0.0020480000000000003, Current Patience: 9\n",
      "Epoch 291, Average Loss: 0.7446278929710388, Average MAE: 0.85926353931427, Learning Rate: 0.0020480000000000003, Current Patience: 10\n",
      "Epoch 292, Average Loss: 0.7446207404136658, Average MAE: 0.8583388924598694, Learning Rate: 0.0016384000000000004, Current Patience: 0\n",
      "Epoch 293, Average Loss: 0.744445264339447, Average MAE: 0.8578479290008545, Learning Rate: 0.0016384000000000004, Current Patience: 1\n",
      "Epoch 294, Average Loss: 0.7444363832473755, Average MAE: 0.8580684065818787, Learning Rate: 0.0016384000000000004, Current Patience: 2\n",
      "Epoch 295, Average Loss: 0.7446982860565186, Average MAE: 0.8589172959327698, Learning Rate: 0.0016384000000000004, Current Patience: 3\n",
      "Epoch 296, Average Loss: 0.7443563938140869, Average MAE: 0.8580623269081116, Learning Rate: 0.0016384000000000004, Current Patience: 4\n",
      "Epoch 297, Average Loss: 0.7444429397583008, Average MAE: 0.8575987219810486, Learning Rate: 0.0016384000000000004, Current Patience: 5\n",
      "Epoch 298, Average Loss: 0.7446184158325195, Average MAE: 0.8578240871429443, Learning Rate: 0.0016384000000000004, Current Patience: 6\n",
      "Epoch 299, Average Loss: 0.7444719672203064, Average MAE: 0.8584071397781372, Learning Rate: 0.0016384000000000004, Current Patience: 7\n",
      "Epoch 300, Average Loss: 0.7445505857467651, Average MAE: 0.85780930519104, Learning Rate: 0.0016384000000000004, Current Patience: 8\n",
      "Epoch 301, Average Loss: 0.7443826794624329, Average MAE: 0.8576318025588989, Learning Rate: 0.0016384000000000004, Current Patience: 9\n",
      "Epoch 302, Average Loss: 0.744682788848877, Average MAE: 0.8581033945083618, Learning Rate: 0.0016384000000000004, Current Patience: 10\n",
      "Epoch 303, Average Loss: 0.7444086670875549, Average MAE: 0.8575263619422913, Learning Rate: 0.0013107200000000005, Current Patience: 0\n",
      "Epoch 304, Average Loss: 0.7445212602615356, Average MAE: 0.8579016327857971, Learning Rate: 0.0013107200000000005, Current Patience: 1\n",
      "Epoch 305, Average Loss: 0.7444251179695129, Average MAE: 0.8580276966094971, Learning Rate: 0.0013107200000000005, Current Patience: 2\n",
      "Epoch 306, Average Loss: 0.7444489002227783, Average MAE: 0.8577783703804016, Learning Rate: 0.0013107200000000005, Current Patience: 3\n",
      "Epoch 307, Average Loss: 0.7444984316825867, Average MAE: 0.8574836254119873, Learning Rate: 0.0013107200000000005, Current Patience: 4\n",
      "Epoch 308, Average Loss: 0.744479775428772, Average MAE: 0.8576022386550903, Learning Rate: 0.0013107200000000005, Current Patience: 5\n",
      "Epoch 309, Average Loss: 0.7445487976074219, Average MAE: 0.8575521111488342, Learning Rate: 0.0013107200000000005, Current Patience: 6\n",
      "Epoch 310, Average Loss: 0.744348406791687, Average MAE: 0.8575416803359985, Learning Rate: 0.0013107200000000005, Current Patience: 7\n",
      "Epoch 311, Average Loss: 0.7442237734794617, Average MAE: 0.8571292161941528, Learning Rate: 0.0013107200000000005, Current Patience: 0\n",
      "Epoch 312, Average Loss: 0.7443923950195312, Average MAE: 0.8570445775985718, Learning Rate: 0.0013107200000000005, Current Patience: 1\n",
      "Epoch 313, Average Loss: 0.7445290088653564, Average MAE: 0.8578795790672302, Learning Rate: 0.0013107200000000005, Current Patience: 2\n",
      "Epoch 314, Average Loss: 0.7444475293159485, Average MAE: 0.8574612140655518, Learning Rate: 0.0013107200000000005, Current Patience: 3\n",
      "Epoch 315, Average Loss: 0.7444550395011902, Average MAE: 0.8574351668357849, Learning Rate: 0.0013107200000000005, Current Patience: 4\n",
      "Epoch 316, Average Loss: 0.7442759275436401, Average MAE: 0.8573524355888367, Learning Rate: 0.0013107200000000005, Current Patience: 5\n",
      "Epoch 317, Average Loss: 0.7446079850196838, Average MAE: 0.8577464818954468, Learning Rate: 0.0013107200000000005, Current Patience: 6\n",
      "Epoch 318, Average Loss: 0.7442188262939453, Average MAE: 0.8569516539573669, Learning Rate: 0.0013107200000000005, Current Patience: 7\n",
      "Epoch 319, Average Loss: 0.7444359064102173, Average MAE: 0.8572261333465576, Learning Rate: 0.0013107200000000005, Current Patience: 8\n",
      "Epoch 320, Average Loss: 0.7444522380828857, Average MAE: 0.8570868372917175, Learning Rate: 0.0013107200000000005, Current Patience: 9\n",
      "Epoch 321, Average Loss: 0.7444357872009277, Average MAE: 0.8574769496917725, Learning Rate: 0.0013107200000000005, Current Patience: 10\n",
      "Epoch 322, Average Loss: 0.7442771196365356, Average MAE: 0.8572579026222229, Learning Rate: 0.0010485760000000005, Current Patience: 0\n",
      "Epoch 323, Average Loss: 0.7443453669548035, Average MAE: 0.8567932844161987, Learning Rate: 0.0010485760000000005, Current Patience: 1\n",
      "Epoch 324, Average Loss: 0.7443995475769043, Average MAE: 0.856989860534668, Learning Rate: 0.0010485760000000005, Current Patience: 2\n",
      "Epoch 325, Average Loss: 0.744358241558075, Average MAE: 0.8576443195343018, Learning Rate: 0.0010485760000000005, Current Patience: 3\n",
      "Epoch 326, Average Loss: 0.7445734143257141, Average MAE: 0.8585101962089539, Learning Rate: 0.0010485760000000005, Current Patience: 4\n",
      "Epoch 327, Average Loss: 0.7443839311599731, Average MAE: 0.8572821617126465, Learning Rate: 0.0010485760000000005, Current Patience: 5\n",
      "Epoch 328, Average Loss: 0.7442018985748291, Average MAE: 0.8565278649330139, Learning Rate: 0.0010485760000000005, Current Patience: 6\n",
      "Epoch 329, Average Loss: 0.7465540766716003, Average MAE: 0.8633707165718079, Learning Rate: 0.0010485760000000005, Current Patience: 7\n",
      "Epoch 330, Average Loss: 0.7457208633422852, Average MAE: 0.8654625415802002, Learning Rate: 0.0010485760000000005, Current Patience: 8\n",
      "Epoch 331, Average Loss: 0.7455930709838867, Average MAE: 0.8610090017318726, Learning Rate: 0.0010485760000000005, Current Patience: 9\n",
      "Epoch 332, Average Loss: 0.7448244690895081, Average MAE: 0.8607425093650818, Learning Rate: 0.0010485760000000005, Current Patience: 10\n",
      "Epoch 333, Average Loss: 0.7449706792831421, Average MAE: 0.8666554093360901, Learning Rate: 0.0008388608000000005, Current Patience: 0\n",
      "Epoch 334, Average Loss: 0.744883120059967, Average MAE: 0.8633951544761658, Learning Rate: 0.0008388608000000005, Current Patience: 1\n",
      "Epoch 335, Average Loss: 0.7451074719429016, Average MAE: 0.8595799207687378, Learning Rate: 0.0008388608000000005, Current Patience: 2\n",
      "Epoch 336, Average Loss: 0.7447901368141174, Average MAE: 0.8593206405639648, Learning Rate: 0.0008388608000000005, Current Patience: 3\n",
      "Epoch 337, Average Loss: 0.7445641160011292, Average MAE: 0.8597553372383118, Learning Rate: 0.0008388608000000005, Current Patience: 4\n",
      "Epoch 338, Average Loss: 0.7448177933692932, Average MAE: 0.859431803226471, Learning Rate: 0.0008388608000000005, Current Patience: 5\n",
      "Epoch 339, Average Loss: 0.7447611689567566, Average MAE: 0.8595870733261108, Learning Rate: 0.0008388608000000005, Current Patience: 6\n",
      "Epoch 340, Average Loss: 0.7450259327888489, Average MAE: 0.861044704914093, Learning Rate: 0.0008388608000000005, Current Patience: 7\n",
      "Epoch 341, Average Loss: 0.7446556687355042, Average MAE: 0.8607139587402344, Learning Rate: 0.0008388608000000005, Current Patience: 8\n",
      "Epoch 342, Average Loss: 0.7445073127746582, Average MAE: 0.8590254187583923, Learning Rate: 0.0008388608000000005, Current Patience: 9\n",
      "Epoch 343, Average Loss: 0.7446975708007812, Average MAE: 0.8581937551498413, Learning Rate: 0.0008388608000000005, Current Patience: 10\n",
      "Epoch 344, Average Loss: 0.7448785901069641, Average MAE: 0.8587469458580017, Learning Rate: 0.0006710886400000004, Current Patience: 0\n",
      "Epoch 345, Average Loss: 0.7443843483924866, Average MAE: 0.858600378036499, Learning Rate: 0.0006710886400000004, Current Patience: 1\n",
      "Epoch 346, Average Loss: 0.7446451783180237, Average MAE: 0.8585944771766663, Learning Rate: 0.0006710886400000004, Current Patience: 2\n",
      "Epoch 347, Average Loss: 0.7445662617683411, Average MAE: 0.8582705855369568, Learning Rate: 0.0006710886400000004, Current Patience: 3\n",
      "Epoch 348, Average Loss: 0.7444024682044983, Average MAE: 0.8584636449813843, Learning Rate: 0.0006710886400000004, Current Patience: 4\n",
      "Epoch 349, Average Loss: 0.7445895075798035, Average MAE: 0.858705997467041, Learning Rate: 0.0006710886400000004, Current Patience: 5\n",
      "Epoch 350, Average Loss: 0.7448444962501526, Average MAE: 0.8585956692695618, Learning Rate: 0.0006710886400000004, Current Patience: 6\n",
      "Epoch 351, Average Loss: 0.7447523474693298, Average MAE: 0.8579942584037781, Learning Rate: 0.0006710886400000004, Current Patience: 7\n",
      "Epoch 352, Average Loss: 0.7446193099021912, Average MAE: 0.8578097224235535, Learning Rate: 0.0006710886400000004, Current Patience: 8\n",
      "Epoch 353, Average Loss: 0.7443546056747437, Average MAE: 0.8576420545578003, Learning Rate: 0.0006710886400000004, Current Patience: 9\n",
      "Epoch 354, Average Loss: 0.744581401348114, Average MAE: 0.8577672839164734, Learning Rate: 0.0006710886400000004, Current Patience: 10\n",
      "Epoch 355, Average Loss: 0.7445210218429565, Average MAE: 0.8577075004577637, Learning Rate: 0.0005368709120000003, Current Patience: 0\n",
      "Epoch 356, Average Loss: 0.744594931602478, Average MAE: 0.8579542636871338, Learning Rate: 0.0005368709120000003, Current Patience: 1\n",
      "Epoch 357, Average Loss: 0.7445376515388489, Average MAE: 0.8578584790229797, Learning Rate: 0.0005368709120000003, Current Patience: 2\n",
      "Epoch 358, Average Loss: 0.7444782257080078, Average MAE: 0.8576717376708984, Learning Rate: 0.0005368709120000003, Current Patience: 3\n",
      "Epoch 359, Average Loss: 0.7447320222854614, Average MAE: 0.85773766040802, Learning Rate: 0.0005368709120000003, Current Patience: 4\n",
      "Epoch 360, Average Loss: 0.7444536089897156, Average MAE: 0.8574305176734924, Learning Rate: 0.0005368709120000003, Current Patience: 5\n",
      "Epoch 361, Average Loss: 0.7445985078811646, Average MAE: 0.8574520945549011, Learning Rate: 0.0005368709120000003, Current Patience: 6\n",
      "Epoch 362, Average Loss: 0.7446542382240295, Average MAE: 0.8573942184448242, Learning Rate: 0.0005368709120000003, Current Patience: 7\n",
      "Epoch 363, Average Loss: 0.7445151209831238, Average MAE: 0.8572595119476318, Learning Rate: 0.0005368709120000003, Current Patience: 8\n",
      "Epoch 364, Average Loss: 0.7446148991584778, Average MAE: 0.8573671579360962, Learning Rate: 0.0005368709120000003, Current Patience: 9\n",
      "Epoch 365, Average Loss: 0.7442798018455505, Average MAE: 0.8570454120635986, Learning Rate: 0.0005368709120000003, Current Patience: 10\n",
      "Epoch 366, Average Loss: 0.7443603873252869, Average MAE: 0.8573106527328491, Learning Rate: 0.0004294967296000003, Current Patience: 0\n",
      "Epoch 367, Average Loss: 0.7447441220283508, Average MAE: 0.8578171133995056, Learning Rate: 0.0004294967296000003, Current Patience: 1\n",
      "Epoch 368, Average Loss: 0.7443981766700745, Average MAE: 0.8572903275489807, Learning Rate: 0.0004294967296000003, Current Patience: 2\n",
      "Epoch 369, Average Loss: 0.7444645166397095, Average MAE: 0.8572702407836914, Learning Rate: 0.0004294967296000003, Current Patience: 3\n",
      "Epoch 370, Average Loss: 0.7445148825645447, Average MAE: 0.8572250008583069, Learning Rate: 0.0004294967296000003, Current Patience: 4\n",
      "Epoch 371, Average Loss: 0.744323194026947, Average MAE: 0.8569685816764832, Learning Rate: 0.0004294967296000003, Current Patience: 5\n",
      "Epoch 372, Average Loss: 0.7442721724510193, Average MAE: 0.8568228483200073, Learning Rate: 0.0004294967296000003, Current Patience: 6\n",
      "Epoch 373, Average Loss: 0.7443958520889282, Average MAE: 0.8569608926773071, Learning Rate: 0.0004294967296000003, Current Patience: 7\n",
      "Epoch 374, Average Loss: 0.7443211078643799, Average MAE: 0.8569765090942383, Learning Rate: 0.0004294967296000003, Current Patience: 8\n",
      "Epoch 375, Average Loss: 0.7445302605628967, Average MAE: 0.8571863174438477, Learning Rate: 0.0004294967296000003, Current Patience: 9\n",
      "Epoch 376, Average Loss: 0.7445911169052124, Average MAE: 0.8572693467140198, Learning Rate: 0.0004294967296000003, Current Patience: 10\n",
      "Epoch 377, Average Loss: 0.7444151639938354, Average MAE: 0.8571919202804565, Learning Rate: 0.00034359738368000027, Current Patience: 0\n",
      "Epoch 378, Average Loss: 0.7445311546325684, Average MAE: 0.8572120070457458, Learning Rate: 0.00034359738368000027, Current Patience: 1\n",
      "Epoch 379, Average Loss: 0.744570791721344, Average MAE: 0.8571481704711914, Learning Rate: 0.00034359738368000027, Current Patience: 2\n",
      "Epoch 380, Average Loss: 0.7446157932281494, Average MAE: 0.8571988344192505, Learning Rate: 0.00034359738368000027, Current Patience: 3\n",
      "Epoch 381, Average Loss: 0.7444904446601868, Average MAE: 0.8569983839988708, Learning Rate: 0.00034359738368000027, Current Patience: 4\n",
      "Epoch 382, Average Loss: 0.7445899248123169, Average MAE: 0.8571602702140808, Learning Rate: 0.00034359738368000027, Current Patience: 5\n",
      "Epoch 383, Average Loss: 0.7443862557411194, Average MAE: 0.8569149374961853, Learning Rate: 0.00034359738368000027, Current Patience: 6\n",
      "Epoch 384, Average Loss: 0.7446393966674805, Average MAE: 0.8572115898132324, Learning Rate: 0.00034359738368000027, Current Patience: 7\n",
      "Epoch 385, Average Loss: 0.7445405125617981, Average MAE: 0.8571507334709167, Learning Rate: 0.00034359738368000027, Current Patience: 8\n",
      "Epoch 386, Average Loss: 0.744547963142395, Average MAE: 0.8571292757987976, Learning Rate: 0.00034359738368000027, Current Patience: 9\n",
      "Epoch 387, Average Loss: 0.744591474533081, Average MAE: 0.8571146130561829, Learning Rate: 0.00034359738368000027, Current Patience: 10\n",
      "Epoch 388, Average Loss: 0.744369626045227, Average MAE: 0.8568856716156006, Learning Rate: 0.00027487790694400024, Current Patience: 0\n",
      "Epoch 389, Average Loss: 0.7443267107009888, Average MAE: 0.8568886518478394, Learning Rate: 0.00027487790694400024, Current Patience: 1\n",
      "Epoch 390, Average Loss: 0.7446718811988831, Average MAE: 0.8571290373802185, Learning Rate: 0.00027487790694400024, Current Patience: 2\n",
      "Epoch 391, Average Loss: 0.7445187568664551, Average MAE: 0.8570052981376648, Learning Rate: 0.00027487790694400024, Current Patience: 3\n",
      "Epoch 392, Average Loss: 0.7445382475852966, Average MAE: 0.8570775985717773, Learning Rate: 0.00027487790694400024, Current Patience: 4\n",
      "Epoch 393, Average Loss: 0.7444562911987305, Average MAE: 0.856941282749176, Learning Rate: 0.00027487790694400024, Current Patience: 5\n",
      "Epoch 394, Average Loss: 0.7443609237670898, Average MAE: 0.8568233251571655, Learning Rate: 0.00027487790694400024, Current Patience: 6\n",
      "Epoch 395, Average Loss: 0.7446832060813904, Average MAE: 0.8571943044662476, Learning Rate: 0.00027487790694400024, Current Patience: 7\n",
      "Epoch 396, Average Loss: 0.7444610595703125, Average MAE: 0.8569198846817017, Learning Rate: 0.00027487790694400024, Current Patience: 8\n",
      "Epoch 397, Average Loss: 0.7447800040245056, Average MAE: 0.8572713732719421, Learning Rate: 0.00027487790694400024, Current Patience: 9\n",
      "Epoch 398, Average Loss: 0.7445469498634338, Average MAE: 0.8570150136947632, Learning Rate: 0.00027487790694400024, Current Patience: 10\n",
      "Epoch 399, Average Loss: 0.7443825602531433, Average MAE: 0.8567932844161987, Learning Rate: 0.0002199023255552002, Current Patience: 0\n",
      "Epoch 400, Average Loss: 0.7446069121360779, Average MAE: 0.8570712208747864, Learning Rate: 0.0002199023255552002, Current Patience: 1\n",
      "Epoch 401, Average Loss: 0.7443535327911377, Average MAE: 0.8567165732383728, Learning Rate: 0.0002199023255552002, Current Patience: 2\n",
      "Epoch 402, Average Loss: 0.7444750666618347, Average MAE: 0.8568964600563049, Learning Rate: 0.0002199023255552002, Current Patience: 3\n",
      "Epoch 403, Average Loss: 0.7443829774856567, Average MAE: 0.8567705154418945, Learning Rate: 0.0002199023255552002, Current Patience: 4\n",
      "Epoch 404, Average Loss: 0.7444629073143005, Average MAE: 0.8568779826164246, Learning Rate: 0.0002199023255552002, Current Patience: 5\n",
      "Epoch 405, Average Loss: 0.7444790601730347, Average MAE: 0.8568090796470642, Learning Rate: 0.0002199023255552002, Current Patience: 6\n",
      "Epoch 406, Average Loss: 0.7443612217903137, Average MAE: 0.8567582964897156, Learning Rate: 0.0002199023255552002, Current Patience: 7\n",
      "Epoch 407, Average Loss: 0.7446187734603882, Average MAE: 0.8570733666419983, Learning Rate: 0.0002199023255552002, Current Patience: 8\n",
      "Epoch 408, Average Loss: 0.7443356513977051, Average MAE: 0.8567235469818115, Learning Rate: 0.0002199023255552002, Current Patience: 9\n",
      "Epoch 409, Average Loss: 0.7443644404411316, Average MAE: 0.8568403720855713, Learning Rate: 0.0002199023255552002, Current Patience: 10\n",
      "Epoch 410, Average Loss: 0.7443332076072693, Average MAE: 0.8567790985107422, Learning Rate: 0.00017592186044416018, Current Patience: 0\n",
      "Epoch 411, Average Loss: 0.7446840405464172, Average MAE: 0.8570776581764221, Learning Rate: 0.00017592186044416018, Current Patience: 1\n",
      "Epoch 412, Average Loss: 0.7444057464599609, Average MAE: 0.8568471670150757, Learning Rate: 0.00017592186044416018, Current Patience: 2\n",
      "Epoch 413, Average Loss: 0.7445213794708252, Average MAE: 0.8569384813308716, Learning Rate: 0.00017592186044416018, Current Patience: 3\n",
      "Epoch 414, Average Loss: 0.744496762752533, Average MAE: 0.856887936592102, Learning Rate: 0.00017592186044416018, Current Patience: 4\n",
      "Epoch 415, Average Loss: 0.7446816563606262, Average MAE: 0.857096791267395, Learning Rate: 0.00017592186044416018, Current Patience: 5\n",
      "Epoch 416, Average Loss: 0.7444865107536316, Average MAE: 0.8568484783172607, Learning Rate: 0.00017592186044416018, Current Patience: 6\n",
      "Epoch 417, Average Loss: 0.7444508075714111, Average MAE: 0.8567777872085571, Learning Rate: 0.00017592186044416018, Current Patience: 7\n",
      "Epoch 418, Average Loss: 0.7443275451660156, Average MAE: 0.8567510843276978, Learning Rate: 0.00017592186044416018, Current Patience: 8\n",
      "Epoch 419, Average Loss: 0.7442080974578857, Average MAE: 0.8566126823425293, Learning Rate: 0.00017592186044416018, Current Patience: 9\n",
      "Epoch 420, Average Loss: 0.7445951700210571, Average MAE: 0.8570117354393005, Learning Rate: 0.00017592186044416018, Current Patience: 10\n",
      "Epoch 421, Average Loss: 0.7443574070930481, Average MAE: 0.8567501902580261, Learning Rate: 0.00014073748835532815, Current Patience: 0\n",
      "Epoch 422, Average Loss: 0.7442331910133362, Average MAE: 0.8565587997436523, Learning Rate: 0.00014073748835532815, Current Patience: 1\n",
      "Epoch 423, Average Loss: 0.744545042514801, Average MAE: 0.8569403290748596, Learning Rate: 0.00014073748835532815, Current Patience: 2\n",
      "Epoch 424, Average Loss: 0.744290828704834, Average MAE: 0.8565986156463623, Learning Rate: 0.00014073748835532815, Current Patience: 3\n",
      "Epoch 425, Average Loss: 0.7443576455116272, Average MAE: 0.8566970825195312, Learning Rate: 0.00014073748835532815, Current Patience: 4\n",
      "Epoch 426, Average Loss: 0.7442888617515564, Average MAE: 0.8566299080848694, Learning Rate: 0.00014073748835532815, Current Patience: 5\n",
      "Epoch 427, Average Loss: 0.7443751096725464, Average MAE: 0.8567391633987427, Learning Rate: 0.00014073748835532815, Current Patience: 6\n",
      "Epoch 428, Average Loss: 0.7443169355392456, Average MAE: 0.8566489815711975, Learning Rate: 0.00014073748835532815, Current Patience: 7\n",
      "Epoch 429, Average Loss: 0.7446379661560059, Average MAE: 0.8569322228431702, Learning Rate: 0.00014073748835532815, Current Patience: 8\n",
      "Epoch 430, Average Loss: 0.7445375323295593, Average MAE: 0.8568584322929382, Learning Rate: 0.00014073748835532815, Current Patience: 9\n",
      "Epoch 431, Average Loss: 0.7444788813591003, Average MAE: 0.8568017482757568, Learning Rate: 0.00014073748835532815, Current Patience: 10\n",
      "Epoch 432, Average Loss: 0.7443832159042358, Average MAE: 0.8566733002662659, Learning Rate: 0.00011258999068426252, Current Patience: 0\n",
      "Epoch 433, Average Loss: 0.7442790269851685, Average MAE: 0.8566017746925354, Learning Rate: 0.00011258999068426252, Current Patience: 1\n",
      "Epoch 434, Average Loss: 0.7445070147514343, Average MAE: 0.8568373918533325, Learning Rate: 0.00011258999068426252, Current Patience: 2\n",
      "Epoch 435, Average Loss: 0.7442725896835327, Average MAE: 0.8565807938575745, Learning Rate: 0.00011258999068426252, Current Patience: 3\n",
      "Epoch 436, Average Loss: 0.7445988655090332, Average MAE: 0.8569343686103821, Learning Rate: 0.00011258999068426252, Current Patience: 4\n",
      "Epoch 437, Average Loss: 0.7445830702781677, Average MAE: 0.8568949699401855, Learning Rate: 0.00011258999068426252, Current Patience: 5\n",
      "Epoch 438, Average Loss: 0.7447338700294495, Average MAE: 0.8570764660835266, Learning Rate: 0.00011258999068426252, Current Patience: 6\n",
      "Epoch 439, Average Loss: 0.7443895936012268, Average MAE: 0.8567658066749573, Learning Rate: 0.00011258999068426252, Current Patience: 7\n",
      "Epoch 440, Average Loss: 0.7442587018013, Average MAE: 0.8566074967384338, Learning Rate: 0.00011258999068426252, Current Patience: 8\n",
      "Epoch 441, Average Loss: 0.7441503405570984, Average MAE: 0.8564556241035461, Learning Rate: 0.00011258999068426252, Current Patience: 9\n",
      "Epoch 442, Average Loss: 0.7444098591804504, Average MAE: 0.8567498922348022, Learning Rate: 0.00011258999068426252, Current Patience: 10\n",
      "Epoch 443, Average Loss: 0.7444918155670166, Average MAE: 0.8567566871643066, Learning Rate: 9.007199254741002e-05, Current Patience: 0\n",
      "Epoch 444, Average Loss: 0.7443612217903137, Average MAE: 0.8566682934761047, Learning Rate: 9.007199254741002e-05, Current Patience: 1\n",
      "Epoch 445, Average Loss: 0.7445064783096313, Average MAE: 0.856809139251709, Learning Rate: 9.007199254741002e-05, Current Patience: 2\n",
      "Epoch 446, Average Loss: 0.7443983554840088, Average MAE: 0.8567430973052979, Learning Rate: 9.007199254741002e-05, Current Patience: 3\n",
      "Epoch 447, Average Loss: 0.744317352771759, Average MAE: 0.8565735816955566, Learning Rate: 9.007199254741002e-05, Current Patience: 4\n",
      "Epoch 448, Average Loss: 0.7446809411048889, Average MAE: 0.8570248484611511, Learning Rate: 9.007199254741002e-05, Current Patience: 5\n",
      "Epoch 449, Average Loss: 0.744324803352356, Average MAE: 0.8565767407417297, Learning Rate: 9.007199254741002e-05, Current Patience: 6\n",
      "Epoch 450, Average Loss: 0.7444665431976318, Average MAE: 0.8567694425582886, Learning Rate: 9.007199254741002e-05, Current Patience: 7\n",
      "Epoch 451, Average Loss: 0.7441207766532898, Average MAE: 0.8563516736030579, Learning Rate: 9.007199254741002e-05, Current Patience: 0\n",
      "Epoch 452, Average Loss: 0.7443182468414307, Average MAE: 0.8565990328788757, Learning Rate: 9.007199254741002e-05, Current Patience: 1\n",
      "Epoch 453, Average Loss: 0.7445589900016785, Average MAE: 0.8568723201751709, Learning Rate: 9.007199254741002e-05, Current Patience: 2\n",
      "Epoch 454, Average Loss: 0.7444431185722351, Average MAE: 0.8567643165588379, Learning Rate: 9.007199254741002e-05, Current Patience: 3\n",
      "Epoch 455, Average Loss: 0.7443877458572388, Average MAE: 0.8566688895225525, Learning Rate: 9.007199254741002e-05, Current Patience: 4\n",
      "Epoch 456, Average Loss: 0.7442816495895386, Average MAE: 0.8565277457237244, Learning Rate: 9.007199254741002e-05, Current Patience: 5\n",
      "Epoch 457, Average Loss: 0.7441986799240112, Average MAE: 0.8564633131027222, Learning Rate: 9.007199254741002e-05, Current Patience: 6\n",
      "Epoch 458, Average Loss: 0.74431312084198, Average MAE: 0.8565765023231506, Learning Rate: 9.007199254741002e-05, Current Patience: 7\n",
      "Epoch 459, Average Loss: 0.7443845272064209, Average MAE: 0.8567011952400208, Learning Rate: 9.007199254741002e-05, Current Patience: 8\n",
      "Epoch 460, Average Loss: 0.7443991899490356, Average MAE: 0.8566318154335022, Learning Rate: 9.007199254741002e-05, Current Patience: 9\n",
      "Epoch 461, Average Loss: 0.74443519115448, Average MAE: 0.8566826581954956, Learning Rate: 9.007199254741002e-05, Current Patience: 10\n",
      "Epoch 462, Average Loss: 0.7444427609443665, Average MAE: 0.8567000031471252, Learning Rate: 7.205759403792802e-05, Current Patience: 0\n",
      "Epoch 463, Average Loss: 0.7445006966590881, Average MAE: 0.8567252159118652, Learning Rate: 7.205759403792802e-05, Current Patience: 1\n",
      "Epoch 464, Average Loss: 0.7445617318153381, Average MAE: 0.8569068908691406, Learning Rate: 7.205759403792802e-05, Current Patience: 2\n",
      "Epoch 465, Average Loss: 0.7443830966949463, Average MAE: 0.8565849661827087, Learning Rate: 7.205759403792802e-05, Current Patience: 3\n",
      "Epoch 466, Average Loss: 0.7444661259651184, Average MAE: 0.8567686080932617, Learning Rate: 7.205759403792802e-05, Current Patience: 4\n",
      "Epoch 467, Average Loss: 0.7445257902145386, Average MAE: 0.8568535447120667, Learning Rate: 7.205759403792802e-05, Current Patience: 5\n",
      "Epoch 468, Average Loss: 0.7445462942123413, Average MAE: 0.8568581938743591, Learning Rate: 7.205759403792802e-05, Current Patience: 6\n",
      "Epoch 469, Average Loss: 0.7441943883895874, Average MAE: 0.8564088940620422, Learning Rate: 7.205759403792802e-05, Current Patience: 7\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epoch_losses = []\n",
    "epoch_maes = []\n",
    "\n",
    "# Set the model to training mode\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    total_mae = 0\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for b in range(num_batches_train):\n",
    "        # Get the batched node features and desired output\n",
    "        node_features_batch = batched_input_train[b].to(device)\n",
    "        desired_output_batch = batched_output_train[b].to(device)\n",
    "        \n",
    "        # Reshape node_features_batch to match the expected input shape for the model\n",
    "        node_features_batch = node_features_batch.view(-1, node_features_batch.size(2)).to(device)\n",
    "        \n",
    "        model_output_batch = model(node_features_batch, edge_index, edge_attr)\n",
    "        \n",
    "        # Reshape model_output_batch back to the original shape\n",
    "        model_output_batch = model_output_batch.view(batched_input_train.size(1), -1, model_output_batch.size(1))\n",
    "        loss = loss_fn(model_output_batch, desired_output_batch)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the optimizer\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute MAE for debugging\n",
    "        mae = torch.mean(torch.abs(model_output_batch - desired_output_batch))\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_mae += mae.item()\n",
    "\n",
    "    average_loss = total_loss / num_batches_train\n",
    "    average_mae = total_mae / num_batches_train\n",
    "    epoch_losses.append(average_loss)\n",
    "    epoch_maes.append(average_mae)\n",
    "\n",
    "    scheduler.step(average_loss)\n",
    "\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    current_patience = scheduler.num_bad_epochs\n",
    "    print(f\"Epoch {epoch+1}, Average Loss: {average_loss}, Average MAE: {average_mae}, Learning Rate: {current_lr}, Current Patience: {current_patience}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">**ZE EPIC GRAPHS**</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the epoch losses and MAEs\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(epoch_losses, label='Loss')\n",
    "plt.plot(epoch_maes, label='MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Training Loss and MAE over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
