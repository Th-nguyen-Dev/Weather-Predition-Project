{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/HDD/RyanFolder/projects/Weather-Predition-Project/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv, GraphConv\n",
    "from torch_geometric.nn.norm import BatchNorm\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "import numpy as np\n",
    "from torch.optim.swa_utils import AveragedModel, SWALR\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">**ZE EPIC DATA**</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([72790024141, 72785524114, 72789094197, 72793024233, 72785794129, 72788594266, 72797624217, 72785024157, 72797094240, 72798594276, 72792424223, 72792894263, 72781024243, 72781524237, 72788324220, 72698824219, 72793894274, 74206024207, 72782724110, 72793724222, 72792594227, 72782594239, 72794504205, 72792394225, 72784524163, 72792024227, 72785694176])\n"
     ]
    }
   ],
   "source": [
    "# Directory containing the CSV files\n",
    "directory = '../processed-final-data-2'\n",
    "\n",
    "# Dictionary to store the dataframes\n",
    "dataframes = {}\n",
    "\n",
    "# Iterate over all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        # Extract the file name without extension and convert it to int\n",
    "        key = int(os.path.splitext(filename)[0])\n",
    "        \n",
    "        # Read the CSV file into a dataframe\n",
    "        df = pd.read_csv(os.path.join(directory, filename))\n",
    "        dataframes[key] = df\n",
    "\n",
    "\n",
    "# Print the dictionary keys to verify\n",
    "print(dataframes.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72790024141: Sequential split verified.\n",
      "72785524114: Sequential split verified.\n",
      "72789094197: Sequential split verified.\n",
      "72793024233: Sequential split verified.\n",
      "72785794129: Sequential split verified.\n",
      "72788594266: Sequential split verified.\n",
      "72797624217: Sequential split verified.\n",
      "72785024157: Sequential split verified.\n",
      "72797094240: Sequential split verified.\n",
      "72798594276: Sequential split verified.\n",
      "72792424223: Sequential split verified.\n",
      "72792894263: Sequential split verified.\n",
      "72781024243: Sequential split verified.\n",
      "72781524237: Sequential split verified.\n",
      "72788324220: Sequential split verified.\n",
      "72698824219: Sequential split verified.\n",
      "72793894274: Sequential split verified.\n",
      "74206024207: Sequential split verified.\n",
      "72782724110: Sequential split verified.\n",
      "72793724222: Sequential split verified.\n",
      "72792594227: Sequential split verified.\n",
      "72782594239: Sequential split verified.\n",
      "72794504205: Sequential split verified.\n",
      "72792394225: Sequential split verified.\n",
      "72784524163: Sequential split verified.\n",
      "72792024227: Sequential split verified.\n",
      "72785694176: Sequential split verified.\n",
      "dict_keys([72790024141, 72785524114, 72789094197, 72793024233, 72785794129, 72788594266, 72797624217, 72785024157, 72797094240, 72798594276, 72792424223, 72792894263, 72781024243, 72781524237, 72788324220, 72698824219, 72793894274, 74206024207, 72782724110, 72793724222, 72792594227, 72782594239, 72794504205, 72792394225, 72784524163, 72792024227, 72785694176])\n",
      "dict_keys([72790024141, 72785524114, 72789094197, 72793024233, 72785794129, 72788594266, 72797624217, 72785024157, 72797094240, 72798594276, 72792424223, 72792894263, 72781024243, 72781524237, 72788324220, 72698824219, 72793894274, 74206024207, 72782724110, 72793724222, 72792594227, 72782594239, 72794504205, 72792394225, 72784524163, 72792024227, 72785694176])\n"
     ]
    }
   ],
   "source": [
    "# Dictionaries to store the training and testing dataframes\n",
    "train_dataframes = {}\n",
    "test_dataframes = {}\n",
    "\n",
    "# Split each dataframe into training and testing sets\n",
    "for key, df in dataframes.items():\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, shuffle=False)\n",
    "    train_dataframes[key] = train_df\n",
    "    test_dataframes[key] = test_df\n",
    "    # Check if the maximum index of the training set is less than the minimum index of the testing set\n",
    "    if train_df.index.max() < test_df.index.min():\n",
    "        print(f\"{key}: Sequential split verified.\")\n",
    "    else:\n",
    "        print(f\"{key}: Sequential split NOT verified.\")\n",
    "\n",
    "# Print the keys of the training and testing dictionaries to verify\n",
    "print(train_dataframes.keys())\n",
    "print(test_dataframes.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([525, 27, 59])\n",
      "torch.Size([131, 27, 59])\n"
     ]
    }
   ],
   "source": [
    "def create_node_features_sequences(dataframes):\n",
    "    # Create a list to store the node features for each time step for input and desired output\n",
    "    node_features_sequence_input = []\n",
    "    node_features_sequence_output = []\n",
    "\n",
    "    # Iterate over the rows of the dataframes (assuming all dataframes have the same number of rows)\n",
    "    for i in range(len(next(iter(dataframes.values())))):\n",
    "        if i == len(next(iter(dataframes.values()))) - 1:\n",
    "            break\n",
    "        # Create a list to store the features of all nodes at the current time step for input\n",
    "        node_features_input = []\n",
    "        # Create a list to store the features of all nodes at the next time step for output\n",
    "        node_features_output = []\n",
    "\n",
    "        # Iterate over each dataframe and extract the features at the current row for input\n",
    "        # and the next row for output\n",
    "        for key, df in dataframes.items():\n",
    "            node_features_input.append((df.iloc[i].values - df.iloc[i].mean()) / df.iloc[i].std())\n",
    "            node_features_output.append(df.iloc[i + 1].values)\n",
    "\n",
    "        # Stack the features of all nodes to create a 2D array (num_nodes, num_features)\n",
    "        node_features_sequence_input.append(np.stack(node_features_input))\n",
    "        node_features_sequence_output.append(np.stack(node_features_output))\n",
    "\n",
    "    # Convert the lists to numpy arrays (time_steps, num_nodes, num_features)\n",
    "    node_features_sequence_input = np.array(node_features_sequence_input)\n",
    "    node_features_sequence_output = np.array(node_features_sequence_output)\n",
    "\n",
    "    # Convert the numpy arrays to torch tensors\n",
    "    node_features_sequence_input = torch.tensor(node_features_sequence_input, dtype=torch.float)\n",
    "    node_features_sequence_output = torch.tensor(node_features_sequence_output, dtype=torch.float)\n",
    "\n",
    "    return node_features_sequence_input, node_features_sequence_output\n",
    "\n",
    "# Call the function and print the shapes of the resulting tensors\n",
    "node_features_sequence_input_train, node_features_sequence_output_train = create_node_features_sequences(train_dataframes)\n",
    "node_features_sequence_input_test, node_features_sequence_output_test = create_node_features_sequences(test_dataframes)\n",
    "print(node_features_sequence_input_train.shape)\n",
    "print(node_features_sequence_output_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: tensor([[[-0.3513, -0.2447, -0.3517,  0.0810,  4.4760,  1.6202,  2.2340,\n",
      "           2.6113,  2.9742, -0.0595,  1.6904,  1.6897, -0.3517, -0.3467,\n",
      "          -0.3515, -0.3516, -0.3516, -0.3514, -0.3494, -0.3177, -0.3505,\n",
      "          -0.3467, -0.3517, -0.3466, -0.3516, -0.3510, -0.3514, -0.3517,\n",
      "          -0.3517, -0.3515, -0.3515, -0.3502, -0.3516, -0.3516, -0.3465,\n",
      "          -0.3514, -0.3516, -0.3516, -0.3517, -0.3163, -0.3511, -0.3512,\n",
      "          -0.3515, -0.3148, -0.3511, -0.3516, -0.3466, -0.3514, -0.3516,\n",
      "          -0.3512, -0.3365, -0.3515, -0.3514, -0.3513, -0.3513, -0.3516,\n",
      "          -0.3515, -0.3515, -0.3511]]])\n",
      "Standard Deviation: tensor([[[0.0163, 0.1418, 0.0161, 0.2685, 1.1921, 0.2288, 0.4864, 0.5617,\n",
      "          0.8244, 0.1007, 0.2498, 0.2488, 0.0161, 0.0229, 0.0165, 0.0163,\n",
      "          0.0163, 0.0165, 0.0216, 0.0314, 0.0179, 0.0229, 0.0161, 0.0277,\n",
      "          0.0163, 0.0172, 0.0166, 0.0162, 0.0161, 0.0165, 0.0164, 0.0197,\n",
      "          0.0163, 0.0162, 0.0279, 0.0166, 0.0163, 0.0162, 0.0161, 0.0311,\n",
      "          0.0171, 0.0172, 0.0163, 0.0306, 0.0170, 0.0163, 0.0230, 0.0166,\n",
      "          0.0162, 0.0169, 0.0368, 0.0167, 0.0165, 0.0172, 0.0174, 0.0163,\n",
      "          0.0164, 0.0168, 0.0170]]])\n"
     ]
    }
   ],
   "source": [
    "# Calculate mean and standard deviation of the training data\n",
    "mean = node_features_sequence_input_train.mean(dim=(0, 1), keepdim=True)\n",
    "std = node_features_sequence_input_train.std(dim=(0, 1), keepdim=True)\n",
    "\n",
    "# Normalize the training and testing data\n",
    "# node_features_sequence_input_train = (node_features_sequence_input_train - mean) / std\n",
    "# node_features_sequence_input_test = (node_features_sequence_input_test - mean) / std\n",
    "\n",
    "# Print the mean and standard deviation to verify\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Standard Deviation:\", std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">**ZE EPIC EDGE DATA**</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        STATION  LONGITUDE  LATITUDE  ELEVATION\n",
      "0  7.279372e+10 -122.28308  47.92322      167.1\n",
      "1  7.278462e+10 -118.28572  46.09456      356.7\n",
      "2  7.420712e+10 -122.58333  47.08333       91.4\n",
      "3  7.279762e+10 -122.54069  48.79910       45.9\n",
      "4  7.279239e+10 -123.93074  46.97288        4.5\n"
     ]
    }
   ],
   "source": [
    "# Import the location-datamap.csv file as a dataframe\n",
    "location_datamap_df = pd.read_csv('../location-datamap.csv')\n",
    "\n",
    "# Print the first few rows of the dataframe to verify\n",
    "print(location_datamap_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_distance(lat1, lon1, lat2, lon2, el1=0, el2=0):\n",
    "    # Radius of the Earth in kilometers\n",
    "    R = 6371.0\n",
    "\n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "\n",
    "    # Difference in coordinates\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "\n",
    "    # Haversine formula\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "\n",
    "    # Distance in kilometers\n",
    "    distance = R * c\n",
    "\n",
    "    # Elevation difference\n",
    "    height = el2 - el1\n",
    "\n",
    "    # Calculate the total distance considering elevation\n",
    "    total_distance = sqrt(distance**2 + height**2)\n",
    "\n",
    "    return total_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 1): 395.4679457314314, (0, 2): 129.15783117310457, (0, 3): 342.5330553210191, (0, 4): 438.10430559873856, (0, 5): 432.0504921333856, (0, 6): 437.11321302131137, (0, 7): 369.0087247919059, (0, 8): 503.7031148557305, (0, 9): 455.6811328123654, (0, 10): 474.59654599988056, (0, 11): 348.0950737502581, (0, 12): 128.6980383337319, (0, 13): 835.9738346074198, (0, 14): 171.59537540199554, (0, 15): 384.56540064951065, (0, 16): 373.58764051749137, (0, 17): 362.1843030623904, (0, 18): 32.53657446575969, (0, 19): 306.4908073667999, (0, 20): 404.50991220772727, (0, 21): 52.81240627430448, (0, 22): 404.8085057270694, (0, 23): 505.4288197147213, (0, 24): 286.0836462058196, (0, 25): 412.6227567328569, (0, 26): 272.19719043646785, (1, 2): 390.14029117618804, (1, 3): 727.6917541369755, (1, 4): 109.85904921372176, (1, 5): 798.995937782049, (1, 6): 802.4581375032445, (1, 7): 33.690956835057186, (1, 8): 865.1737430163511, (1, 9): 825.6499844120128, (1, 10): 860.6728795779842, (1, 11): 724.9963923066945, (1, 12): 495.92270395996894, (1, 13): 535.6266022869553, (1, 14): 314.54161595295784, (1, 15): 763.6141112163369, (1, 16): 759.3841400612968, (1, 17): 748.5486591666476, (1, 18): 416.12981588895354, (1, 19): 678.7815012654146, (1, 20): 784.7770299622911, (1, 21): 419.79175595055284, (1, 22): 786.18443118649, (1, 23): 886.2658783050533, (1, 24): 655.5895444605051, (1, 25): 798.318691116012, (1, 26): 157.5431429141368, (2, 3): 370.42190175405364, (2, 4): 460.5219705003162, (2, 5): 432.51979230044606, (2, 6): 417.3828379309705, (2, 7): 365.04041085309774, (2, 8): 508.89691489063597, (2, 9): 445.736559884783, (2, 10): 534.5520542530354, (2, 11): 372.6745336577991, (2, 12): 236.9285970548754, (2, 13): 831.0617187024466, (2, 14): 221.69557651346355, (2, 15): 470.9577296531585, (2, 16): 406.24280451088293, (2, 17): 399.71114545506987, (2, 18): 147.68778122386612, (2, 19): 313.8982068438734, (2, 20): 435.3656912848407, (2, 21): 130.20708355028395, (2, 22): 406.9699486305622, (2, 23): 539.0132078425393, (2, 24): 368.91523621650714, (2, 25): 452.3832910640423, (2, 26): 270.2281521689557, (3, 4): 775.0588941479449, (3, 5): 120.26611512741678, (3, 6): 165.52848353942377, (3, 7): 704.0879836514051, (3, 8): 185.28412975619443, (3, 9): 152.43166309928537, (3, 10): 187.27787071696568, (3, 11): 41.159846544112625, (3, 12): 266.9618608270944, (3, 13): 1096.8318771247875, (3, 14): 443.4038293955239, (3, 15): 225.02216925780087, (3, 16): 36.85266364881154, (3, 17): 37.9895765777111, (3, 18): 333.6917238582668, (3, 19): 76.27723241526142, (3, 20): 72.18185980856553, (3, 21): 309.0749255571238, (3, 22): 107.09860018045799, (3, 23): 171.23674671831446, (3, 24): 276.0731537992283, (3, 25): 85.99186556469607, (3, 26): 611.0889227645558, (4, 5): 856.701667766572, (4, 6): 865.4792367557994, (4, 7): 117.74480456011335, (4, 8): 921.4842980890041, (4, 9): 885.9031028312606, (4, 10): 890.5893542480043, (4, 11): 774.366157782777, (4, 12): 524.6337306289196, (4, 13): 540.0809528724242, (4, 14): 357.13279391570444, (4, 15): 780.4774458332262, (4, 16): 804.444912957628, (4, 17): 791.9148221844617, (4, 18): 454.1201925958629, (4, 19): 734.2694292819892, (4, 20): 830.6418814848106, (4, 21): 467.37603414879794, (4, 22): 840.9110561664348, (4, 23): 929.3740741921883, (4, 24): 673.3913688173589, (4, 25): 839.7968409183139, (4, 26): 209.78499150005845, (5, 6): 110.15242913456197, (5, 7): 777.4585937545535, (5, 8): 85.13925057078639, (5, 9): 74.97130998219406, (5, 10): 240.43942234567194, (5, 11): 103.9757703201679, (5, 12): 369.69695959787833, (5, 13): 1139.0521959187856, (5, 14): 516.0490346912795, (5, 15): 330.5216959336131, (5, 16): 117.91278236100351, (5, 17): 133.34207595548997, (5, 18): 428.7153028579556, (5, 19): 125.7282441840991, (5, 20): 101.97987733596389, (5, 21): 392.60154297249403, (5, 22): 108.31645122446824, (5, 23): 153.40108385187867, (5, 24): 392.1898886399527, (5, 25): 137.25092367847324, (5, 26): 690.174750212649, (6, 7): 778.6874797868297, (6, 8): 177.46961023734613, (6, 9): 48.71149879169117, (6, 10): 302.0846527899822, (6, 11): 172.27646695285324, (6, 12): 399.97805818217563, (6, 13): 1176.4852153080844, (6, 14): 545.6427005439592, (6, 15): 369.3977887607593, (6, 16): 175.55537172802033, (6, 17): 190.70446482220075, (6, 18): 431.096161984872, (6, 19): 156.64330089805705, (6, 20): 183.24573042058552, (6, 21): 405.4909815288361, (6, 22): 76.4859126118275, (6, 23): 231.7098185991757, (6, 24): 388.4359336032824, (6, 25): 205.33116784821942, (6, 26): 682.4260680955291, (7, 8): 845.1366146343045, (7, 9): 802.4776134962176, (7, 10): 837.0612376685274, (7, 11): 702.8044715545802, (7, 12): 472.23598917705993, (7, 13): 567.9966156275789, (7, 14): 299.672336109386, (7, 15): 738.0303386253106, (7, 16): 735.9632694864449, (7, 17): 725.1092500944276, (7, 18): 388.53428759844127, (7, 19): 656.1420174916371, (7, 20): 762.4710980022056, (7, 21): 395.81370216605853, (7, 22): 761.3237576215986, (7, 23): 864.2082908552868, (7, 24): 626.0309963149288, (7, 25): 775.1913077729471, (7, 26): 124.27275180830249, (8, 9): 132.5097212349861, (8, 10): 243.7073144688935, (8, 11): 163.47287592761103, (8, 12): 430.6615227573649, (8, 13): 1177.64872286148, (8, 14): 572.9402144031785, (8, 15): 365.1202761234219, (8, 16): 169.20923499165121, (8, 17): 183.39281681071296, (8, 18): 501.4584011725272, (8, 19): 202.41419212127153, (8, 20): 134.47678138943274, (8, 21): 461.61393826175714, (8, 22): 180.6112148305096, (8, 23): 128.08708542091654, (8, 24): 456.1447880245876, (8, 25): 164.101942102309, (8, 26): 763.0321197403028, (9, 10): 268.89465547374556, (9, 11): 154.53742133091865, (9, 12): 405.60664848136616, (9, 13): 1187.3176896736784, (9, 14): 557.1020923327553, (9, 15): 353.92282978101207, (9, 16): 153.1723115228679, (9, 17): 170.09372061722416, (9, 18): 449.65266636740307, (9, 19): 158.4233084920311, (9, 20): 151.2637681156068, (9, 21): 421.5198568724629, (9, 22): 75.9354339763056, (9, 23): 187.20830867381414, (9, 24): 396.00851322484175, (9, 25): 174.23493854899442, (9, 26): 708.5304039928845, (10, 11): 199.69966656538062, (10, 12): 366.6429361022303, (10, 13): 1213.3303112543872, (10, 14): 565.7742092205691, (10, 15): 158.75602862100996, (10, 16): 154.05814985745886, (10, 17): 150.49523188191975, (10, 18): 459.96707604415764, (10, 19): 261.56576121699044, (10, 20): 148.37325199067052, (10, 21): 447.531376407682, (10, 22): 236.6757463314917, (10, 23): 123.91187323342717, (10, 24): 313.3574949442864, (10, 25): 109.93432994721853, (10, 26): 745.579780595948, (11, 12): 271.20003239227077, (11, 13): 1077.0374760399814, (11, 14): 434.420916611982, (11, 15): 249.12813853110475, (11, 16): 54.60340565924095, (11, 17): 56.69074125354086, (11, 18): 342.83747887822733, (11, 19): 68.6532776654429, (11, 20): 64.41876337172579, (11, 21): 309.79708423419913, (11, 22): 128.34321457181807, (11, 23): 167.4998627149666, (11, 24): 308.66359444651886, (11, 25): 94.10375597750865, (11, 26): 614.9675457923169, (12, 13): 891.3237029023409, (12, 14): 217.85555378560997, (12, 15): 275.33106494313824, (12, 16): 290.27386357894756, (12, 17): 275.46064771106325, (12, 18): 121.40670159689395, (12, 19): 252.86361822595595, (12, 20): 317.66456605180196, (12, 21): 111.45482222908821, (12, 22): 352.74223587837065, (12, 23): 411.2440393694844, (12, 24): 228.97116863460516, (12, 25): 319.85603194172165, (12, 26): 386.6621826263452, (13, 14): 677.4867035602812, (13, 15): 1150.4386824494006, (13, 16): 1122.20080799066, (13, 17): 1112.0640517140919, (13, 18): 864.0517998527835, (13, 19): 1044.4930844703567, (13, 20): 1130.7362905229766, (13, 21): 833.7147080759753, (13, 22): 1170.6533446424978, (13, 23): 1218.574885845403, (13, 24): 1103.3829323700534, (13, 25): 1152.414996794818, (13, 26): 683.9489018806388, (14, 15): 489.8134161401565, (14, 16): 470.9676802550459, (14, 17): 459.0274176052015, (14, 18): 199.59384465449898, (14, 19): 400.7840441174383, (14, 20): 490.3741382897129, (14, 21): 162.11769623669784, (14, 22): 520.9373250801362, (14, 23): 587.5949150283049, (14, 24): 432.7632336033511, (14, 25): 504.8208347435879, (14, 26): 259.92185050812566, (15, 16): 213.47199631168857, (15, 17): 199.7906856396986, (15, 18): 363.3873739871944, (15, 19): 286.49766075950384, (15, 20): 235.6189290359573, (15, 21): 371.98475050597, (15, 22): 293.97835633481196, (15, 23): 268.7494270590419, (15, 24): 181.34059793217904, (15, 25): 201.61183866011444, (15, 26): 642.4950004501114, (16, 17): 17.716247962574563, (16, 18): 363.9821600522806, (16, 19): 109.51765254130393, (16, 20): 42.988933127483634, (16, 21): 340.3237239231559, (16, 22): 114.05670021657697, (16, 23): 136.43815241862657, (16, 24): 287.7450730575042, (16, 25): 49.41202473657626, (16, 26): 643.6682517559431, (17, 18): 352.2158347034709, (17, 19): 111.26188855442645, (17, 20): 52.561875466910436, (17, 21): 329.3368670529716, (17, 22): 128.01154758942099, (17, 23): 145.49531716211806, (17, 24): 275.6435250697721, (17, 25): 52.80087090274467, (17, 26): 633.1197418989807, (18, 19): 303.31516235822005, (18, 20): 396.9755284116139, (18, 21): 73.74026824660453, (18, 22): 394.74559894207687, (18, 23): 496.478460291939, (18, 24): 256.1592053257525, (18, 25): 402.1731282608459, (18, 26): 286.80868520838226, (19, 20): 129.67576043022495, (19, 21): 268.31498195846285, (19, 22): 128.38153131965652, (19, 23): 230.11767302669344, (19, 24): 305.30416194483115, (19, 25): 156.88223261226412, (19, 26): 566.3246868369048, (20, 21): 368.04423285388, (20, 22): 133.10933796080906, (20, 23): 103.88553424589651, (20, 24): 327.04970158433304, (20, 25): 41.59043713208587, (20, 26): 674.0342619771884, (21, 22): 375.88513957717953, (21, 23): 469.86938496204897, (21, 24): 296.22180486691445, (21, 25): 379.7335338263519, (21, 26): 308.0303254221726, (22, 23): 191.05105434406818, (22, 24): 321.52952520777865, (22, 25): 144.61575947894278, (22, 26): 662.014427725483, (23, 24): 393.67944122771735, (23, 25): 96.11729883125135, (23, 26): 776.1835969743106, (24, 25): 305.7627599001508, (24, 26): 516.3645983655035, (25, 26): 684.1746652775099}\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary to store the distances between nodes\n",
    "distances = {}\n",
    "\n",
    "# Define the number of nodes\n",
    "num_nodes = len(dataframes)\n",
    "\n",
    "# Iterate over each pair of nodes\n",
    "for i, j in itertools.combinations(range(num_nodes), 2):\n",
    "    # Get the station IDs for the nodes\n",
    "    station_i = list(dataframes.keys())[i]\n",
    "    station_j = list(dataframes.keys())[j]\n",
    "    \n",
    "    # Get the location data for the stations\n",
    "    location_i = location_datamap_df[location_datamap_df['STATION'] == station_i].iloc[0]\n",
    "    location_j = location_datamap_df[location_datamap_df['STATION'] == station_j].iloc[0]\n",
    "    \n",
    "    # Calculate the distance between the stations\n",
    "    distance = haversine_distance(location_i['LATITUDE'], location_i['LONGITUDE'], location_j['LATITUDE'], location_j['LONGITUDE'], location_i['ELEVATION'], location_j['ELEVATION'])\n",
    "    \n",
    "    # Store the distance in the dictionary\n",
    "    distances[(i, j)] = distance\n",
    "\n",
    "# Print the distances to verify\n",
    "print(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create edge index and edge attributes from distances dictionary\n",
    "edge_index = []\n",
    "edge_attr = []\n",
    "\n",
    "for (i, j), distance in distances.items():\n",
    "    edge_index.append([i, j])\n",
    "    edge_index.append([j, i])  # Assuming undirected graph\n",
    "    edge_attr.append([distance])\n",
    "    edge_attr.append([distance])  # Assuming undirected graph\n",
    "\n",
    "edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "edge_attr = torch.tensor(edge_attr, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">**ZE EPIC GPU IMPORT**</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move data to device\n",
    "node_features_sequence_input_train = node_features_sequence_input_train.to(device)\n",
    "node_features_sequence_output_test = node_features_sequence_output_test.to(device)\n",
    "edge_index = edge_index.to(device)\n",
    "edge_attr = edge_attr.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">**ZE EPIC BATCHING**</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 525\n",
      "Number of full batches: 525\n"
     ]
    }
   ],
   "source": [
    "batch_size = node_features_sequence_input_train.shape[0] # Adjust this value based on your GPU memory capacity\n",
    "# Calculate the number of full batches\n",
    "num_full_batches_train = (node_features_sequence_input_train.size(0) // batch_size) * batch_size\n",
    "\n",
    "# Trim the input and output tensors to have a size divisible by the batch size\n",
    "trimmed_input_train = node_features_sequence_input_train[:num_full_batches_train]\n",
    "trimmed_output_train = node_features_sequence_output_train[:num_full_batches_train]\n",
    "\n",
    "# Create batches of data\n",
    "batched_input_train = trimmed_input_train.view(-1, batch_size, trimmed_input_train.size(1), trimmed_input_train.size(2))\n",
    "batched_output_train = trimmed_output_train.view(-1, batch_size, trimmed_output_train.size(1), trimmed_output_train.size(2))\n",
    "\n",
    "# Adjust the number of batches\n",
    "num_batches_train = batched_input_train.size(0)\n",
    "\n",
    "# Print the batch size and number of full batches to verify\n",
    "print(\"Batch size:\", batch_size)\n",
    "print(\"Number of full batches:\", num_full_batches_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 131\n",
      "Number of full batches: 525\n"
     ]
    }
   ],
   "source": [
    "batch_size = node_features_sequence_input_test.shape[0]  # Adjust this value based on your GPU memory capacity\n",
    "# Calculate the number of full batches\n",
    "num_full_batches_test = (node_features_sequence_input_test.size(0) // batch_size) * batch_size\n",
    "\n",
    "# Trim the input and output tensors to have a size divisible by the batch size\n",
    "trimmed_input_test = node_features_sequence_input_test[:num_full_batches_test]\n",
    "trimmed_output_test = node_features_sequence_output_test[:num_full_batches_test]\n",
    "\n",
    "# Create batches of data\n",
    "batched_input_test = trimmed_input_test.view(-1, batch_size, trimmed_input_test.size(1), trimmed_input_test.size(2))\n",
    "batched_output_test = trimmed_output_test.view(-1, batch_size, trimmed_output_test.size(1), trimmed_output_test.size(2))\n",
    "\n",
    "# Adjust the number of batches\n",
    "num_batches_test = batched_input_test.size(0)\n",
    "print(\"Batch size:\", batch_size)\n",
    "print(\"Number of full batches:\", num_full_batches_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">**ZE EPIC MODEL**</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNWithTransformer(torch.nn.Module):\n",
    "    def __init__(self, in_channels, edge_in_channels, hidden_channels, out_channels, num_transformer_layers):\n",
    "        super(GNNWithTransformer, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GATConv(hidden_channels, hidden_channels, heads=4, concat=False)  # GAT layer\n",
    "        self.conv3 = GraphConv(hidden_channels, hidden_channels)\n",
    "        self.batch_norm1 = BatchNorm(hidden_channels)\n",
    "        self.batch_norm2 = BatchNorm(hidden_channels)\n",
    "        self.batch_norm3 = BatchNorm(hidden_channels)\n",
    "        self.dropout = torch.nn.Dropout(p=0.5)\n",
    "        \n",
    "        # Transformer encoder layer\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=hidden_channels, nhead=16),\n",
    "            num_layers=num_transformer_layers\n",
    "        )\n",
    "        \n",
    "        self.output_layer = torch.nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # Compute inverse distance weights\n",
    "        edge_weights = 1.0 / (edge_attr + 1e-6)  # Adding a small value to avoid division by zero\n",
    "        \n",
    "        # Apply the first GCN convolution with edge weights\n",
    "        x = self.conv1(x, edge_index, edge_weight=edge_weights)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Apply the second GAT convolution with edge weights\n",
    "        x = self.conv2(x, edge_index, edge_attr=edge_weights)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Apply the third Graph convolution with edge weights\n",
    "        x = self.conv3(x, edge_index, edge_weight=edge_weights)\n",
    "        x = self.batch_norm3(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Prepare data for Transformer\n",
    "        x = x.unsqueeze(0)  # Add batch dimension\n",
    "        \n",
    "        # Apply Transformer encoder\n",
    "        x = self.transformer_encoder(x)\n",
    "        \n",
    "        x = x.squeeze(0)  # Remove batch dimension\n",
    "        \n",
    "        # Apply the output layer\n",
    "        x = self.output_layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">**ZE EPIC MODEL SETUP**</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input channels: 59\n",
      "Number of output channels: 59\n",
      "Number of hidden channels: 256\n",
      "Number of transformer layers: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/HDD/RyanFolder/projects/Weather-Predition-Project/.venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "in_channels = node_features_sequence_input_train.shape[2]\n",
    "out_channels = in_channels  \n",
    "\n",
    "edge_in_channels = 1\n",
    "hidden_channels = 256\n",
    "\n",
    "num_transformer_layers = 8\n",
    "print(\"Number of input channels:\", in_channels)\n",
    "print(\"Number of output channels:\", out_channels)\n",
    "print(\"Number of hidden channels:\", hidden_channels)\n",
    "print(\"Number of transformer layers:\", num_transformer_layers)\n",
    "\n",
    "model = GNNWithTransformer(in_channels, edge_in_channels, hidden_channels, out_channels, num_transformer_layers).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of epochs: 1000\n",
      "Learning rate: 0.5\n",
      "Scheduler mode: min\n",
      "Scheduler factor: 0.8\n",
      "Scheduler patience: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/HDD/RyanFolder/projects/Weather-Predition-Project/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters\n",
    "num_epochs = 1000  # Adjust the number of epochs as needed\n",
    "learning_rate = 0.5\n",
    "scheduler_mode = 'min'\n",
    "scheduler_factor = 0.5\n",
    "scheduler_patience = 5\n",
    "# num_warmpup_steps = 10 \n",
    "# num_training_steps = num_epochs * num_batches_train\n",
    "\n",
    "#Print all parameter \n",
    "print(\"Number of epochs:\", num_epochs)\n",
    "print(\"Learning rate:\", learning_rate)\n",
    "print(\"Scheduler mode:\", scheduler_mode)\n",
    "print(\"Scheduler factor:\", scheduler_factor)\n",
    "print(\"Scheduler patience:\", scheduler_patience)\n",
    "# print(\"Number of warmup steps:\", num_warmpup_steps)\n",
    "# print(\"Number of training steps:\", num_training_steps)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "swa_model = AveragedModel(model)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=scheduler_mode, factor=scheduler_factor, patience=scheduler_patience, verbose=True)\n",
    "swa_scheduler = SWALR(optimizer, swa_lr=0.05)\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">**ZE EPIC MODEL TRAIN**</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 248.6512451171875, Average MAE: 5.599358081817627, Learning Rate: 0.5\n",
      "Epoch 2, Average Loss: 1457.7384033203125, Average MAE: 34.20182418823242, Learning Rate: 0.5\n",
      "Epoch 3, Average Loss: 27896.666015625, Average MAE: 164.5419464111328, Learning Rate: 0.5\n",
      "Epoch 4, Average Loss: 1105.972412109375, Average MAE: 30.61089324951172, Learning Rate: 0.5\n",
      "Epoch 5, Average Loss: 2103.6279296875, Average MAE: 44.71921157836914, Learning Rate: 0.5\n",
      "Epoch 6, Average Loss: 913.46875, Average MAE: 29.012243270874023, Learning Rate: 0.5\n",
      "Epoch 7, Average Loss: 2119.80029296875, Average MAE: 35.38002014160156, Learning Rate: 0.5\n",
      "Epoch 8, Average Loss: 2924.48291015625, Average MAE: 49.33660125732422, Learning Rate: 0.5\n",
      "Epoch 9, Average Loss: 465.01373291015625, Average MAE: 19.010194778442383, Learning Rate: 0.5\n",
      "Epoch 10, Average Loss: 1169.4412841796875, Average MAE: 24.54741668701172, Learning Rate: 0.5\n",
      "Epoch 11, Average Loss: 1851.9539794921875, Average MAE: 32.55799865722656, Learning Rate: 0.5\n",
      "Epoch 12, Average Loss: 411.7330017089844, Average MAE: 11.642536163330078, Learning Rate: 0.4\n",
      "Epoch 13, Average Loss: 1009.3623046875, Average MAE: 31.06146812438965, Learning Rate: 0.4\n",
      "Epoch 14, Average Loss: 1036.739990234375, Average MAE: 31.71560287475586, Learning Rate: 0.4\n",
      "Epoch 15, Average Loss: 143.21644592285156, Average MAE: 10.374799728393555, Learning Rate: 0.4\n",
      "Epoch 16, Average Loss: 379.77783203125, Average MAE: 18.192874908447266, Learning Rate: 0.4\n",
      "Epoch 17, Average Loss: 870.9231567382812, Average MAE: 28.60788917541504, Learning Rate: 0.4\n",
      "Epoch 18, Average Loss: 309.7927551269531, Average MAE: 16.39317512512207, Learning Rate: 0.4\n",
      "Epoch 19, Average Loss: 218.32852172851562, Average MAE: 9.45185375213623, Learning Rate: 0.4\n",
      "Epoch 20, Average Loss: 692.6591186523438, Average MAE: 23.27780532836914, Learning Rate: 0.4\n",
      "Epoch 21, Average Loss: 448.6776428222656, Average MAE: 17.851829528808594, Learning Rate: 0.4\n",
      "Epoch 22, Average Loss: 96.01811981201172, Average MAE: 6.9279303550720215, Learning Rate: 0.4\n",
      "Epoch 23, Average Loss: 385.9904479980469, Average MAE: 18.687088012695312, Learning Rate: 0.4\n",
      "Epoch 24, Average Loss: 384.3332824707031, Average MAE: 18.77969741821289, Learning Rate: 0.4\n",
      "Epoch 25, Average Loss: 49.171775817871094, Average MAE: 5.337699890136719, Learning Rate: 0.4\n",
      "Epoch 26, Average Loss: 185.05905151367188, Average MAE: 12.82491397857666, Learning Rate: 0.4\n",
      "Epoch 27, Average Loss: 322.82916259765625, Average MAE: 17.39487648010254, Learning Rate: 0.4\n",
      "Epoch 28, Average Loss: 105.54693603515625, Average MAE: 9.149066925048828, Learning Rate: 0.4\n",
      "Epoch 29, Average Loss: 112.35655212402344, Average MAE: 7.299955368041992, Learning Rate: 0.4\n",
      "Epoch 30, Average Loss: 271.7428894042969, Average MAE: 14.122541427612305, Learning Rate: 0.4\n",
      "Epoch 31, Average Loss: 127.70232391357422, Average MAE: 8.59226131439209, Learning Rate: 0.4\n",
      "Epoch 32, Average Loss: 53.882728576660156, Average MAE: 5.653443336486816, Learning Rate: 0.4\n",
      "Epoch 33, Average Loss: 173.0912628173828, Average MAE: 12.375893592834473, Learning Rate: 0.4\n",
      "Epoch 34, Average Loss: 119.3432388305664, Average MAE: 9.898900032043457, Learning Rate: 0.4\n",
      "Epoch 35, Average Loss: 30.02130699157715, Average MAE: 3.604121685028076, Learning Rate: 0.4\n",
      "Epoch 36, Average Loss: 115.47331237792969, Average MAE: 9.770593643188477, Learning Rate: 0.4\n",
      "Epoch 37, Average Loss: 119.62611389160156, Average MAE: 10.023244857788086, Learning Rate: 0.4\n",
      "Epoch 38, Average Loss: 36.440528869628906, Average MAE: 4.442342281341553, Learning Rate: 0.4\n",
      "Epoch 39, Average Loss: 80.1272201538086, Average MAE: 6.902937889099121, Learning Rate: 0.4\n",
      "Epoch 40, Average Loss: 105.81236267089844, Average MAE: 8.50255012512207, Learning Rate: 0.4\n",
      "Epoch 41, Average Loss: 41.58340835571289, Average MAE: 4.158545017242432, Learning Rate: 0.4\n",
      "Epoch 42, Average Loss: 53.85591506958008, Average MAE: 5.973029136657715, Learning Rate: 0.4\n",
      "Epoch 43, Average Loss: 88.3318099975586, Average MAE: 8.290884017944336, Learning Rate: 0.4\n",
      "Epoch 44, Average Loss: 43.27509307861328, Average MAE: 4.991771697998047, Learning Rate: 0.4\n",
      "Epoch 45, Average Loss: 37.89022445678711, Average MAE: 4.352890491485596, Learning Rate: 0.4\n",
      "Epoch 46, Average Loss: 67.6598129272461, Average MAE: 6.900299549102783, Learning Rate: 0.32000000000000006\n",
      "Epoch 47, Average Loss: 40.97647476196289, Average MAE: 4.89040994644165, Learning Rate: 0.32000000000000006\n",
      "Epoch 48, Average Loss: 26.801197052001953, Average MAE: 3.4543211460113525, Learning Rate: 0.32000000000000006\n",
      "Epoch 49, Average Loss: 45.1130485534668, Average MAE: 5.047573089599609, Learning Rate: 0.32000000000000006\n",
      "Epoch 50, Average Loss: 46.8795280456543, Average MAE: 5.145044803619385, Learning Rate: 0.32000000000000006\n",
      "Epoch 51, Average Loss: 28.624814987182617, Average MAE: 3.528294563293457, Learning Rate: 0.32000000000000006\n",
      "Epoch 52, Average Loss: 31.83920669555664, Average MAE: 4.038759231567383, Learning Rate: 0.32000000000000006\n",
      "Epoch 53, Average Loss: 42.50804138183594, Average MAE: 5.090434551239014, Learning Rate: 0.32000000000000006\n",
      "Epoch 54, Average Loss: 32.68830490112305, Average MAE: 4.110429286956787, Learning Rate: 0.32000000000000006\n",
      "Epoch 55, Average Loss: 25.674551010131836, Average MAE: 3.287367582321167, Learning Rate: 0.32000000000000006\n",
      "Epoch 56, Average Loss: 35.05418014526367, Average MAE: 4.257753372192383, Learning Rate: 0.32000000000000006\n",
      "Epoch 57, Average Loss: 35.126808166503906, Average MAE: 4.293667793273926, Learning Rate: 0.32000000000000006\n",
      "Epoch 58, Average Loss: 25.634456634521484, Average MAE: 3.337347984313965, Learning Rate: 0.32000000000000006\n",
      "Epoch 59, Average Loss: 28.762731552124023, Average MAE: 3.6347975730895996, Learning Rate: 0.32000000000000006\n",
      "Epoch 60, Average Loss: 33.545631408691406, Average MAE: 4.106766223907471, Learning Rate: 0.32000000000000006\n",
      "Epoch 61, Average Loss: 26.769813537597656, Average MAE: 3.4192216396331787, Learning Rate: 0.32000000000000006\n",
      "Epoch 62, Average Loss: 25.041322708129883, Average MAE: 3.320221424102783, Learning Rate: 0.32000000000000006\n",
      "Epoch 63, Average Loss: 29.968097686767578, Average MAE: 3.9220621585845947, Learning Rate: 0.32000000000000006\n",
      "Epoch 64, Average Loss: 28.04411506652832, Average MAE: 3.6936349868774414, Learning Rate: 0.32000000000000006\n",
      "Epoch 65, Average Loss: 24.310808181762695, Average MAE: 3.197408437728882, Learning Rate: 0.32000000000000006\n",
      "Epoch 66, Average Loss: 26.667551040649414, Average MAE: 3.413642644882202, Learning Rate: 0.32000000000000006\n",
      "Epoch 67, Average Loss: 27.694005966186523, Average MAE: 3.5239531993865967, Learning Rate: 0.32000000000000006\n",
      "Epoch 68, Average Loss: 24.274587631225586, Average MAE: 3.1686367988586426, Learning Rate: 0.32000000000000006\n",
      "Epoch 69, Average Loss: 24.981966018676758, Average MAE: 3.2818450927734375, Learning Rate: 0.32000000000000006\n",
      "Epoch 70, Average Loss: 26.69072914123535, Average MAE: 3.487269401550293, Learning Rate: 0.32000000000000006\n",
      "Epoch 71, Average Loss: 24.527076721191406, Average MAE: 3.226732015609741, Learning Rate: 0.32000000000000006\n",
      "Epoch 72, Average Loss: 23.984169006347656, Average MAE: 3.1417205333709717, Learning Rate: 0.32000000000000006\n",
      "Epoch 73, Average Loss: 25.786012649536133, Average MAE: 3.3578267097473145, Learning Rate: 0.32000000000000006\n",
      "Epoch 74, Average Loss: 24.83883285522461, Average MAE: 3.2523062229156494, Learning Rate: 0.32000000000000006\n",
      "Epoch 75, Average Loss: 23.32694435119629, Average MAE: 3.0791215896606445, Learning Rate: 0.32000000000000006\n",
      "Epoch 76, Average Loss: 24.687509536743164, Average MAE: 3.2397663593292236, Learning Rate: 0.32000000000000006\n",
      "Epoch 77, Average Loss: 24.502016067504883, Average MAE: 3.2210588455200195, Learning Rate: 0.32000000000000006\n",
      "Epoch 78, Average Loss: 23.224809646606445, Average MAE: 3.0620808601379395, Learning Rate: 0.32000000000000006\n",
      "Epoch 79, Average Loss: 23.868989944458008, Average MAE: 3.133054733276367, Learning Rate: 0.32000000000000006\n",
      "Epoch 80, Average Loss: 23.965499877929688, Average MAE: 3.138399362564087, Learning Rate: 0.32000000000000006\n",
      "Epoch 81, Average Loss: 23.191368103027344, Average MAE: 3.052621364593506, Learning Rate: 0.32000000000000006\n",
      "Epoch 82, Average Loss: 23.302051544189453, Average MAE: 3.084082841873169, Learning Rate: 0.32000000000000006\n",
      "Epoch 83, Average Loss: 23.58116912841797, Average MAE: 3.132279396057129, Learning Rate: 0.32000000000000006\n",
      "Epoch 84, Average Loss: 23.092342376708984, Average MAE: 3.056278705596924, Learning Rate: 0.32000000000000006\n",
      "Epoch 85, Average Loss: 22.943897247314453, Average MAE: 3.019444704055786, Learning Rate: 0.32000000000000006\n",
      "Epoch 86, Average Loss: 23.356910705566406, Average MAE: 3.0702803134918213, Learning Rate: 0.32000000000000006\n",
      "Epoch 87, Average Loss: 22.916507720947266, Average MAE: 3.023115634918213, Learning Rate: 0.32000000000000006\n",
      "Epoch 88, Average Loss: 22.662872314453125, Average MAE: 3.003932237625122, Learning Rate: 0.32000000000000006\n",
      "Epoch 89, Average Loss: 23.235292434692383, Average MAE: 3.075610399246216, Learning Rate: 0.32000000000000006\n",
      "Epoch 90, Average Loss: 22.65041160583496, Average MAE: 3.0112295150756836, Learning Rate: 0.32000000000000006\n",
      "Epoch 91, Average Loss: 22.43340492248535, Average MAE: 2.9773240089416504, Learning Rate: 0.32000000000000006\n",
      "Epoch 92, Average Loss: 22.94586753845215, Average MAE: 3.025926113128662, Learning Rate: 0.32000000000000006\n",
      "Epoch 93, Average Loss: 22.51376724243164, Average MAE: 2.984002113342285, Learning Rate: 0.32000000000000006\n",
      "Epoch 94, Average Loss: 22.33881187438965, Average MAE: 2.963801145553589, Learning Rate: 0.32000000000000006\n",
      "Epoch 95, Average Loss: 22.619354248046875, Average MAE: 2.996798515319824, Learning Rate: 0.32000000000000006\n",
      "Epoch 96, Average Loss: 22.40165901184082, Average MAE: 2.974228620529175, Learning Rate: 0.32000000000000006\n",
      "Epoch 97, Average Loss: 22.31844139099121, Average MAE: 2.9559760093688965, Learning Rate: 0.32000000000000006\n",
      "Epoch 98, Average Loss: 22.370847702026367, Average MAE: 2.964029550552368, Learning Rate: 0.32000000000000006\n",
      "Epoch 99, Average Loss: 22.37372398376465, Average MAE: 2.956648349761963, Learning Rate: 0.32000000000000006\n",
      "Epoch 100, Average Loss: 22.12480926513672, Average MAE: 2.932300567626953, Learning Rate: 0.32000000000000006\n",
      "Epoch 101, Average Loss: 22.166202545166016, Average MAE: 2.947085380554199, Learning Rate: 0.32000000000000006\n",
      "Epoch 102, Average Loss: 22.099353790283203, Average MAE: 2.9424006938934326, Learning Rate: 0.32000000000000006\n",
      "Epoch 103, Average Loss: 21.869577407836914, Average MAE: 2.9094815254211426, Learning Rate: 0.32000000000000006\n",
      "Epoch 104, Average Loss: 21.915361404418945, Average MAE: 2.9013476371765137, Learning Rate: 0.32000000000000006\n",
      "Epoch 105, Average Loss: 21.927114486694336, Average MAE: 2.9104039669036865, Learning Rate: 0.32000000000000006\n",
      "Epoch 106, Average Loss: 21.76531410217285, Average MAE: 2.8802828788757324, Learning Rate: 0.32000000000000006\n",
      "Epoch 107, Average Loss: 21.799524307250977, Average MAE: 2.896158218383789, Learning Rate: 0.32000000000000006\n",
      "Epoch 108, Average Loss: 21.816062927246094, Average MAE: 2.9026050567626953, Learning Rate: 0.32000000000000006\n",
      "Epoch 109, Average Loss: 21.778200149536133, Average MAE: 2.887669086456299, Learning Rate: 0.32000000000000006\n",
      "Epoch 110, Average Loss: 21.649883270263672, Average MAE: 2.8762307167053223, Learning Rate: 0.32000000000000006\n",
      "Epoch 111, Average Loss: 21.630413055419922, Average MAE: 2.8668084144592285, Learning Rate: 0.32000000000000006\n",
      "Epoch 112, Average Loss: 21.545650482177734, Average MAE: 2.8626554012298584, Learning Rate: 0.32000000000000006\n",
      "Epoch 113, Average Loss: 21.40696144104004, Average MAE: 2.83986496925354, Learning Rate: 0.32000000000000006\n",
      "Epoch 114, Average Loss: 21.61414337158203, Average MAE: 2.8663625717163086, Learning Rate: 0.32000000000000006\n",
      "Epoch 115, Average Loss: 21.449970245361328, Average MAE: 2.8494818210601807, Learning Rate: 0.32000000000000006\n",
      "Epoch 116, Average Loss: 21.472232818603516, Average MAE: 2.856502056121826, Learning Rate: 0.32000000000000006\n",
      "Epoch 117, Average Loss: 21.274198532104492, Average MAE: 2.8250155448913574, Learning Rate: 0.32000000000000006\n",
      "Epoch 118, Average Loss: 21.528947830200195, Average MAE: 2.863215446472168, Learning Rate: 0.32000000000000006\n",
      "Epoch 119, Average Loss: 21.23884391784668, Average MAE: 2.82719349861145, Learning Rate: 0.32000000000000006\n",
      "Epoch 120, Average Loss: 21.092262268066406, Average MAE: 2.8039913177490234, Learning Rate: 0.32000000000000006\n",
      "Epoch 121, Average Loss: 21.23850440979004, Average MAE: 2.8212172985076904, Learning Rate: 0.32000000000000006\n",
      "Epoch 122, Average Loss: 21.206008911132812, Average MAE: 2.8175177574157715, Learning Rate: 0.32000000000000006\n",
      "Epoch 123, Average Loss: 20.93309783935547, Average MAE: 2.7737061977386475, Learning Rate: 0.32000000000000006\n",
      "Epoch 124, Average Loss: 20.951753616333008, Average MAE: 2.7786412239074707, Learning Rate: 0.32000000000000006\n",
      "Epoch 125, Average Loss: 21.114503860473633, Average MAE: 2.8003170490264893, Learning Rate: 0.32000000000000006\n",
      "Epoch 126, Average Loss: 21.02943992614746, Average MAE: 2.7946460247039795, Learning Rate: 0.32000000000000006\n",
      "Epoch 127, Average Loss: 20.818111419677734, Average MAE: 2.7679877281188965, Learning Rate: 0.32000000000000006\n",
      "Epoch 128, Average Loss: 20.8465518951416, Average MAE: 2.7671942710876465, Learning Rate: 0.32000000000000006\n",
      "Epoch 129, Average Loss: 20.8035831451416, Average MAE: 2.766846179962158, Learning Rate: 0.32000000000000006\n",
      "Epoch 130, Average Loss: 20.99188995361328, Average MAE: 2.7932868003845215, Learning Rate: 0.32000000000000006\n",
      "Epoch 131, Average Loss: 20.765663146972656, Average MAE: 2.756051778793335, Learning Rate: 0.32000000000000006\n",
      "Epoch 132, Average Loss: 20.646488189697266, Average MAE: 2.7403674125671387, Learning Rate: 0.32000000000000006\n",
      "Epoch 133, Average Loss: 20.78476905822754, Average MAE: 2.760065793991089, Learning Rate: 0.32000000000000006\n",
      "Epoch 134, Average Loss: 20.717111587524414, Average MAE: 2.7524003982543945, Learning Rate: 0.32000000000000006\n",
      "Epoch 135, Average Loss: 20.653823852539062, Average MAE: 2.7398715019226074, Learning Rate: 0.32000000000000006\n",
      "Epoch 136, Average Loss: 20.626453399658203, Average MAE: 2.7360477447509766, Learning Rate: 0.32000000000000006\n",
      "Epoch 137, Average Loss: 20.535079956054688, Average MAE: 2.724806308746338, Learning Rate: 0.32000000000000006\n",
      "Epoch 138, Average Loss: 20.54453468322754, Average MAE: 2.727415084838867, Learning Rate: 0.32000000000000006\n",
      "Epoch 139, Average Loss: 20.633182525634766, Average MAE: 2.726792812347412, Learning Rate: 0.32000000000000006\n",
      "Epoch 140, Average Loss: 20.457172393798828, Average MAE: 2.707667589187622, Learning Rate: 0.32000000000000006\n",
      "Epoch 141, Average Loss: 20.515766143798828, Average MAE: 2.7148571014404297, Learning Rate: 0.32000000000000006\n",
      "Epoch 142, Average Loss: 20.440279006958008, Average MAE: 2.7113516330718994, Learning Rate: 0.32000000000000006\n",
      "Epoch 143, Average Loss: 20.462202072143555, Average MAE: 2.7167983055114746, Learning Rate: 0.32000000000000006\n",
      "Epoch 144, Average Loss: 20.231609344482422, Average MAE: 2.683140993118286, Learning Rate: 0.32000000000000006\n",
      "Epoch 145, Average Loss: 20.332185745239258, Average MAE: 2.6985154151916504, Learning Rate: 0.32000000000000006\n",
      "Epoch 146, Average Loss: 20.326335906982422, Average MAE: 2.6912708282470703, Learning Rate: 0.32000000000000006\n",
      "Epoch 147, Average Loss: 20.22966766357422, Average MAE: 2.67523455619812, Learning Rate: 0.32000000000000006\n",
      "Epoch 148, Average Loss: 20.070476531982422, Average MAE: 2.6698360443115234, Learning Rate: 0.32000000000000006\n",
      "Epoch 149, Average Loss: 20.156152725219727, Average MAE: 2.6717116832733154, Learning Rate: 0.32000000000000006\n",
      "Epoch 150, Average Loss: 20.11711883544922, Average MAE: 2.6633968353271484, Learning Rate: 0.32000000000000006\n",
      "Epoch 151, Average Loss: 20.041223526000977, Average MAE: 2.652207136154175, Learning Rate: 0.32000000000000006\n",
      "Epoch 152, Average Loss: 19.989389419555664, Average MAE: 2.6453771591186523, Learning Rate: 0.32000000000000006\n",
      "Epoch 153, Average Loss: 20.067913055419922, Average MAE: 2.6557748317718506, Learning Rate: 0.32000000000000006\n",
      "Epoch 154, Average Loss: 19.951913833618164, Average MAE: 2.6397783756256104, Learning Rate: 0.32000000000000006\n",
      "Epoch 155, Average Loss: 19.831634521484375, Average MAE: 2.623565912246704, Learning Rate: 0.32000000000000006\n",
      "Epoch 156, Average Loss: 19.862812042236328, Average MAE: 2.630950927734375, Learning Rate: 0.32000000000000006\n",
      "Epoch 157, Average Loss: 19.88520622253418, Average MAE: 2.631190776824951, Learning Rate: 0.32000000000000006\n",
      "Epoch 158, Average Loss: 19.825273513793945, Average MAE: 2.6202807426452637, Learning Rate: 0.32000000000000006\n",
      "Epoch 159, Average Loss: 19.74660873413086, Average MAE: 2.6131207942962646, Learning Rate: 0.32000000000000006\n",
      "Epoch 160, Average Loss: 19.760276794433594, Average MAE: 2.616295576095581, Learning Rate: 0.32000000000000006\n",
      "Epoch 161, Average Loss: 19.710927963256836, Average MAE: 2.608538866043091, Learning Rate: 0.32000000000000006\n",
      "Epoch 162, Average Loss: 19.735519409179688, Average MAE: 2.6040403842926025, Learning Rate: 0.32000000000000006\n",
      "Epoch 163, Average Loss: 19.620662689208984, Average MAE: 2.5985865592956543, Learning Rate: 0.32000000000000006\n",
      "Epoch 164, Average Loss: 19.70878791809082, Average MAE: 2.602567195892334, Learning Rate: 0.32000000000000006\n",
      "Epoch 165, Average Loss: 19.589019775390625, Average MAE: 2.5882272720336914, Learning Rate: 0.32000000000000006\n",
      "Epoch 166, Average Loss: 19.43788719177246, Average MAE: 2.5638701915740967, Learning Rate: 0.32000000000000006\n",
      "Epoch 167, Average Loss: 19.437379837036133, Average MAE: 2.5726778507232666, Learning Rate: 0.32000000000000006\n",
      "Epoch 168, Average Loss: 19.490713119506836, Average MAE: 2.5714821815490723, Learning Rate: 0.32000000000000006\n",
      "Epoch 169, Average Loss: 19.436199188232422, Average MAE: 2.5650734901428223, Learning Rate: 0.32000000000000006\n",
      "Epoch 170, Average Loss: 19.33994483947754, Average MAE: 2.5548760890960693, Learning Rate: 0.32000000000000006\n",
      "Epoch 171, Average Loss: 19.48139762878418, Average MAE: 2.5669519901275635, Learning Rate: 0.32000000000000006\n",
      "Epoch 172, Average Loss: 19.320049285888672, Average MAE: 2.5514965057373047, Learning Rate: 0.32000000000000006\n",
      "Epoch 173, Average Loss: 19.334854125976562, Average MAE: 2.5462565422058105, Learning Rate: 0.32000000000000006\n",
      "Epoch 174, Average Loss: 19.310556411743164, Average MAE: 2.545802593231201, Learning Rate: 0.32000000000000006\n",
      "Epoch 175, Average Loss: 19.35219383239746, Average MAE: 2.552760124206543, Learning Rate: 0.32000000000000006\n",
      "Epoch 176, Average Loss: 19.156391143798828, Average MAE: 2.5284414291381836, Learning Rate: 0.32000000000000006\n",
      "Epoch 177, Average Loss: 19.199966430664062, Average MAE: 2.5230441093444824, Learning Rate: 0.32000000000000006\n",
      "Epoch 178, Average Loss: 19.197185516357422, Average MAE: 2.5255982875823975, Learning Rate: 0.32000000000000006\n",
      "Epoch 179, Average Loss: 19.125747680664062, Average MAE: 2.51995849609375, Learning Rate: 0.32000000000000006\n",
      "Epoch 180, Average Loss: 19.083974838256836, Average MAE: 2.5103025436401367, Learning Rate: 0.32000000000000006\n",
      "Epoch 181, Average Loss: 19.106033325195312, Average MAE: 2.5103952884674072, Learning Rate: 0.32000000000000006\n",
      "Epoch 182, Average Loss: 18.995121002197266, Average MAE: 2.4954681396484375, Learning Rate: 0.32000000000000006\n",
      "Epoch 183, Average Loss: 19.05170249938965, Average MAE: 2.5062661170959473, Learning Rate: 0.32000000000000006\n",
      "Epoch 184, Average Loss: 19.013505935668945, Average MAE: 2.5086710453033447, Learning Rate: 0.32000000000000006\n",
      "Epoch 185, Average Loss: 18.78653907775879, Average MAE: 2.471215009689331, Learning Rate: 0.32000000000000006\n",
      "Epoch 186, Average Loss: 18.874441146850586, Average MAE: 2.480891227722168, Learning Rate: 0.32000000000000006\n",
      "Epoch 187, Average Loss: 18.844141006469727, Average MAE: 2.474531412124634, Learning Rate: 0.32000000000000006\n",
      "Epoch 188, Average Loss: 18.813947677612305, Average MAE: 2.469770669937134, Learning Rate: 0.32000000000000006\n",
      "Epoch 189, Average Loss: 18.717573165893555, Average MAE: 2.4512176513671875, Learning Rate: 0.32000000000000006\n",
      "Epoch 190, Average Loss: 18.866317749023438, Average MAE: 2.4786288738250732, Learning Rate: 0.32000000000000006\n",
      "Epoch 191, Average Loss: 18.681249618530273, Average MAE: 2.4508416652679443, Learning Rate: 0.32000000000000006\n",
      "Epoch 192, Average Loss: 18.686363220214844, Average MAE: 2.4519965648651123, Learning Rate: 0.32000000000000006\n",
      "Epoch 193, Average Loss: 18.683475494384766, Average MAE: 2.452500820159912, Learning Rate: 0.32000000000000006\n",
      "Epoch 194, Average Loss: 18.74812889099121, Average MAE: 2.4505484104156494, Learning Rate: 0.32000000000000006\n",
      "Epoch 195, Average Loss: 18.610183715820312, Average MAE: 2.433176040649414, Learning Rate: 0.32000000000000006\n",
      "Epoch 196, Average Loss: 18.643651962280273, Average MAE: 2.441441297531128, Learning Rate: 0.32000000000000006\n",
      "Epoch 197, Average Loss: 18.50908088684082, Average MAE: 2.423431158065796, Learning Rate: 0.32000000000000006\n",
      "Epoch 198, Average Loss: 18.562332153320312, Average MAE: 2.4319064617156982, Learning Rate: 0.32000000000000006\n",
      "Epoch 199, Average Loss: 18.537248611450195, Average MAE: 2.4163358211517334, Learning Rate: 0.32000000000000006\n",
      "Epoch 200, Average Loss: 18.53243064880371, Average MAE: 2.420917510986328, Learning Rate: 0.32000000000000006\n",
      "Epoch 201, Average Loss: 18.46659278869629, Average MAE: 2.4157609939575195, Learning Rate: 0.32000000000000006\n",
      "Epoch 202, Average Loss: 18.389760971069336, Average MAE: 2.39664888381958, Learning Rate: 0.32000000000000006\n",
      "Epoch 203, Average Loss: 18.443988800048828, Average MAE: 2.4049606323242188, Learning Rate: 0.32000000000000006\n",
      "Epoch 204, Average Loss: 18.313129425048828, Average MAE: 2.3906900882720947, Learning Rate: 0.32000000000000006\n",
      "Epoch 205, Average Loss: 18.416187286376953, Average MAE: 2.400571584701538, Learning Rate: 0.32000000000000006\n",
      "Epoch 206, Average Loss: 18.306903839111328, Average MAE: 2.3920562267303467, Learning Rate: 0.32000000000000006\n",
      "Epoch 207, Average Loss: 18.2951717376709, Average MAE: 2.3862807750701904, Learning Rate: 0.32000000000000006\n",
      "Epoch 208, Average Loss: 18.30590057373047, Average MAE: 2.3868932723999023, Learning Rate: 0.32000000000000006\n",
      "Epoch 209, Average Loss: 18.23883819580078, Average MAE: 2.3648715019226074, Learning Rate: 0.32000000000000006\n",
      "Epoch 210, Average Loss: 18.146469116210938, Average MAE: 2.357534170150757, Learning Rate: 0.32000000000000006\n",
      "Epoch 211, Average Loss: 18.264049530029297, Average MAE: 2.3779592514038086, Learning Rate: 0.32000000000000006\n",
      "Epoch 212, Average Loss: 18.12929344177246, Average MAE: 2.347074508666992, Learning Rate: 0.32000000000000006\n",
      "Epoch 213, Average Loss: 18.135839462280273, Average MAE: 2.351668119430542, Learning Rate: 0.32000000000000006\n",
      "Epoch 214, Average Loss: 18.13026237487793, Average MAE: 2.358745574951172, Learning Rate: 0.32000000000000006\n",
      "Epoch 215, Average Loss: 18.075891494750977, Average MAE: 2.349066734313965, Learning Rate: 0.32000000000000006\n",
      "Epoch 216, Average Loss: 18.068090438842773, Average MAE: 2.3480684757232666, Learning Rate: 0.32000000000000006\n",
      "Epoch 217, Average Loss: 17.98235321044922, Average MAE: 2.334418773651123, Learning Rate: 0.32000000000000006\n",
      "Epoch 218, Average Loss: 18.033172607421875, Average MAE: 2.3420166969299316, Learning Rate: 0.32000000000000006\n",
      "Epoch 219, Average Loss: 18.034605026245117, Average MAE: 2.3442130088806152, Learning Rate: 0.32000000000000006\n",
      "Epoch 220, Average Loss: 17.969289779663086, Average MAE: 2.3349854946136475, Learning Rate: 0.32000000000000006\n",
      "Epoch 221, Average Loss: 18.009201049804688, Average MAE: 2.3312313556671143, Learning Rate: 0.32000000000000006\n",
      "Epoch 222, Average Loss: 17.88971519470215, Average MAE: 2.318984270095825, Learning Rate: 0.32000000000000006\n",
      "Epoch 223, Average Loss: 17.971126556396484, Average MAE: 2.3315091133117676, Learning Rate: 0.32000000000000006\n",
      "Epoch 224, Average Loss: 17.919944763183594, Average MAE: 2.313265562057495, Learning Rate: 0.32000000000000006\n",
      "Epoch 225, Average Loss: 17.953859329223633, Average MAE: 2.3181426525115967, Learning Rate: 0.32000000000000006\n",
      "Epoch 226, Average Loss: 17.823753356933594, Average MAE: 2.3053104877471924, Learning Rate: 0.32000000000000006\n",
      "Epoch 227, Average Loss: 17.778621673583984, Average MAE: 2.3005573749542236, Learning Rate: 0.32000000000000006\n",
      "Epoch 228, Average Loss: 17.758651733398438, Average MAE: 2.287040948867798, Learning Rate: 0.32000000000000006\n",
      "Epoch 229, Average Loss: 17.729734420776367, Average MAE: 2.286012649536133, Learning Rate: 0.32000000000000006\n",
      "Epoch 230, Average Loss: 17.7369327545166, Average MAE: 2.293501615524292, Learning Rate: 0.32000000000000006\n",
      "Epoch 231, Average Loss: 17.728557586669922, Average MAE: 2.285780191421509, Learning Rate: 0.32000000000000006\n",
      "Epoch 232, Average Loss: 17.570114135742188, Average MAE: 2.257603406906128, Learning Rate: 0.32000000000000006\n",
      "Epoch 233, Average Loss: 17.695459365844727, Average MAE: 2.2798538208007812, Learning Rate: 0.32000000000000006\n",
      "Epoch 234, Average Loss: 17.667430877685547, Average MAE: 2.2747154235839844, Learning Rate: 0.32000000000000006\n",
      "Epoch 235, Average Loss: 17.589712142944336, Average MAE: 2.2617194652557373, Learning Rate: 0.32000000000000006\n",
      "Epoch 236, Average Loss: 17.709697723388672, Average MAE: 2.2812445163726807, Learning Rate: 0.32000000000000006\n",
      "Epoch 237, Average Loss: 17.55921173095703, Average MAE: 2.253830909729004, Learning Rate: 0.32000000000000006\n",
      "Epoch 238, Average Loss: 17.59877586364746, Average MAE: 2.2609496116638184, Learning Rate: 0.32000000000000006\n",
      "Epoch 239, Average Loss: 17.543577194213867, Average MAE: 2.250833749771118, Learning Rate: 0.32000000000000006\n",
      "Epoch 240, Average Loss: 17.531394958496094, Average MAE: 2.2528939247131348, Learning Rate: 0.32000000000000006\n",
      "Epoch 241, Average Loss: 17.513729095458984, Average MAE: 2.242851495742798, Learning Rate: 0.32000000000000006\n",
      "Epoch 242, Average Loss: 17.439285278320312, Average MAE: 2.2324182987213135, Learning Rate: 0.32000000000000006\n",
      "Epoch 243, Average Loss: 17.46442222595215, Average MAE: 2.2371668815612793, Learning Rate: 0.32000000000000006\n",
      "Epoch 244, Average Loss: 17.416851043701172, Average MAE: 2.2308685779571533, Learning Rate: 0.32000000000000006\n",
      "Epoch 245, Average Loss: 17.388835906982422, Average MAE: 2.2202365398406982, Learning Rate: 0.32000000000000006\n",
      "Epoch 246, Average Loss: 17.354324340820312, Average MAE: 2.215064764022827, Learning Rate: 0.32000000000000006\n",
      "Epoch 247, Average Loss: 17.39374351501465, Average MAE: 2.220182180404663, Learning Rate: 0.32000000000000006\n",
      "Epoch 248, Average Loss: 17.416181564331055, Average MAE: 2.2252960205078125, Learning Rate: 0.32000000000000006\n",
      "Epoch 249, Average Loss: 17.349000930786133, Average MAE: 2.2198593616485596, Learning Rate: 0.32000000000000006\n",
      "Epoch 250, Average Loss: 17.359394073486328, Average MAE: 2.2218821048736572, Learning Rate: 0.32000000000000006\n",
      "Epoch 251, Average Loss: 17.332168579101562, Average MAE: 2.2158639430999756, Learning Rate: 0.32000000000000006\n",
      "Epoch 252, Average Loss: 17.313884735107422, Average MAE: 2.210378885269165, Learning Rate: 0.32000000000000006\n",
      "Epoch 253, Average Loss: 17.202054977416992, Average MAE: 2.1914772987365723, Learning Rate: 0.32000000000000006\n",
      "Epoch 254, Average Loss: 17.2918758392334, Average MAE: 2.2042508125305176, Learning Rate: 0.32000000000000006\n",
      "Epoch 255, Average Loss: 17.252727508544922, Average MAE: 2.1923909187316895, Learning Rate: 0.32000000000000006\n",
      "Epoch 256, Average Loss: 17.227630615234375, Average MAE: 2.1995532512664795, Learning Rate: 0.32000000000000006\n",
      "Epoch 257, Average Loss: 17.14398765563965, Average MAE: 2.1732442378997803, Learning Rate: 0.32000000000000006\n",
      "Epoch 258, Average Loss: 17.168867111206055, Average MAE: 2.188678741455078, Learning Rate: 0.32000000000000006\n",
      "Epoch 259, Average Loss: 17.197153091430664, Average MAE: 2.183427333831787, Learning Rate: 0.32000000000000006\n",
      "Epoch 260, Average Loss: 17.132070541381836, Average MAE: 2.1768271923065186, Learning Rate: 0.32000000000000006\n",
      "Epoch 261, Average Loss: 17.115482330322266, Average MAE: 2.1704084873199463, Learning Rate: 0.32000000000000006\n",
      "Epoch 262, Average Loss: 17.073932647705078, Average MAE: 2.1621739864349365, Learning Rate: 0.32000000000000006\n",
      "Epoch 263, Average Loss: 17.01238250732422, Average MAE: 2.155005693435669, Learning Rate: 0.32000000000000006\n",
      "Epoch 264, Average Loss: 17.100759506225586, Average MAE: 2.1626617908477783, Learning Rate: 0.32000000000000006\n",
      "Epoch 265, Average Loss: 17.021228790283203, Average MAE: 2.1544110774993896, Learning Rate: 0.32000000000000006\n",
      "Epoch 266, Average Loss: 17.019683837890625, Average MAE: 2.15041446685791, Learning Rate: 0.32000000000000006\n",
      "Epoch 267, Average Loss: 16.994346618652344, Average MAE: 2.145709276199341, Learning Rate: 0.32000000000000006\n",
      "Epoch 268, Average Loss: 16.982717514038086, Average MAE: 2.1439626216888428, Learning Rate: 0.32000000000000006\n",
      "Epoch 269, Average Loss: 16.931398391723633, Average MAE: 2.134080410003662, Learning Rate: 0.32000000000000006\n",
      "Epoch 270, Average Loss: 16.996740341186523, Average MAE: 2.1425814628601074, Learning Rate: 0.32000000000000006\n",
      "Epoch 271, Average Loss: 16.938661575317383, Average MAE: 2.1348888874053955, Learning Rate: 0.32000000000000006\n",
      "Epoch 272, Average Loss: 16.85601234436035, Average MAE: 2.1237354278564453, Learning Rate: 0.32000000000000006\n",
      "Epoch 273, Average Loss: 16.899505615234375, Average MAE: 2.122628927230835, Learning Rate: 0.32000000000000006\n",
      "Epoch 274, Average Loss: 16.91518783569336, Average MAE: 2.129335641860962, Learning Rate: 0.32000000000000006\n",
      "Epoch 275, Average Loss: 16.802227020263672, Average MAE: 2.1139791011810303, Learning Rate: 0.32000000000000006\n",
      "Epoch 276, Average Loss: 16.869775772094727, Average MAE: 2.1163933277130127, Learning Rate: 0.32000000000000006\n",
      "Epoch 277, Average Loss: 16.864261627197266, Average MAE: 2.121015787124634, Learning Rate: 0.32000000000000006\n",
      "Epoch 278, Average Loss: 16.888452529907227, Average MAE: 2.1314830780029297, Learning Rate: 0.32000000000000006\n",
      "Epoch 279, Average Loss: 16.78312873840332, Average MAE: 2.103231430053711, Learning Rate: 0.32000000000000006\n",
      "Epoch 280, Average Loss: 16.724525451660156, Average MAE: 2.0938637256622314, Learning Rate: 0.32000000000000006\n",
      "Epoch 281, Average Loss: 16.83155632019043, Average MAE: 2.115725517272949, Learning Rate: 0.32000000000000006\n",
      "Epoch 282, Average Loss: 16.76567268371582, Average MAE: 2.0966269969940186, Learning Rate: 0.32000000000000006\n",
      "Epoch 283, Average Loss: 16.749629974365234, Average MAE: 2.099403142929077, Learning Rate: 0.32000000000000006\n",
      "Epoch 284, Average Loss: 16.698606491088867, Average MAE: 2.094233751296997, Learning Rate: 0.32000000000000006\n",
      "Epoch 285, Average Loss: 16.721193313598633, Average MAE: 2.0893213748931885, Learning Rate: 0.32000000000000006\n",
      "Epoch 286, Average Loss: 16.61914825439453, Average MAE: 2.074349880218506, Learning Rate: 0.32000000000000006\n",
      "Epoch 287, Average Loss: 16.64175796508789, Average MAE: 2.0828161239624023, Learning Rate: 0.32000000000000006\n",
      "Epoch 288, Average Loss: 16.631805419921875, Average MAE: 2.079084873199463, Learning Rate: 0.32000000000000006\n",
      "Epoch 289, Average Loss: 16.63745880126953, Average MAE: 2.072547435760498, Learning Rate: 0.32000000000000006\n",
      "Epoch 290, Average Loss: 16.56989097595215, Average MAE: 2.066605806350708, Learning Rate: 0.32000000000000006\n",
      "Epoch 291, Average Loss: 16.615510940551758, Average MAE: 2.066850185394287, Learning Rate: 0.32000000000000006\n",
      "Epoch 292, Average Loss: 16.59006118774414, Average MAE: 2.067624568939209, Learning Rate: 0.32000000000000006\n",
      "Epoch 293, Average Loss: 16.538494110107422, Average MAE: 2.052898406982422, Learning Rate: 0.32000000000000006\n",
      "Epoch 294, Average Loss: 16.542522430419922, Average MAE: 2.0558676719665527, Learning Rate: 0.32000000000000006\n",
      "Epoch 295, Average Loss: 16.554834365844727, Average MAE: 2.0602853298187256, Learning Rate: 0.32000000000000006\n",
      "Epoch 296, Average Loss: 16.498653411865234, Average MAE: 2.0491485595703125, Learning Rate: 0.32000000000000006\n",
      "Epoch 297, Average Loss: 16.535106658935547, Average MAE: 2.0604374408721924, Learning Rate: 0.32000000000000006\n",
      "Epoch 298, Average Loss: 16.464656829833984, Average MAE: 2.0469541549682617, Learning Rate: 0.32000000000000006\n",
      "Epoch 299, Average Loss: 16.489723205566406, Average MAE: 2.040922164916992, Learning Rate: 0.32000000000000006\n",
      "Epoch 300, Average Loss: 16.523977279663086, Average MAE: 2.0469908714294434, Learning Rate: 0.32000000000000006\n",
      "Epoch 301, Average Loss: 16.47418212890625, Average MAE: 2.039626359939575, Learning Rate: 0.32000000000000006\n",
      "Epoch 302, Average Loss: 16.40854263305664, Average MAE: 2.0281288623809814, Learning Rate: 0.32000000000000006\n",
      "Epoch 303, Average Loss: 16.42354965209961, Average MAE: 2.03196382522583, Learning Rate: 0.32000000000000006\n",
      "Epoch 304, Average Loss: 16.358598709106445, Average MAE: 2.0124151706695557, Learning Rate: 0.32000000000000006\n",
      "Epoch 305, Average Loss: 16.366008758544922, Average MAE: 2.0198922157287598, Learning Rate: 0.32000000000000006\n",
      "Epoch 306, Average Loss: 16.37757682800293, Average MAE: 2.0256097316741943, Learning Rate: 0.32000000000000006\n",
      "Epoch 307, Average Loss: 16.34652328491211, Average MAE: 2.018441677093506, Learning Rate: 0.32000000000000006\n",
      "Epoch 308, Average Loss: 16.328857421875, Average MAE: 2.0074515342712402, Learning Rate: 0.32000000000000006\n",
      "Epoch 309, Average Loss: 16.347091674804688, Average MAE: 2.0131006240844727, Learning Rate: 0.32000000000000006\n",
      "Epoch 310, Average Loss: 16.32990264892578, Average MAE: 2.0118660926818848, Learning Rate: 0.32000000000000006\n",
      "Epoch 311, Average Loss: 16.253902435302734, Average MAE: 2.001210927963257, Learning Rate: 0.32000000000000006\n",
      "Epoch 312, Average Loss: 16.29939079284668, Average MAE: 2.006998062133789, Learning Rate: 0.32000000000000006\n",
      "Epoch 313, Average Loss: 16.324310302734375, Average MAE: 2.0082197189331055, Learning Rate: 0.32000000000000006\n",
      "Epoch 314, Average Loss: 16.250835418701172, Average MAE: 1.9941906929016113, Learning Rate: 0.32000000000000006\n",
      "Epoch 315, Average Loss: 16.215974807739258, Average MAE: 1.9838056564331055, Learning Rate: 0.32000000000000006\n",
      "Epoch 316, Average Loss: 16.23799705505371, Average MAE: 1.9874050617218018, Learning Rate: 0.32000000000000006\n",
      "Epoch 317, Average Loss: 16.242124557495117, Average MAE: 1.988892674446106, Learning Rate: 0.32000000000000006\n",
      "Epoch 318, Average Loss: 16.171451568603516, Average MAE: 1.9811489582061768, Learning Rate: 0.32000000000000006\n",
      "Epoch 319, Average Loss: 16.149829864501953, Average MAE: 1.9732308387756348, Learning Rate: 0.32000000000000006\n",
      "Epoch 320, Average Loss: 16.175899505615234, Average MAE: 1.9709787368774414, Learning Rate: 0.32000000000000006\n",
      "Epoch 321, Average Loss: 16.17145347595215, Average MAE: 1.9849119186401367, Learning Rate: 0.32000000000000006\n",
      "Epoch 322, Average Loss: 16.141630172729492, Average MAE: 1.9720348119735718, Learning Rate: 0.32000000000000006\n",
      "Epoch 323, Average Loss: 16.120342254638672, Average MAE: 1.9649152755737305, Learning Rate: 0.32000000000000006\n",
      "Epoch 324, Average Loss: 16.118722915649414, Average MAE: 1.9725362062454224, Learning Rate: 0.32000000000000006\n",
      "Epoch 325, Average Loss: 16.079938888549805, Average MAE: 1.9612274169921875, Learning Rate: 0.32000000000000006\n",
      "Epoch 326, Average Loss: 16.109285354614258, Average MAE: 1.9609392881393433, Learning Rate: 0.32000000000000006\n",
      "Epoch 327, Average Loss: 16.10881805419922, Average MAE: 1.9618958234786987, Learning Rate: 0.32000000000000006\n",
      "Epoch 328, Average Loss: 16.121292114257812, Average MAE: 1.9604398012161255, Learning Rate: 0.32000000000000006\n",
      "Epoch 329, Average Loss: 16.067453384399414, Average MAE: 1.952505350112915, Learning Rate: 0.32000000000000006\n",
      "Epoch 330, Average Loss: 16.074413299560547, Average MAE: 1.9552857875823975, Learning Rate: 0.32000000000000006\n",
      "Epoch 331, Average Loss: 16.039730072021484, Average MAE: 1.940505862236023, Learning Rate: 0.32000000000000006\n",
      "Epoch 332, Average Loss: 16.018173217773438, Average MAE: 1.943665862083435, Learning Rate: 0.32000000000000006\n",
      "Epoch 333, Average Loss: 16.0046329498291, Average MAE: 1.9454387426376343, Learning Rate: 0.32000000000000006\n",
      "Epoch 334, Average Loss: 16.006759643554688, Average MAE: 1.9369347095489502, Learning Rate: 0.32000000000000006\n",
      "Epoch 335, Average Loss: 15.95199203491211, Average MAE: 1.9272887706756592, Learning Rate: 0.32000000000000006\n",
      "Epoch 336, Average Loss: 15.999688148498535, Average MAE: 1.9398099184036255, Learning Rate: 0.32000000000000006\n",
      "Epoch 337, Average Loss: 15.995092391967773, Average MAE: 1.9300000667572021, Learning Rate: 0.32000000000000006\n",
      "Epoch 338, Average Loss: 15.925085067749023, Average MAE: 1.9232186079025269, Learning Rate: 0.32000000000000006\n",
      "Epoch 339, Average Loss: 15.977238655090332, Average MAE: 1.9288474321365356, Learning Rate: 0.32000000000000006\n",
      "Epoch 340, Average Loss: 15.965208053588867, Average MAE: 1.9292134046554565, Learning Rate: 0.32000000000000006\n",
      "Epoch 341, Average Loss: 15.872979164123535, Average MAE: 1.9092888832092285, Learning Rate: 0.32000000000000006\n",
      "Epoch 342, Average Loss: 15.921998977661133, Average MAE: 1.9261802434921265, Learning Rate: 0.32000000000000006\n",
      "Epoch 343, Average Loss: 15.876893997192383, Average MAE: 1.9022579193115234, Learning Rate: 0.32000000000000006\n",
      "Epoch 344, Average Loss: 15.934473037719727, Average MAE: 1.9140373468399048, Learning Rate: 0.32000000000000006\n",
      "Epoch 345, Average Loss: 15.85892391204834, Average MAE: 1.904807209968567, Learning Rate: 0.32000000000000006\n",
      "Epoch 346, Average Loss: 15.900458335876465, Average MAE: 1.90807044506073, Learning Rate: 0.32000000000000006\n",
      "Epoch 347, Average Loss: 15.880903244018555, Average MAE: 1.9068734645843506, Learning Rate: 0.32000000000000006\n",
      "Epoch 348, Average Loss: 15.848162651062012, Average MAE: 1.9027570486068726, Learning Rate: 0.32000000000000006\n",
      "Epoch 349, Average Loss: 15.838173866271973, Average MAE: 1.8985401391983032, Learning Rate: 0.32000000000000006\n",
      "Epoch 350, Average Loss: 15.831153869628906, Average MAE: 1.8968669176101685, Learning Rate: 0.32000000000000006\n",
      "Epoch 351, Average Loss: 15.831035614013672, Average MAE: 1.8995262384414673, Learning Rate: 0.32000000000000006\n",
      "Epoch 352, Average Loss: 15.805671691894531, Average MAE: 1.8891643285751343, Learning Rate: 0.32000000000000006\n",
      "Epoch 353, Average Loss: 15.820074081420898, Average MAE: 1.8935120105743408, Learning Rate: 0.32000000000000006\n",
      "Epoch 354, Average Loss: 15.849042892456055, Average MAE: 1.9033740758895874, Learning Rate: 0.32000000000000006\n",
      "Epoch 355, Average Loss: 15.783259391784668, Average MAE: 1.883259892463684, Learning Rate: 0.32000000000000006\n",
      "Epoch 356, Average Loss: 15.727815628051758, Average MAE: 1.8719754219055176, Learning Rate: 0.32000000000000006\n",
      "Epoch 357, Average Loss: 15.763427734375, Average MAE: 1.8794491291046143, Learning Rate: 0.32000000000000006\n",
      "Epoch 358, Average Loss: 15.75931167602539, Average MAE: 1.8729721307754517, Learning Rate: 0.32000000000000006\n",
      "Epoch 359, Average Loss: 15.741823196411133, Average MAE: 1.8717647790908813, Learning Rate: 0.32000000000000006\n",
      "Epoch 360, Average Loss: 15.666905403137207, Average MAE: 1.8598803281784058, Learning Rate: 0.32000000000000006\n",
      "Epoch 361, Average Loss: 15.704211235046387, Average MAE: 1.8684993982315063, Learning Rate: 0.32000000000000006\n",
      "Epoch 362, Average Loss: 15.691194534301758, Average MAE: 1.863163709640503, Learning Rate: 0.32000000000000006\n",
      "Epoch 363, Average Loss: 15.670821189880371, Average MAE: 1.855538249015808, Learning Rate: 0.32000000000000006\n",
      "Epoch 364, Average Loss: 15.630871772766113, Average MAE: 1.8502355813980103, Learning Rate: 0.32000000000000006\n",
      "Epoch 365, Average Loss: 15.687067985534668, Average MAE: 1.8551329374313354, Learning Rate: 0.32000000000000006\n",
      "Epoch 366, Average Loss: 15.635899543762207, Average MAE: 1.8521201610565186, Learning Rate: 0.32000000000000006\n",
      "Epoch 367, Average Loss: 15.643729209899902, Average MAE: 1.8525917530059814, Learning Rate: 0.32000000000000006\n",
      "Epoch 368, Average Loss: 15.676043510437012, Average MAE: 1.85556161403656, Learning Rate: 0.32000000000000006\n",
      "Epoch 369, Average Loss: 15.636886596679688, Average MAE: 1.8445285558700562, Learning Rate: 0.32000000000000006\n",
      "Epoch 370, Average Loss: 15.660469055175781, Average MAE: 1.846548080444336, Learning Rate: 0.32000000000000006\n",
      "Epoch 371, Average Loss: 15.584145545959473, Average MAE: 1.8367582559585571, Learning Rate: 0.32000000000000006\n",
      "Epoch 372, Average Loss: 15.590975761413574, Average MAE: 1.8328970670700073, Learning Rate: 0.32000000000000006\n",
      "Epoch 373, Average Loss: 15.56594181060791, Average MAE: 1.8293986320495605, Learning Rate: 0.32000000000000006\n",
      "Epoch 374, Average Loss: 15.598386764526367, Average MAE: 1.8407548666000366, Learning Rate: 0.32000000000000006\n",
      "Epoch 375, Average Loss: 15.587124824523926, Average MAE: 1.8392143249511719, Learning Rate: 0.32000000000000006\n",
      "Epoch 376, Average Loss: 15.602728843688965, Average MAE: 1.8366920948028564, Learning Rate: 0.32000000000000006\n",
      "Epoch 377, Average Loss: 15.564033508300781, Average MAE: 1.826242446899414, Learning Rate: 0.32000000000000006\n",
      "Epoch 378, Average Loss: 15.553638458251953, Average MAE: 1.826641321182251, Learning Rate: 0.32000000000000006\n",
      "Epoch 379, Average Loss: 15.539565086364746, Average MAE: 1.8215306997299194, Learning Rate: 0.32000000000000006\n",
      "Epoch 380, Average Loss: 15.557827949523926, Average MAE: 1.826080560684204, Learning Rate: 0.32000000000000006\n",
      "Epoch 381, Average Loss: 15.57958984375, Average MAE: 1.8298876285552979, Learning Rate: 0.32000000000000006\n",
      "Epoch 382, Average Loss: 15.538816452026367, Average MAE: 1.8189454078674316, Learning Rate: 0.32000000000000006\n",
      "Epoch 383, Average Loss: 15.505463600158691, Average MAE: 1.8156025409698486, Learning Rate: 0.32000000000000006\n",
      "Epoch 384, Average Loss: 15.495966911315918, Average MAE: 1.8071998357772827, Learning Rate: 0.32000000000000006\n",
      "Epoch 385, Average Loss: 15.476351737976074, Average MAE: 1.8052473068237305, Learning Rate: 0.32000000000000006\n",
      "Epoch 386, Average Loss: 15.486870765686035, Average MAE: 1.8119982481002808, Learning Rate: 0.32000000000000006\n",
      "Epoch 387, Average Loss: 15.457930564880371, Average MAE: 1.8031296730041504, Learning Rate: 0.32000000000000006\n",
      "Epoch 388, Average Loss: 15.446109771728516, Average MAE: 1.799726128578186, Learning Rate: 0.32000000000000006\n",
      "Epoch 389, Average Loss: 15.428485870361328, Average MAE: 1.7943371534347534, Learning Rate: 0.32000000000000006\n",
      "Epoch 390, Average Loss: 15.45058536529541, Average MAE: 1.7988545894622803, Learning Rate: 0.32000000000000006\n",
      "Epoch 391, Average Loss: 15.421428680419922, Average MAE: 1.7913305759429932, Learning Rate: 0.32000000000000006\n",
      "Epoch 392, Average Loss: 15.44775104522705, Average MAE: 1.8030978441238403, Learning Rate: 0.32000000000000006\n",
      "Epoch 393, Average Loss: 15.39407730102539, Average MAE: 1.7881033420562744, Learning Rate: 0.32000000000000006\n",
      "Epoch 394, Average Loss: 15.400932312011719, Average MAE: 1.7906116247177124, Learning Rate: 0.32000000000000006\n",
      "Epoch 395, Average Loss: 15.39579963684082, Average MAE: 1.7894636392593384, Learning Rate: 0.32000000000000006\n",
      "Epoch 396, Average Loss: 15.412275314331055, Average MAE: 1.7842752933502197, Learning Rate: 0.32000000000000006\n",
      "Epoch 397, Average Loss: 15.382719039916992, Average MAE: 1.7840715646743774, Learning Rate: 0.32000000000000006\n",
      "Epoch 398, Average Loss: 15.350020408630371, Average MAE: 1.7808341979980469, Learning Rate: 0.32000000000000006\n",
      "Epoch 399, Average Loss: 15.368484497070312, Average MAE: 1.7784647941589355, Learning Rate: 0.32000000000000006\n",
      "Epoch 400, Average Loss: 15.343155860900879, Average MAE: 1.7701228857040405, Learning Rate: 0.32000000000000006\n",
      "Epoch 401, Average Loss: 15.370941162109375, Average MAE: 1.7813081741333008, Learning Rate: 0.32000000000000006\n",
      "Epoch 402, Average Loss: 15.369948387145996, Average MAE: 1.7755833864212036, Learning Rate: 0.32000000000000006\n",
      "Epoch 403, Average Loss: 15.353839874267578, Average MAE: 1.778578519821167, Learning Rate: 0.32000000000000006\n",
      "Epoch 404, Average Loss: 15.30550479888916, Average MAE: 1.7579671144485474, Learning Rate: 0.32000000000000006\n",
      "Epoch 405, Average Loss: 15.330404281616211, Average MAE: 1.7608364820480347, Learning Rate: 0.32000000000000006\n",
      "Epoch 406, Average Loss: 15.338096618652344, Average MAE: 1.7781109809875488, Learning Rate: 0.32000000000000006\n",
      "Epoch 407, Average Loss: 15.263696670532227, Average MAE: 1.7501840591430664, Learning Rate: 0.32000000000000006\n",
      "Epoch 408, Average Loss: 15.291321754455566, Average MAE: 1.7603567838668823, Learning Rate: 0.32000000000000006\n",
      "Epoch 409, Average Loss: 15.28807258605957, Average MAE: 1.7628002166748047, Learning Rate: 0.32000000000000006\n",
      "Epoch 410, Average Loss: 15.306683540344238, Average MAE: 1.7537480592727661, Learning Rate: 0.32000000000000006\n",
      "Epoch 411, Average Loss: 15.281371116638184, Average MAE: 1.76129949092865, Learning Rate: 0.32000000000000006\n",
      "Epoch 412, Average Loss: 15.257955551147461, Average MAE: 1.747020959854126, Learning Rate: 0.32000000000000006\n",
      "Epoch 413, Average Loss: 15.272591590881348, Average MAE: 1.7496758699417114, Learning Rate: 0.32000000000000006\n",
      "Epoch 414, Average Loss: 15.25135612487793, Average MAE: 1.746738076210022, Learning Rate: 0.32000000000000006\n",
      "Epoch 415, Average Loss: 15.240601539611816, Average MAE: 1.7407660484313965, Learning Rate: 0.32000000000000006\n",
      "Epoch 416, Average Loss: 15.252466201782227, Average MAE: 1.7404073476791382, Learning Rate: 0.32000000000000006\n",
      "Epoch 417, Average Loss: 15.244383811950684, Average MAE: 1.7438347339630127, Learning Rate: 0.32000000000000006\n",
      "Epoch 418, Average Loss: 15.206506729125977, Average MAE: 1.7387083768844604, Learning Rate: 0.32000000000000006\n",
      "Epoch 419, Average Loss: 15.212383270263672, Average MAE: 1.734290599822998, Learning Rate: 0.32000000000000006\n",
      "Epoch 420, Average Loss: 15.220871925354004, Average MAE: 1.7368354797363281, Learning Rate: 0.32000000000000006\n",
      "Epoch 421, Average Loss: 15.199487686157227, Average MAE: 1.7314271926879883, Learning Rate: 0.32000000000000006\n",
      "Epoch 422, Average Loss: 15.182644844055176, Average MAE: 1.7260035276412964, Learning Rate: 0.32000000000000006\n",
      "Epoch 423, Average Loss: 15.18769645690918, Average MAE: 1.7289965152740479, Learning Rate: 0.32000000000000006\n",
      "Epoch 424, Average Loss: 15.162630081176758, Average MAE: 1.7237099409103394, Learning Rate: 0.32000000000000006\n",
      "Epoch 425, Average Loss: 15.134654998779297, Average MAE: 1.712566614151001, Learning Rate: 0.32000000000000006\n",
      "Epoch 426, Average Loss: 15.148303031921387, Average MAE: 1.7170253992080688, Learning Rate: 0.32000000000000006\n",
      "Epoch 427, Average Loss: 15.146512031555176, Average MAE: 1.7117974758148193, Learning Rate: 0.32000000000000006\n",
      "Epoch 428, Average Loss: 15.143346786499023, Average MAE: 1.717517375946045, Learning Rate: 0.32000000000000006\n",
      "Epoch 429, Average Loss: 15.131011009216309, Average MAE: 1.7193975448608398, Learning Rate: 0.32000000000000006\n",
      "Epoch 430, Average Loss: 15.128165245056152, Average MAE: 1.7112011909484863, Learning Rate: 0.32000000000000006\n",
      "Epoch 431, Average Loss: 15.145684242248535, Average MAE: 1.7123881578445435, Learning Rate: 0.32000000000000006\n",
      "Epoch 432, Average Loss: 15.111322402954102, Average MAE: 1.7021626234054565, Learning Rate: 0.32000000000000006\n",
      "Epoch 433, Average Loss: 15.091205596923828, Average MAE: 1.704138994216919, Learning Rate: 0.32000000000000006\n",
      "Epoch 434, Average Loss: 15.117886543273926, Average MAE: 1.7093144655227661, Learning Rate: 0.32000000000000006\n",
      "Epoch 435, Average Loss: 15.09701919555664, Average MAE: 1.6976042985916138, Learning Rate: 0.32000000000000006\n",
      "Epoch 436, Average Loss: 15.10084056854248, Average MAE: 1.704374074935913, Learning Rate: 0.32000000000000006\n",
      "Epoch 437, Average Loss: 15.076294898986816, Average MAE: 1.6992862224578857, Learning Rate: 0.32000000000000006\n",
      "Epoch 438, Average Loss: 15.10303783416748, Average MAE: 1.7083055973052979, Learning Rate: 0.32000000000000006\n",
      "Epoch 439, Average Loss: 15.101523399353027, Average MAE: 1.6976302862167358, Learning Rate: 0.32000000000000006\n",
      "Epoch 440, Average Loss: 15.072855949401855, Average MAE: 1.6907542943954468, Learning Rate: 0.32000000000000006\n",
      "Epoch 441, Average Loss: 15.038556098937988, Average MAE: 1.6908968687057495, Learning Rate: 0.32000000000000006\n",
      "Epoch 442, Average Loss: 15.077317237854004, Average MAE: 1.6901549100875854, Learning Rate: 0.32000000000000006\n",
      "Epoch 443, Average Loss: 15.083969116210938, Average MAE: 1.7010588645935059, Learning Rate: 0.32000000000000006\n",
      "Epoch 444, Average Loss: 15.046457290649414, Average MAE: 1.684425711631775, Learning Rate: 0.32000000000000006\n",
      "Epoch 445, Average Loss: 15.02150821685791, Average MAE: 1.6819746494293213, Learning Rate: 0.32000000000000006\n",
      "Epoch 446, Average Loss: 15.061659812927246, Average MAE: 1.6851576566696167, Learning Rate: 0.32000000000000006\n",
      "Epoch 447, Average Loss: 15.003347396850586, Average MAE: 1.6739435195922852, Learning Rate: 0.32000000000000006\n",
      "Epoch 448, Average Loss: 15.017633438110352, Average MAE: 1.6763758659362793, Learning Rate: 0.32000000000000006\n",
      "Epoch 449, Average Loss: 15.0281400680542, Average MAE: 1.6809742450714111, Learning Rate: 0.32000000000000006\n",
      "Epoch 450, Average Loss: 15.031400680541992, Average MAE: 1.6823397874832153, Learning Rate: 0.32000000000000006\n",
      "Epoch 451, Average Loss: 15.01681137084961, Average MAE: 1.670589804649353, Learning Rate: 0.32000000000000006\n",
      "Epoch 452, Average Loss: 15.017926216125488, Average MAE: 1.6733206510543823, Learning Rate: 0.32000000000000006\n",
      "Epoch 453, Average Loss: 14.982874870300293, Average MAE: 1.6677623987197876, Learning Rate: 0.32000000000000006\n",
      "Epoch 454, Average Loss: 15.007341384887695, Average MAE: 1.6777727603912354, Learning Rate: 0.32000000000000006\n",
      "Epoch 455, Average Loss: 15.00670051574707, Average MAE: 1.6706719398498535, Learning Rate: 0.32000000000000006\n",
      "Epoch 456, Average Loss: 14.960927963256836, Average MAE: 1.6680546998977661, Learning Rate: 0.32000000000000006\n",
      "Epoch 457, Average Loss: 14.990981101989746, Average MAE: 1.6640851497650146, Learning Rate: 0.32000000000000006\n",
      "Epoch 458, Average Loss: 14.95521068572998, Average MAE: 1.6558470726013184, Learning Rate: 0.32000000000000006\n",
      "Epoch 459, Average Loss: 14.95230770111084, Average MAE: 1.6595990657806396, Learning Rate: 0.32000000000000006\n",
      "Epoch 460, Average Loss: 14.981619834899902, Average MAE: 1.656013011932373, Learning Rate: 0.32000000000000006\n",
      "Epoch 461, Average Loss: 14.947212219238281, Average MAE: 1.6610738039016724, Learning Rate: 0.32000000000000006\n",
      "Epoch 462, Average Loss: 14.94446086883545, Average MAE: 1.6530482769012451, Learning Rate: 0.32000000000000006\n",
      "Epoch 463, Average Loss: 14.931021690368652, Average MAE: 1.6504420042037964, Learning Rate: 0.32000000000000006\n",
      "Epoch 464, Average Loss: 14.937588691711426, Average MAE: 1.644524335861206, Learning Rate: 0.32000000000000006\n",
      "Epoch 465, Average Loss: 14.916409492492676, Average MAE: 1.6467852592468262, Learning Rate: 0.32000000000000006\n",
      "Epoch 466, Average Loss: 14.908695220947266, Average MAE: 1.64651358127594, Learning Rate: 0.32000000000000006\n",
      "Epoch 467, Average Loss: 14.94373607635498, Average MAE: 1.6470786333084106, Learning Rate: 0.32000000000000006\n",
      "Epoch 468, Average Loss: 14.902613639831543, Average MAE: 1.642608404159546, Learning Rate: 0.32000000000000006\n",
      "Epoch 469, Average Loss: 14.883286476135254, Average MAE: 1.6395691633224487, Learning Rate: 0.32000000000000006\n",
      "Epoch 470, Average Loss: 14.9042329788208, Average MAE: 1.6359095573425293, Learning Rate: 0.32000000000000006\n",
      "Epoch 471, Average Loss: 14.87376594543457, Average MAE: 1.6348819732666016, Learning Rate: 0.32000000000000006\n",
      "Epoch 472, Average Loss: 14.891050338745117, Average MAE: 1.6361002922058105, Learning Rate: 0.32000000000000006\n",
      "Epoch 473, Average Loss: 14.876585960388184, Average MAE: 1.634668231010437, Learning Rate: 0.32000000000000006\n",
      "Epoch 474, Average Loss: 14.864272117614746, Average MAE: 1.6322548389434814, Learning Rate: 0.32000000000000006\n",
      "Epoch 475, Average Loss: 14.845803260803223, Average MAE: 1.6236993074417114, Learning Rate: 0.32000000000000006\n",
      "Epoch 476, Average Loss: 14.821374893188477, Average MAE: 1.6328524351119995, Learning Rate: 0.32000000000000006\n",
      "Epoch 477, Average Loss: 14.838024139404297, Average MAE: 1.6236774921417236, Learning Rate: 0.32000000000000006\n",
      "Epoch 478, Average Loss: 14.898260116577148, Average MAE: 1.636489748954773, Learning Rate: 0.32000000000000006\n",
      "Epoch 479, Average Loss: 14.831653594970703, Average MAE: 1.6166338920593262, Learning Rate: 0.32000000000000006\n",
      "Epoch 480, Average Loss: 14.83069896697998, Average MAE: 1.6184602975845337, Learning Rate: 0.32000000000000006\n",
      "Epoch 481, Average Loss: 14.813887596130371, Average MAE: 1.6267043352127075, Learning Rate: 0.32000000000000006\n",
      "Epoch 482, Average Loss: 14.867353439331055, Average MAE: 1.6283326148986816, Learning Rate: 0.32000000000000006\n",
      "Epoch 483, Average Loss: 14.852039337158203, Average MAE: 1.6337934732437134, Learning Rate: 0.32000000000000006\n",
      "Epoch 484, Average Loss: 14.847411155700684, Average MAE: 1.6244118213653564, Learning Rate: 0.32000000000000006\n",
      "Epoch 485, Average Loss: 14.83055305480957, Average MAE: 1.6235556602478027, Learning Rate: 0.32000000000000006\n",
      "Epoch 486, Average Loss: 14.795391082763672, Average MAE: 1.6076830625534058, Learning Rate: 0.32000000000000006\n",
      "Epoch 487, Average Loss: 14.815862655639648, Average MAE: 1.6102584600448608, Learning Rate: 0.32000000000000006\n",
      "Epoch 488, Average Loss: 14.816046714782715, Average MAE: 1.6168407201766968, Learning Rate: 0.32000000000000006\n",
      "Epoch 489, Average Loss: 14.828508377075195, Average MAE: 1.6091824769973755, Learning Rate: 0.32000000000000006\n",
      "Epoch 490, Average Loss: 14.793403625488281, Average MAE: 1.6139949560165405, Learning Rate: 0.32000000000000006\n",
      "Epoch 491, Average Loss: 14.766392707824707, Average MAE: 1.602199673652649, Learning Rate: 0.32000000000000006\n",
      "Epoch 492, Average Loss: 14.777795791625977, Average MAE: 1.6014419794082642, Learning Rate: 0.32000000000000006\n",
      "Epoch 493, Average Loss: 14.768238067626953, Average MAE: 1.6016672849655151, Learning Rate: 0.32000000000000006\n",
      "Epoch 494, Average Loss: 14.79106616973877, Average MAE: 1.6028920412063599, Learning Rate: 0.32000000000000006\n",
      "Epoch 495, Average Loss: 14.822599411010742, Average MAE: 1.6260870695114136, Learning Rate: 0.32000000000000006\n",
      "Epoch 496, Average Loss: 14.793919563293457, Average MAE: 1.6073137521743774, Learning Rate: 0.32000000000000006\n",
      "Epoch 497, Average Loss: 14.767438888549805, Average MAE: 1.5984405279159546, Learning Rate: 0.32000000000000006\n",
      "Epoch 498, Average Loss: 14.748207092285156, Average MAE: 1.5893410444259644, Learning Rate: 0.32000000000000006\n",
      "Epoch 499, Average Loss: 14.742383003234863, Average MAE: 1.586851716041565, Learning Rate: 0.32000000000000006\n",
      "Epoch 500, Average Loss: 14.733719825744629, Average MAE: 1.5895986557006836, Learning Rate: 0.32000000000000006\n",
      "Epoch 501, Average Loss: 14.747029304504395, Average MAE: 1.587451696395874, Learning Rate: 0.32000000000000006\n",
      "Epoch 502, Average Loss: 14.736824035644531, Average MAE: 1.590399980545044, Learning Rate: 0.32000000000000006\n",
      "Epoch 503, Average Loss: 14.774612426757812, Average MAE: 1.590728521347046, Learning Rate: 0.32000000000000006\n",
      "Epoch 504, Average Loss: 14.723042488098145, Average MAE: 1.5894339084625244, Learning Rate: 0.32000000000000006\n",
      "Epoch 505, Average Loss: 14.729700088500977, Average MAE: 1.5826140642166138, Learning Rate: 0.32000000000000006\n",
      "Epoch 506, Average Loss: 14.730669975280762, Average MAE: 1.588889718055725, Learning Rate: 0.32000000000000006\n",
      "Epoch 507, Average Loss: 14.754660606384277, Average MAE: 1.585209846496582, Learning Rate: 0.32000000000000006\n",
      "Epoch 508, Average Loss: 14.74561595916748, Average MAE: 1.5971336364746094, Learning Rate: 0.32000000000000006\n",
      "Epoch 509, Average Loss: 14.74385929107666, Average MAE: 1.5807234048843384, Learning Rate: 0.32000000000000006\n",
      "Epoch 510, Average Loss: 14.732402801513672, Average MAE: 1.5887867212295532, Learning Rate: 0.32000000000000006\n",
      "Epoch 511, Average Loss: 14.700206756591797, Average MAE: 1.5715588331222534, Learning Rate: 0.32000000000000006\n",
      "Epoch 512, Average Loss: 14.715306282043457, Average MAE: 1.5733745098114014, Learning Rate: 0.32000000000000006\n",
      "Epoch 513, Average Loss: 14.657215118408203, Average MAE: 1.5699834823608398, Learning Rate: 0.32000000000000006\n",
      "Epoch 514, Average Loss: 14.690752029418945, Average MAE: 1.569186806678772, Learning Rate: 0.32000000000000006\n",
      "Epoch 515, Average Loss: 14.68696117401123, Average MAE: 1.5769500732421875, Learning Rate: 0.32000000000000006\n",
      "Epoch 516, Average Loss: 14.662318229675293, Average MAE: 1.5610573291778564, Learning Rate: 0.32000000000000006\n",
      "Epoch 517, Average Loss: 14.661102294921875, Average MAE: 1.5617927312850952, Learning Rate: 0.32000000000000006\n",
      "Epoch 518, Average Loss: 14.691630363464355, Average MAE: 1.5646190643310547, Learning Rate: 0.32000000000000006\n",
      "Epoch 519, Average Loss: 14.678834915161133, Average MAE: 1.559733510017395, Learning Rate: 0.32000000000000006\n",
      "Epoch 520, Average Loss: 14.632015228271484, Average MAE: 1.5596622228622437, Learning Rate: 0.32000000000000006\n",
      "Epoch 521, Average Loss: 14.670798301696777, Average MAE: 1.5573668479919434, Learning Rate: 0.32000000000000006\n",
      "Epoch 522, Average Loss: 14.632111549377441, Average MAE: 1.5526434183120728, Learning Rate: 0.32000000000000006\n",
      "Epoch 523, Average Loss: 14.650582313537598, Average MAE: 1.5588303804397583, Learning Rate: 0.32000000000000006\n",
      "Epoch 524, Average Loss: 14.606213569641113, Average MAE: 1.5509905815124512, Learning Rate: 0.32000000000000006\n",
      "Epoch 525, Average Loss: 14.628387451171875, Average MAE: 1.5526807308197021, Learning Rate: 0.32000000000000006\n",
      "Epoch 526, Average Loss: 14.650840759277344, Average MAE: 1.5506036281585693, Learning Rate: 0.32000000000000006\n",
      "Epoch 527, Average Loss: 14.605100631713867, Average MAE: 1.544761061668396, Learning Rate: 0.32000000000000006\n",
      "Epoch 528, Average Loss: 14.626163482666016, Average MAE: 1.550844430923462, Learning Rate: 0.32000000000000006\n",
      "Epoch 529, Average Loss: 14.623400688171387, Average MAE: 1.5476375818252563, Learning Rate: 0.32000000000000006\n",
      "Epoch 530, Average Loss: 14.628464698791504, Average MAE: 1.5504798889160156, Learning Rate: 0.32000000000000006\n",
      "Epoch 531, Average Loss: 14.73367977142334, Average MAE: 1.5979747772216797, Learning Rate: 0.32000000000000006\n",
      "Epoch 532, Average Loss: 15.012348175048828, Average MAE: 1.6649938821792603, Learning Rate: 0.32000000000000006\n",
      "Epoch 533, Average Loss: 15.514554977416992, Average MAE: 1.855633020401001, Learning Rate: 0.32000000000000006\n",
      "Epoch 534, Average Loss: 15.709851264953613, Average MAE: 1.8932528495788574, Learning Rate: 0.32000000000000006\n",
      "Epoch 535, Average Loss: 15.341684341430664, Average MAE: 1.8033758401870728, Learning Rate: 0.25600000000000006\n",
      "Epoch 536, Average Loss: 14.690895080566406, Average MAE: 1.5591007471084595, Learning Rate: 0.25600000000000006\n",
      "Epoch 537, Average Loss: 14.693714141845703, Average MAE: 1.5658085346221924, Learning Rate: 0.25600000000000006\n",
      "Epoch 538, Average Loss: 14.862637519836426, Average MAE: 1.6390961408615112, Learning Rate: 0.25600000000000006\n",
      "Epoch 539, Average Loss: 14.54987621307373, Average MAE: 1.5320335626602173, Learning Rate: 0.25600000000000006\n",
      "Epoch 540, Average Loss: 14.77607250213623, Average MAE: 1.591063141822815, Learning Rate: 0.25600000000000006\n",
      "Epoch 541, Average Loss: 14.702857971191406, Average MAE: 1.5825669765472412, Learning Rate: 0.25600000000000006\n",
      "Epoch 542, Average Loss: 14.614745140075684, Average MAE: 1.5437064170837402, Learning Rate: 0.25600000000000006\n",
      "Epoch 543, Average Loss: 14.713342666625977, Average MAE: 1.569898247718811, Learning Rate: 0.25600000000000006\n",
      "Epoch 544, Average Loss: 14.566001892089844, Average MAE: 1.5351214408874512, Learning Rate: 0.25600000000000006\n",
      "Epoch 545, Average Loss: 14.62113094329834, Average MAE: 1.549484372138977, Learning Rate: 0.25600000000000006\n",
      "Epoch 546, Average Loss: 14.643948554992676, Average MAE: 1.5416510105133057, Learning Rate: 0.25600000000000006\n",
      "Epoch 547, Average Loss: 14.544283866882324, Average MAE: 1.5247673988342285, Learning Rate: 0.25600000000000006\n",
      "Epoch 548, Average Loss: 14.630563735961914, Average MAE: 1.5522431135177612, Learning Rate: 0.25600000000000006\n",
      "Epoch 549, Average Loss: 14.571290016174316, Average MAE: 1.520926594734192, Learning Rate: 0.25600000000000006\n",
      "Epoch 550, Average Loss: 14.544693946838379, Average MAE: 1.5262913703918457, Learning Rate: 0.25600000000000006\n",
      "Epoch 551, Average Loss: 14.589018821716309, Average MAE: 1.539231777191162, Learning Rate: 0.25600000000000006\n",
      "Epoch 552, Average Loss: 14.556474685668945, Average MAE: 1.520470142364502, Learning Rate: 0.25600000000000006\n",
      "Epoch 553, Average Loss: 14.546615600585938, Average MAE: 1.5173569917678833, Learning Rate: 0.25600000000000006\n",
      "Epoch 554, Average Loss: 14.557140350341797, Average MAE: 1.5255855321884155, Learning Rate: 0.25600000000000006\n",
      "Epoch 555, Average Loss: 14.560826301574707, Average MAE: 1.5257092714309692, Learning Rate: 0.25600000000000006\n",
      "Epoch 556, Average Loss: 14.554811477661133, Average MAE: 1.5213464498519897, Learning Rate: 0.25600000000000006\n",
      "Epoch 557, Average Loss: 14.515032768249512, Average MAE: 1.5165036916732788, Learning Rate: 0.25600000000000006\n",
      "Epoch 558, Average Loss: 14.5593843460083, Average MAE: 1.5231767892837524, Learning Rate: 0.25600000000000006\n",
      "Epoch 559, Average Loss: 14.572782516479492, Average MAE: 1.517892599105835, Learning Rate: 0.25600000000000006\n",
      "Epoch 560, Average Loss: 14.511419296264648, Average MAE: 1.506900668144226, Learning Rate: 0.25600000000000006\n",
      "Epoch 561, Average Loss: 14.522281646728516, Average MAE: 1.5135650634765625, Learning Rate: 0.25600000000000006\n",
      "Epoch 562, Average Loss: 14.559345245361328, Average MAE: 1.5156363248825073, Learning Rate: 0.25600000000000006\n",
      "Epoch 563, Average Loss: 14.484537124633789, Average MAE: 1.4988503456115723, Learning Rate: 0.25600000000000006\n",
      "Epoch 564, Average Loss: 14.556262969970703, Average MAE: 1.5195293426513672, Learning Rate: 0.25600000000000006\n",
      "Epoch 565, Average Loss: 14.54332160949707, Average MAE: 1.514276146888733, Learning Rate: 0.25600000000000006\n",
      "Epoch 566, Average Loss: 14.480672836303711, Average MAE: 1.5025938749313354, Learning Rate: 0.25600000000000006\n",
      "Epoch 567, Average Loss: 14.54141902923584, Average MAE: 1.52240788936615, Learning Rate: 0.25600000000000006\n",
      "Epoch 568, Average Loss: 14.49944019317627, Average MAE: 1.5028671026229858, Learning Rate: 0.25600000000000006\n",
      "Epoch 569, Average Loss: 14.496345520019531, Average MAE: 1.501470685005188, Learning Rate: 0.25600000000000006\n",
      "Epoch 570, Average Loss: 14.488700866699219, Average MAE: 1.5014338493347168, Learning Rate: 0.25600000000000006\n",
      "Epoch 571, Average Loss: 14.480493545532227, Average MAE: 1.4975638389587402, Learning Rate: 0.25600000000000006\n",
      "Epoch 572, Average Loss: 14.48215389251709, Average MAE: 1.495836853981018, Learning Rate: 0.25600000000000006\n",
      "Epoch 573, Average Loss: 14.507841110229492, Average MAE: 1.5073713064193726, Learning Rate: 0.25600000000000006\n",
      "Epoch 574, Average Loss: 14.501058578491211, Average MAE: 1.501032829284668, Learning Rate: 0.25600000000000006\n",
      "Epoch 575, Average Loss: 14.499532699584961, Average MAE: 1.4979015588760376, Learning Rate: 0.25600000000000006\n",
      "Epoch 576, Average Loss: 14.490274429321289, Average MAE: 1.500715732574463, Learning Rate: 0.25600000000000006\n",
      "Epoch 577, Average Loss: 14.464078903198242, Average MAE: 1.4965815544128418, Learning Rate: 0.25600000000000006\n",
      "Epoch 578, Average Loss: 14.482784271240234, Average MAE: 1.4946672916412354, Learning Rate: 0.25600000000000006\n",
      "Epoch 579, Average Loss: 14.48882007598877, Average MAE: 1.5006434917449951, Learning Rate: 0.25600000000000006\n",
      "Epoch 580, Average Loss: 14.46831226348877, Average MAE: 1.4955532550811768, Learning Rate: 0.25600000000000006\n",
      "Epoch 581, Average Loss: 14.442578315734863, Average MAE: 1.4864397048950195, Learning Rate: 0.25600000000000006\n",
      "Epoch 582, Average Loss: 14.47756290435791, Average MAE: 1.4941447973251343, Learning Rate: 0.25600000000000006\n",
      "Epoch 583, Average Loss: 14.470932006835938, Average MAE: 1.488761305809021, Learning Rate: 0.25600000000000006\n",
      "Epoch 584, Average Loss: 14.469756126403809, Average MAE: 1.4889068603515625, Learning Rate: 0.25600000000000006\n",
      "Epoch 585, Average Loss: 14.45911979675293, Average MAE: 1.4886006116867065, Learning Rate: 0.25600000000000006\n",
      "Epoch 586, Average Loss: 14.486370086669922, Average MAE: 1.4896948337554932, Learning Rate: 0.25600000000000006\n",
      "Epoch 587, Average Loss: 14.47897720336914, Average MAE: 1.490195393562317, Learning Rate: 0.25600000000000006\n",
      "Epoch 588, Average Loss: 14.433277130126953, Average MAE: 1.4827405214309692, Learning Rate: 0.25600000000000006\n",
      "Epoch 589, Average Loss: 14.477710723876953, Average MAE: 1.4849517345428467, Learning Rate: 0.25600000000000006\n",
      "Epoch 590, Average Loss: 14.483782768249512, Average MAE: 1.4862313270568848, Learning Rate: 0.25600000000000006\n",
      "Epoch 591, Average Loss: 14.457356452941895, Average MAE: 1.4858365058898926, Learning Rate: 0.25600000000000006\n",
      "Epoch 592, Average Loss: 14.46347713470459, Average MAE: 1.4847179651260376, Learning Rate: 0.25600000000000006\n",
      "Epoch 593, Average Loss: 14.41998291015625, Average MAE: 1.4790912866592407, Learning Rate: 0.25600000000000006\n",
      "Epoch 594, Average Loss: 14.458649635314941, Average MAE: 1.485487699508667, Learning Rate: 0.25600000000000006\n",
      "Epoch 595, Average Loss: 14.436965942382812, Average MAE: 1.4794137477874756, Learning Rate: 0.25600000000000006\n",
      "Epoch 596, Average Loss: 14.441841125488281, Average MAE: 1.4764316082000732, Learning Rate: 0.25600000000000006\n",
      "Epoch 597, Average Loss: 14.411845207214355, Average MAE: 1.4748952388763428, Learning Rate: 0.25600000000000006\n",
      "Epoch 598, Average Loss: 14.44763469696045, Average MAE: 1.4724411964416504, Learning Rate: 0.25600000000000006\n",
      "Epoch 599, Average Loss: 14.46747875213623, Average MAE: 1.4797428846359253, Learning Rate: 0.25600000000000006\n",
      "Epoch 600, Average Loss: 14.401610374450684, Average MAE: 1.4766343832015991, Learning Rate: 0.25600000000000006\n",
      "Epoch 601, Average Loss: 14.428062438964844, Average MAE: 1.470576286315918, Learning Rate: 0.25600000000000006\n",
      "Epoch 602, Average Loss: 14.424610137939453, Average MAE: 1.470335602760315, Learning Rate: 0.25600000000000006\n",
      "Epoch 603, Average Loss: 14.417876243591309, Average MAE: 1.4714926481246948, Learning Rate: 0.25600000000000006\n",
      "Epoch 604, Average Loss: 14.366700172424316, Average MAE: 1.4628331661224365, Learning Rate: 0.25600000000000006\n",
      "Epoch 605, Average Loss: 14.396647453308105, Average MAE: 1.4631556272506714, Learning Rate: 0.25600000000000006\n",
      "Epoch 606, Average Loss: 14.397599220275879, Average MAE: 1.468658208847046, Learning Rate: 0.25600000000000006\n",
      "Epoch 607, Average Loss: 14.380768775939941, Average MAE: 1.4624426364898682, Learning Rate: 0.25600000000000006\n",
      "Epoch 608, Average Loss: 14.406795501708984, Average MAE: 1.4622710943222046, Learning Rate: 0.25600000000000006\n",
      "Epoch 609, Average Loss: 14.401863098144531, Average MAE: 1.461326003074646, Learning Rate: 0.25600000000000006\n",
      "Epoch 610, Average Loss: 14.40133285522461, Average MAE: 1.4575635194778442, Learning Rate: 0.25600000000000006\n",
      "Epoch 611, Average Loss: 14.381917953491211, Average MAE: 1.4590877294540405, Learning Rate: 0.25600000000000006\n",
      "Epoch 612, Average Loss: 14.4249906539917, Average MAE: 1.466882586479187, Learning Rate: 0.25600000000000006\n",
      "Epoch 613, Average Loss: 14.366143226623535, Average MAE: 1.4609273672103882, Learning Rate: 0.25600000000000006\n",
      "Epoch 614, Average Loss: 14.38765811920166, Average MAE: 1.4574594497680664, Learning Rate: 0.25600000000000006\n",
      "Epoch 615, Average Loss: 14.395015716552734, Average MAE: 1.4577155113220215, Learning Rate: 0.20480000000000007\n",
      "Epoch 616, Average Loss: 14.369150161743164, Average MAE: 1.4580814838409424, Learning Rate: 0.20480000000000007\n",
      "Epoch 617, Average Loss: 14.386049270629883, Average MAE: 1.4601458311080933, Learning Rate: 0.20480000000000007\n",
      "Epoch 618, Average Loss: 14.389580726623535, Average MAE: 1.453033447265625, Learning Rate: 0.20480000000000007\n",
      "Epoch 619, Average Loss: 14.36731243133545, Average MAE: 1.4534257650375366, Learning Rate: 0.20480000000000007\n",
      "Epoch 620, Average Loss: 14.368946075439453, Average MAE: 1.4534958600997925, Learning Rate: 0.20480000000000007\n",
      "Epoch 621, Average Loss: 14.390535354614258, Average MAE: 1.4540529251098633, Learning Rate: 0.20480000000000007\n",
      "Epoch 622, Average Loss: 14.380727767944336, Average MAE: 1.4496296644210815, Learning Rate: 0.20480000000000007\n",
      "Epoch 623, Average Loss: 14.357065200805664, Average MAE: 1.4468069076538086, Learning Rate: 0.20480000000000007\n",
      "Epoch 624, Average Loss: 14.370551109313965, Average MAE: 1.450411081314087, Learning Rate: 0.20480000000000007\n",
      "Epoch 625, Average Loss: 14.372520446777344, Average MAE: 1.448620319366455, Learning Rate: 0.20480000000000007\n",
      "Epoch 626, Average Loss: 14.40273666381836, Average MAE: 1.4532239437103271, Learning Rate: 0.20480000000000007\n",
      "Epoch 627, Average Loss: 14.38457202911377, Average MAE: 1.4478247165679932, Learning Rate: 0.20480000000000007\n",
      "Epoch 628, Average Loss: 14.35703182220459, Average MAE: 1.4468085765838623, Learning Rate: 0.20480000000000007\n",
      "Epoch 629, Average Loss: 14.333797454833984, Average MAE: 1.442744493484497, Learning Rate: 0.20480000000000007\n",
      "Epoch 630, Average Loss: 14.353130340576172, Average MAE: 1.4450017213821411, Learning Rate: 0.20480000000000007\n",
      "Epoch 631, Average Loss: 14.328353881835938, Average MAE: 1.441006064414978, Learning Rate: 0.20480000000000007\n",
      "Epoch 632, Average Loss: 14.36208724975586, Average MAE: 1.4490747451782227, Learning Rate: 0.20480000000000007\n",
      "Epoch 633, Average Loss: 14.37364673614502, Average MAE: 1.4476042985916138, Learning Rate: 0.20480000000000007\n",
      "Epoch 634, Average Loss: 14.3454008102417, Average MAE: 1.4416303634643555, Learning Rate: 0.20480000000000007\n",
      "Epoch 635, Average Loss: 14.34074592590332, Average MAE: 1.4406613111495972, Learning Rate: 0.20480000000000007\n",
      "Epoch 636, Average Loss: 14.35280990600586, Average MAE: 1.4416395425796509, Learning Rate: 0.20480000000000007\n",
      "Epoch 637, Average Loss: 14.321257591247559, Average MAE: 1.4390240907669067, Learning Rate: 0.20480000000000007\n",
      "Epoch 638, Average Loss: 14.369134902954102, Average MAE: 1.4434617757797241, Learning Rate: 0.20480000000000007\n",
      "Epoch 639, Average Loss: 14.329742431640625, Average MAE: 1.4374781847000122, Learning Rate: 0.20480000000000007\n",
      "Epoch 640, Average Loss: 14.31289005279541, Average MAE: 1.4372334480285645, Learning Rate: 0.20480000000000007\n",
      "Epoch 641, Average Loss: 14.357017517089844, Average MAE: 1.4396111965179443, Learning Rate: 0.20480000000000007\n",
      "Epoch 642, Average Loss: 14.327290534973145, Average MAE: 1.4341059923171997, Learning Rate: 0.20480000000000007\n",
      "Epoch 643, Average Loss: 14.33912467956543, Average MAE: 1.4410690069198608, Learning Rate: 0.20480000000000007\n",
      "Epoch 644, Average Loss: 14.334419250488281, Average MAE: 1.4391074180603027, Learning Rate: 0.20480000000000007\n",
      "Epoch 645, Average Loss: 14.344627380371094, Average MAE: 1.438214898109436, Learning Rate: 0.20480000000000007\n",
      "Epoch 646, Average Loss: 14.340487480163574, Average MAE: 1.4411697387695312, Learning Rate: 0.20480000000000007\n",
      "Epoch 647, Average Loss: 14.334138870239258, Average MAE: 1.436689853668213, Learning Rate: 0.20480000000000007\n",
      "Epoch 648, Average Loss: 14.366708755493164, Average MAE: 1.439170479774475, Learning Rate: 0.20480000000000007\n",
      "Epoch 649, Average Loss: 14.318504333496094, Average MAE: 1.4308981895446777, Learning Rate: 0.20480000000000007\n",
      "Epoch 650, Average Loss: 14.326961517333984, Average MAE: 1.4381382465362549, Learning Rate: 0.20480000000000007\n",
      "Epoch 651, Average Loss: 14.315522193908691, Average MAE: 1.4293255805969238, Learning Rate: 0.16384000000000007\n",
      "Epoch 652, Average Loss: 14.338622093200684, Average MAE: 1.4366447925567627, Learning Rate: 0.16384000000000007\n",
      "Epoch 653, Average Loss: 14.310843467712402, Average MAE: 1.4305264949798584, Learning Rate: 0.16384000000000007\n",
      "Epoch 654, Average Loss: 14.333735466003418, Average MAE: 1.437073826789856, Learning Rate: 0.16384000000000007\n",
      "Epoch 655, Average Loss: 14.290789604187012, Average MAE: 1.4285868406295776, Learning Rate: 0.16384000000000007\n",
      "Epoch 656, Average Loss: 14.319665908813477, Average MAE: 1.4241316318511963, Learning Rate: 0.16384000000000007\n",
      "Epoch 657, Average Loss: 14.328156471252441, Average MAE: 1.4319474697113037, Learning Rate: 0.16384000000000007\n",
      "Epoch 658, Average Loss: 14.323171615600586, Average MAE: 1.4318300485610962, Learning Rate: 0.16384000000000007\n",
      "Epoch 659, Average Loss: 14.318979263305664, Average MAE: 1.4268288612365723, Learning Rate: 0.16384000000000007\n",
      "Epoch 660, Average Loss: 14.31814956665039, Average MAE: 1.430214524269104, Learning Rate: 0.16384000000000007\n",
      "Epoch 661, Average Loss: 14.316245079040527, Average MAE: 1.426131010055542, Learning Rate: 0.16384000000000007\n",
      "Epoch 662, Average Loss: 14.31393814086914, Average MAE: 1.4303516149520874, Learning Rate: 0.16384000000000007\n",
      "Epoch 663, Average Loss: 14.330314636230469, Average MAE: 1.427582025527954, Learning Rate: 0.16384000000000007\n",
      "Epoch 664, Average Loss: 14.286447525024414, Average MAE: 1.4211913347244263, Learning Rate: 0.16384000000000007\n",
      "Epoch 665, Average Loss: 14.301191329956055, Average MAE: 1.4222688674926758, Learning Rate: 0.16384000000000007\n",
      "Epoch 666, Average Loss: 14.318521499633789, Average MAE: 1.4241467714309692, Learning Rate: 0.16384000000000007\n",
      "Epoch 667, Average Loss: 14.292640686035156, Average MAE: 1.4251909255981445, Learning Rate: 0.16384000000000007\n",
      "Epoch 668, Average Loss: 14.31656265258789, Average MAE: 1.4234861135482788, Learning Rate: 0.16384000000000007\n",
      "Epoch 669, Average Loss: 14.319096565246582, Average MAE: 1.4229336977005005, Learning Rate: 0.16384000000000007\n",
      "Epoch 670, Average Loss: 14.301048278808594, Average MAE: 1.4213863611221313, Learning Rate: 0.16384000000000007\n",
      "Epoch 671, Average Loss: 14.309900283813477, Average MAE: 1.4203500747680664, Learning Rate: 0.16384000000000007\n",
      "Epoch 672, Average Loss: 14.275604248046875, Average MAE: 1.4166444540023804, Learning Rate: 0.16384000000000007\n",
      "Epoch 673, Average Loss: 14.297351837158203, Average MAE: 1.4160137176513672, Learning Rate: 0.16384000000000007\n",
      "Epoch 674, Average Loss: 14.270523071289062, Average MAE: 1.42242431640625, Learning Rate: 0.16384000000000007\n",
      "Epoch 675, Average Loss: 14.274909973144531, Average MAE: 1.4202570915222168, Learning Rate: 0.16384000000000007\n",
      "Epoch 676, Average Loss: 14.312692642211914, Average MAE: 1.4222959280014038, Learning Rate: 0.16384000000000007\n",
      "Epoch 677, Average Loss: 14.268970489501953, Average MAE: 1.4174484014511108, Learning Rate: 0.16384000000000007\n",
      "Epoch 678, Average Loss: 14.285719871520996, Average MAE: 1.4193778038024902, Learning Rate: 0.16384000000000007\n",
      "Epoch 679, Average Loss: 14.289012908935547, Average MAE: 1.4169526100158691, Learning Rate: 0.16384000000000007\n",
      "Epoch 680, Average Loss: 14.266921997070312, Average MAE: 1.4153766632080078, Learning Rate: 0.16384000000000007\n",
      "Epoch 681, Average Loss: 14.284411430358887, Average MAE: 1.4183026552200317, Learning Rate: 0.16384000000000007\n",
      "Epoch 682, Average Loss: 14.304937362670898, Average MAE: 1.42076575756073, Learning Rate: 0.16384000000000007\n",
      "Epoch 683, Average Loss: 14.27268123626709, Average MAE: 1.411177396774292, Learning Rate: 0.16384000000000007\n",
      "Epoch 684, Average Loss: 14.27074909210205, Average MAE: 1.4137639999389648, Learning Rate: 0.16384000000000007\n",
      "Epoch 685, Average Loss: 14.292642593383789, Average MAE: 1.412456750869751, Learning Rate: 0.16384000000000007\n",
      "Epoch 686, Average Loss: 14.281335830688477, Average MAE: 1.4156802892684937, Learning Rate: 0.16384000000000007\n",
      "Epoch 687, Average Loss: 14.27392292022705, Average MAE: 1.4159549474716187, Learning Rate: 0.16384000000000007\n",
      "Epoch 688, Average Loss: 14.270979881286621, Average MAE: 1.414728045463562, Learning Rate: 0.16384000000000007\n",
      "Epoch 689, Average Loss: 14.26654052734375, Average MAE: 1.40938401222229, Learning Rate: 0.16384000000000007\n",
      "Epoch 690, Average Loss: 14.276105880737305, Average MAE: 1.4111956357955933, Learning Rate: 0.16384000000000007\n",
      "Epoch 691, Average Loss: 14.266693115234375, Average MAE: 1.4069206714630127, Learning Rate: 0.13107200000000005\n",
      "Epoch 692, Average Loss: 14.248794555664062, Average MAE: 1.4085302352905273, Learning Rate: 0.13107200000000005\n",
      "Epoch 693, Average Loss: 14.278964042663574, Average MAE: 1.413101315498352, Learning Rate: 0.13107200000000005\n",
      "Epoch 694, Average Loss: 14.279289245605469, Average MAE: 1.4087462425231934, Learning Rate: 0.13107200000000005\n",
      "Epoch 695, Average Loss: 14.263139724731445, Average MAE: 1.4065302610397339, Learning Rate: 0.13107200000000005\n",
      "Epoch 696, Average Loss: 14.270426750183105, Average MAE: 1.4104493856430054, Learning Rate: 0.13107200000000005\n",
      "Epoch 697, Average Loss: 14.261597633361816, Average MAE: 1.4091627597808838, Learning Rate: 0.13107200000000005\n",
      "Epoch 698, Average Loss: 14.277074813842773, Average MAE: 1.414793848991394, Learning Rate: 0.13107200000000005\n",
      "Epoch 699, Average Loss: 14.251091957092285, Average MAE: 1.4059128761291504, Learning Rate: 0.13107200000000005\n",
      "Epoch 700, Average Loss: 14.250178337097168, Average MAE: 1.4008914232254028, Learning Rate: 0.13107200000000005\n",
      "Epoch 701, Average Loss: 14.283926010131836, Average MAE: 1.4071946144104004, Learning Rate: 0.13107200000000005\n",
      "Epoch 702, Average Loss: 14.258615493774414, Average MAE: 1.4044990539550781, Learning Rate: 0.13107200000000005\n",
      "Epoch 703, Average Loss: 14.238799095153809, Average MAE: 1.4028083086013794, Learning Rate: 0.13107200000000005\n",
      "Epoch 704, Average Loss: 14.267380714416504, Average MAE: 1.4068167209625244, Learning Rate: 0.13107200000000005\n",
      "Epoch 705, Average Loss: 14.230401039123535, Average MAE: 1.4018771648406982, Learning Rate: 0.13107200000000005\n",
      "Epoch 706, Average Loss: 14.290422439575195, Average MAE: 1.4069697856903076, Learning Rate: 0.13107200000000005\n",
      "Epoch 707, Average Loss: 14.24232292175293, Average MAE: 1.395902156829834, Learning Rate: 0.13107200000000005\n",
      "Epoch 708, Average Loss: 14.255963325500488, Average MAE: 1.4082151651382446, Learning Rate: 0.13107200000000005\n",
      "Epoch 709, Average Loss: 14.231307983398438, Average MAE: 1.403853178024292, Learning Rate: 0.13107200000000005\n",
      "Epoch 710, Average Loss: 14.256391525268555, Average MAE: 1.4019463062286377, Learning Rate: 0.13107200000000005\n",
      "Epoch 711, Average Loss: 14.261542320251465, Average MAE: 1.4024869203567505, Learning Rate: 0.13107200000000005\n",
      "Epoch 712, Average Loss: 14.260024070739746, Average MAE: 1.4008907079696655, Learning Rate: 0.13107200000000005\n",
      "Epoch 713, Average Loss: 14.254796028137207, Average MAE: 1.403754711151123, Learning Rate: 0.13107200000000005\n",
      "Epoch 714, Average Loss: 14.242009162902832, Average MAE: 1.4006541967391968, Learning Rate: 0.13107200000000005\n",
      "Epoch 715, Average Loss: 14.254399299621582, Average MAE: 1.4008846282958984, Learning Rate: 0.13107200000000005\n",
      "Epoch 716, Average Loss: 14.237454414367676, Average MAE: 1.398927927017212, Learning Rate: 0.10485760000000005\n",
      "Epoch 717, Average Loss: 14.269706726074219, Average MAE: 1.4024784564971924, Learning Rate: 0.10485760000000005\n",
      "Epoch 718, Average Loss: 14.250776290893555, Average MAE: 1.4028664827346802, Learning Rate: 0.10485760000000005\n",
      "Epoch 719, Average Loss: 14.252551078796387, Average MAE: 1.4010192155838013, Learning Rate: 0.10485760000000005\n",
      "Epoch 720, Average Loss: 14.245551109313965, Average MAE: 1.3994148969650269, Learning Rate: 0.10485760000000005\n",
      "Epoch 721, Average Loss: 14.233444213867188, Average MAE: 1.3930772542953491, Learning Rate: 0.10485760000000005\n",
      "Epoch 722, Average Loss: 14.243775367736816, Average MAE: 1.4042757749557495, Learning Rate: 0.10485760000000005\n",
      "Epoch 723, Average Loss: 14.273801803588867, Average MAE: 1.4005284309387207, Learning Rate: 0.10485760000000005\n",
      "Epoch 724, Average Loss: 14.268890380859375, Average MAE: 1.4017328023910522, Learning Rate: 0.10485760000000005\n",
      "Epoch 725, Average Loss: 14.24892807006836, Average MAE: 1.400351643562317, Learning Rate: 0.10485760000000005\n",
      "Epoch 726, Average Loss: 14.253993034362793, Average MAE: 1.397675633430481, Learning Rate: 0.10485760000000005\n",
      "Epoch 727, Average Loss: 14.24494743347168, Average MAE: 1.395134687423706, Learning Rate: 0.08388608000000004\n",
      "Epoch 728, Average Loss: 14.254956245422363, Average MAE: 1.3960281610488892, Learning Rate: 0.08388608000000004\n",
      "Epoch 729, Average Loss: 14.259098052978516, Average MAE: 1.403037190437317, Learning Rate: 0.08388608000000004\n",
      "Epoch 730, Average Loss: 14.220070838928223, Average MAE: 1.3936700820922852, Learning Rate: 0.08388608000000004\n",
      "Epoch 731, Average Loss: 14.217764854431152, Average MAE: 1.391186237335205, Learning Rate: 0.08388608000000004\n",
      "Epoch 732, Average Loss: 14.227145195007324, Average MAE: 1.3917758464813232, Learning Rate: 0.08388608000000004\n",
      "Epoch 733, Average Loss: 14.236204147338867, Average MAE: 1.3950308561325073, Learning Rate: 0.08388608000000004\n",
      "Epoch 734, Average Loss: 14.238347053527832, Average MAE: 1.3937829732894897, Learning Rate: 0.08388608000000004\n",
      "Epoch 735, Average Loss: 14.227606773376465, Average MAE: 1.3953337669372559, Learning Rate: 0.08388608000000004\n",
      "Epoch 736, Average Loss: 14.268298149108887, Average MAE: 1.3985520601272583, Learning Rate: 0.08388608000000004\n",
      "Epoch 737, Average Loss: 14.211861610412598, Average MAE: 1.393739938735962, Learning Rate: 0.08388608000000004\n",
      "Epoch 738, Average Loss: 14.219313621520996, Average MAE: 1.3910419940948486, Learning Rate: 0.08388608000000004\n",
      "Epoch 739, Average Loss: 14.257645606994629, Average MAE: 1.394443392753601, Learning Rate: 0.08388608000000004\n",
      "Epoch 740, Average Loss: 14.221710205078125, Average MAE: 1.3937195539474487, Learning Rate: 0.08388608000000004\n",
      "Epoch 741, Average Loss: 14.219986915588379, Average MAE: 1.3922340869903564, Learning Rate: 0.08388608000000004\n",
      "Epoch 742, Average Loss: 14.232843399047852, Average MAE: 1.3945960998535156, Learning Rate: 0.08388608000000004\n",
      "Epoch 743, Average Loss: 14.214678764343262, Average MAE: 1.3931899070739746, Learning Rate: 0.08388608000000004\n",
      "Epoch 744, Average Loss: 14.221915245056152, Average MAE: 1.3902568817138672, Learning Rate: 0.08388608000000004\n",
      "Epoch 745, Average Loss: 14.218419075012207, Average MAE: 1.3893299102783203, Learning Rate: 0.08388608000000004\n",
      "Epoch 746, Average Loss: 14.22739315032959, Average MAE: 1.3929857015609741, Learning Rate: 0.08388608000000004\n",
      "Epoch 747, Average Loss: 14.232416152954102, Average MAE: 1.3939765691757202, Learning Rate: 0.08388608000000004\n",
      "Epoch 748, Average Loss: 14.246729850769043, Average MAE: 1.3887923955917358, Learning Rate: 0.06710886400000003\n",
      "Epoch 749, Average Loss: 14.229796409606934, Average MAE: 1.3897731304168701, Learning Rate: 0.06710886400000003\n",
      "Epoch 750, Average Loss: 14.258894920349121, Average MAE: 1.3893476724624634, Learning Rate: 0.06710886400000003\n",
      "Epoch 751, Average Loss: 14.21825885772705, Average MAE: 1.388388991355896, Learning Rate: 0.06710886400000003\n",
      "Epoch 752, Average Loss: 14.226520538330078, Average MAE: 1.3895490169525146, Learning Rate: 0.06710886400000003\n",
      "Epoch 753, Average Loss: 14.220999717712402, Average MAE: 1.3905929327011108, Learning Rate: 0.06710886400000003\n",
      "Epoch 754, Average Loss: 14.222007751464844, Average MAE: 1.3895785808563232, Learning Rate: 0.06710886400000003\n",
      "Epoch 755, Average Loss: 14.222737312316895, Average MAE: 1.3918808698654175, Learning Rate: 0.06710886400000003\n",
      "Epoch 756, Average Loss: 14.219218254089355, Average MAE: 1.3905010223388672, Learning Rate: 0.06710886400000003\n",
      "Epoch 757, Average Loss: 14.233484268188477, Average MAE: 1.3860069513320923, Learning Rate: 0.06710886400000003\n",
      "Epoch 758, Average Loss: 14.21353816986084, Average MAE: 1.387853741645813, Learning Rate: 0.06710886400000003\n",
      "Epoch 759, Average Loss: 14.236518859863281, Average MAE: 1.3844460248947144, Learning Rate: 0.05368709120000003\n",
      "Epoch 760, Average Loss: 14.226082801818848, Average MAE: 1.3876888751983643, Learning Rate: 0.05368709120000003\n",
      "Epoch 761, Average Loss: 14.195496559143066, Average MAE: 1.3817458152770996, Learning Rate: 0.05368709120000003\n",
      "Epoch 762, Average Loss: 14.185526847839355, Average MAE: 1.3834527730941772, Learning Rate: 0.05368709120000003\n",
      "Epoch 763, Average Loss: 14.231319427490234, Average MAE: 1.3917534351348877, Learning Rate: 0.05368709120000003\n",
      "Epoch 764, Average Loss: 14.220014572143555, Average MAE: 1.3880016803741455, Learning Rate: 0.05368709120000003\n",
      "Epoch 765, Average Loss: 14.233322143554688, Average MAE: 1.391903281211853, Learning Rate: 0.05368709120000003\n",
      "Epoch 766, Average Loss: 14.198999404907227, Average MAE: 1.3891044855117798, Learning Rate: 0.05368709120000003\n",
      "Epoch 767, Average Loss: 14.197617530822754, Average MAE: 1.3829525709152222, Learning Rate: 0.05368709120000003\n",
      "Epoch 768, Average Loss: 14.212358474731445, Average MAE: 1.3818249702453613, Learning Rate: 0.05368709120000003\n",
      "Epoch 769, Average Loss: 14.226938247680664, Average MAE: 1.3869644403457642, Learning Rate: 0.05368709120000003\n",
      "Epoch 770, Average Loss: 14.207825660705566, Average MAE: 1.3854165077209473, Learning Rate: 0.05368709120000003\n",
      "Epoch 771, Average Loss: 14.20982551574707, Average MAE: 1.3839153051376343, Learning Rate: 0.05368709120000003\n",
      "Epoch 772, Average Loss: 14.22887897491455, Average MAE: 1.3869773149490356, Learning Rate: 0.05368709120000003\n",
      "Epoch 773, Average Loss: 14.22523307800293, Average MAE: 1.3861827850341797, Learning Rate: 0.042949672960000025\n",
      "Epoch 774, Average Loss: 14.214635848999023, Average MAE: 1.3869532346725464, Learning Rate: 0.042949672960000025\n",
      "Epoch 775, Average Loss: 14.206192970275879, Average MAE: 1.3870456218719482, Learning Rate: 0.042949672960000025\n",
      "Epoch 776, Average Loss: 14.237961769104004, Average MAE: 1.3893576860427856, Learning Rate: 0.042949672960000025\n",
      "Epoch 777, Average Loss: 14.232266426086426, Average MAE: 1.3846033811569214, Learning Rate: 0.042949672960000025\n",
      "Epoch 778, Average Loss: 14.216662406921387, Average MAE: 1.38753080368042, Learning Rate: 0.042949672960000025\n",
      "Epoch 779, Average Loss: 14.192773818969727, Average MAE: 1.3849084377288818, Learning Rate: 0.042949672960000025\n",
      "Epoch 780, Average Loss: 14.218711853027344, Average MAE: 1.3845808506011963, Learning Rate: 0.042949672960000025\n",
      "Epoch 781, Average Loss: 14.214971542358398, Average MAE: 1.3842042684555054, Learning Rate: 0.042949672960000025\n",
      "Epoch 782, Average Loss: 14.198532104492188, Average MAE: 1.3831318616867065, Learning Rate: 0.042949672960000025\n",
      "Epoch 783, Average Loss: 14.222626686096191, Average MAE: 1.3879050016403198, Learning Rate: 0.042949672960000025\n",
      "Epoch 784, Average Loss: 14.209699630737305, Average MAE: 1.3870009183883667, Learning Rate: 0.03435973836800002\n",
      "Epoch 785, Average Loss: 14.228340148925781, Average MAE: 1.384000539779663, Learning Rate: 0.03435973836800002\n",
      "Epoch 786, Average Loss: 14.210433006286621, Average MAE: 1.3816391229629517, Learning Rate: 0.03435973836800002\n",
      "Epoch 787, Average Loss: 14.229894638061523, Average MAE: 1.3877253532409668, Learning Rate: 0.03435973836800002\n",
      "Epoch 788, Average Loss: 14.198235511779785, Average MAE: 1.3811177015304565, Learning Rate: 0.03435973836800002\n",
      "Epoch 789, Average Loss: 14.238725662231445, Average MAE: 1.3865028619766235, Learning Rate: 0.03435973836800002\n",
      "Epoch 790, Average Loss: 14.22138500213623, Average MAE: 1.3847905397415161, Learning Rate: 0.03435973836800002\n",
      "Epoch 791, Average Loss: 14.213324546813965, Average MAE: 1.3866214752197266, Learning Rate: 0.03435973836800002\n",
      "Epoch 792, Average Loss: 14.230884552001953, Average MAE: 1.3865139484405518, Learning Rate: 0.03435973836800002\n",
      "Epoch 793, Average Loss: 14.23953914642334, Average MAE: 1.382318377494812, Learning Rate: 0.03435973836800002\n",
      "Epoch 794, Average Loss: 14.226731300354004, Average MAE: 1.3848912715911865, Learning Rate: 0.03435973836800002\n",
      "Epoch 795, Average Loss: 14.203625679016113, Average MAE: 1.3863741159439087, Learning Rate: 0.027487790694400018\n",
      "Epoch 796, Average Loss: 14.222993850708008, Average MAE: 1.3835172653198242, Learning Rate: 0.027487790694400018\n",
      "Epoch 797, Average Loss: 14.209132194519043, Average MAE: 1.3820267915725708, Learning Rate: 0.027487790694400018\n",
      "Epoch 798, Average Loss: 14.220402717590332, Average MAE: 1.3857567310333252, Learning Rate: 0.027487790694400018\n",
      "Epoch 799, Average Loss: 14.20065975189209, Average MAE: 1.3815503120422363, Learning Rate: 0.027487790694400018\n",
      "Epoch 800, Average Loss: 14.23326587677002, Average MAE: 1.38291335105896, Learning Rate: 0.027487790694400018\n",
      "Epoch 801, Average Loss: 14.203566551208496, Average MAE: 1.3889986276626587, Learning Rate: 0.027487790694400018\n",
      "Epoch 802, Average Loss: 14.225228309631348, Average MAE: 1.387129783630371, Learning Rate: 0.027487790694400018\n",
      "Epoch 803, Average Loss: 14.229342460632324, Average MAE: 1.38926100730896, Learning Rate: 0.027487790694400018\n",
      "Epoch 804, Average Loss: 14.20660400390625, Average MAE: 1.3776123523712158, Learning Rate: 0.027487790694400018\n",
      "Epoch 805, Average Loss: 14.20223331451416, Average MAE: 1.3849180936813354, Learning Rate: 0.027487790694400018\n",
      "Epoch 806, Average Loss: 14.228713035583496, Average MAE: 1.3892371654510498, Learning Rate: 0.021990232555520017\n",
      "Epoch 807, Average Loss: 14.194846153259277, Average MAE: 1.380254864692688, Learning Rate: 0.021990232555520017\n",
      "Epoch 808, Average Loss: 14.229206085205078, Average MAE: 1.382007360458374, Learning Rate: 0.021990232555520017\n",
      "Epoch 809, Average Loss: 14.210467338562012, Average MAE: 1.3805515766143799, Learning Rate: 0.021990232555520017\n",
      "Epoch 810, Average Loss: 14.187955856323242, Average MAE: 1.3793246746063232, Learning Rate: 0.021990232555520017\n",
      "Epoch 811, Average Loss: 14.226664543151855, Average MAE: 1.386989951133728, Learning Rate: 0.021990232555520017\n",
      "Epoch 812, Average Loss: 14.19803524017334, Average MAE: 1.3793939352035522, Learning Rate: 0.021990232555520017\n",
      "Epoch 813, Average Loss: 14.193367958068848, Average MAE: 1.3772395849227905, Learning Rate: 0.021990232555520017\n",
      "Epoch 814, Average Loss: 14.231082916259766, Average MAE: 1.387028455734253, Learning Rate: 0.021990232555520017\n",
      "Epoch 815, Average Loss: 14.227212905883789, Average MAE: 1.3803983926773071, Learning Rate: 0.021990232555520017\n",
      "Epoch 816, Average Loss: 14.226959228515625, Average MAE: 1.383155345916748, Learning Rate: 0.021990232555520017\n",
      "Epoch 817, Average Loss: 14.21618938446045, Average MAE: 1.3805781602859497, Learning Rate: 0.017592186044416015\n",
      "Epoch 818, Average Loss: 14.210047721862793, Average MAE: 1.3798744678497314, Learning Rate: 0.017592186044416015\n",
      "Epoch 819, Average Loss: 14.21703815460205, Average MAE: 1.3837792873382568, Learning Rate: 0.017592186044416015\n",
      "Epoch 820, Average Loss: 14.20261287689209, Average MAE: 1.3787870407104492, Learning Rate: 0.017592186044416015\n",
      "Epoch 821, Average Loss: 14.221298217773438, Average MAE: 1.3860244750976562, Learning Rate: 0.017592186044416015\n",
      "Epoch 822, Average Loss: 14.213051795959473, Average MAE: 1.3843817710876465, Learning Rate: 0.017592186044416015\n",
      "Epoch 823, Average Loss: 14.21480941772461, Average MAE: 1.3810031414031982, Learning Rate: 0.017592186044416015\n",
      "Epoch 824, Average Loss: 14.232483863830566, Average MAE: 1.3851900100708008, Learning Rate: 0.017592186044416015\n",
      "Epoch 825, Average Loss: 14.219991683959961, Average MAE: 1.3854460716247559, Learning Rate: 0.017592186044416015\n",
      "Epoch 826, Average Loss: 14.19756031036377, Average MAE: 1.3794093132019043, Learning Rate: 0.017592186044416015\n",
      "Epoch 827, Average Loss: 14.186874389648438, Average MAE: 1.3786890506744385, Learning Rate: 0.017592186044416015\n",
      "Epoch 828, Average Loss: 14.212363243103027, Average MAE: 1.3827803134918213, Learning Rate: 0.014073748835532814\n",
      "Epoch 829, Average Loss: 14.182819366455078, Average MAE: 1.3762682676315308, Learning Rate: 0.014073748835532814\n",
      "Epoch 830, Average Loss: 14.210775375366211, Average MAE: 1.3810150623321533, Learning Rate: 0.014073748835532814\n",
      "Epoch 831, Average Loss: 14.208318710327148, Average MAE: 1.3837766647338867, Learning Rate: 0.014073748835532814\n",
      "Epoch 832, Average Loss: 14.183223724365234, Average MAE: 1.374208927154541, Learning Rate: 0.014073748835532814\n",
      "Epoch 833, Average Loss: 14.206093788146973, Average MAE: 1.3786174058914185, Learning Rate: 0.014073748835532814\n",
      "Epoch 834, Average Loss: 14.211648941040039, Average MAE: 1.3761056661605835, Learning Rate: 0.014073748835532814\n",
      "Epoch 835, Average Loss: 14.175463676452637, Average MAE: 1.374645709991455, Learning Rate: 0.014073748835532814\n",
      "Epoch 836, Average Loss: 14.228866577148438, Average MAE: 1.3822367191314697, Learning Rate: 0.014073748835532814\n",
      "Epoch 837, Average Loss: 14.222464561462402, Average MAE: 1.3810228109359741, Learning Rate: 0.014073748835532814\n",
      "Epoch 838, Average Loss: 14.195772171020508, Average MAE: 1.3800897598266602, Learning Rate: 0.014073748835532814\n",
      "Epoch 839, Average Loss: 14.218179702758789, Average MAE: 1.3800445795059204, Learning Rate: 0.014073748835532814\n",
      "Epoch 840, Average Loss: 14.22398567199707, Average MAE: 1.3791571855545044, Learning Rate: 0.014073748835532814\n",
      "Epoch 841, Average Loss: 14.20455265045166, Average MAE: 1.3763325214385986, Learning Rate: 0.014073748835532814\n",
      "Epoch 842, Average Loss: 14.206521987915039, Average MAE: 1.3809632062911987, Learning Rate: 0.014073748835532814\n",
      "Epoch 843, Average Loss: 14.220304489135742, Average MAE: 1.386371374130249, Learning Rate: 0.014073748835532814\n",
      "Epoch 844, Average Loss: 14.19693374633789, Average MAE: 1.3743948936462402, Learning Rate: 0.014073748835532814\n",
      "Epoch 845, Average Loss: 14.194812774658203, Average MAE: 1.3819503784179688, Learning Rate: 0.014073748835532814\n",
      "Epoch 846, Average Loss: 14.222474098205566, Average MAE: 1.381505012512207, Learning Rate: 0.011258999068426251\n",
      "Epoch 847, Average Loss: 14.199459075927734, Average MAE: 1.3832106590270996, Learning Rate: 0.011258999068426251\n",
      "Epoch 848, Average Loss: 14.196669578552246, Average MAE: 1.3787418603897095, Learning Rate: 0.011258999068426251\n",
      "Epoch 849, Average Loss: 14.189419746398926, Average MAE: 1.3799880743026733, Learning Rate: 0.011258999068426251\n",
      "Epoch 850, Average Loss: 14.203619956970215, Average MAE: 1.380405068397522, Learning Rate: 0.011258999068426251\n",
      "Epoch 851, Average Loss: 14.17949390411377, Average MAE: 1.3774813413619995, Learning Rate: 0.011258999068426251\n",
      "Epoch 852, Average Loss: 14.19046688079834, Average MAE: 1.3807339668273926, Learning Rate: 0.011258999068426251\n",
      "Epoch 853, Average Loss: 14.200469970703125, Average MAE: 1.3796831369400024, Learning Rate: 0.011258999068426251\n",
      "Epoch 854, Average Loss: 14.193840980529785, Average MAE: 1.3765921592712402, Learning Rate: 0.011258999068426251\n",
      "Epoch 855, Average Loss: 14.180543899536133, Average MAE: 1.3779003620147705, Learning Rate: 0.011258999068426251\n",
      "Epoch 856, Average Loss: 14.176095962524414, Average MAE: 1.379947543144226, Learning Rate: 0.011258999068426251\n",
      "Epoch 857, Average Loss: 14.18954086303711, Average MAE: 1.379280924797058, Learning Rate: 0.009007199254741001\n",
      "Epoch 858, Average Loss: 14.207015991210938, Average MAE: 1.3823966979980469, Learning Rate: 0.009007199254741001\n",
      "Epoch 859, Average Loss: 14.207171440124512, Average MAE: 1.3739981651306152, Learning Rate: 0.009007199254741001\n",
      "Epoch 860, Average Loss: 14.226333618164062, Average MAE: 1.380632758140564, Learning Rate: 0.009007199254741001\n",
      "Epoch 861, Average Loss: 14.22057056427002, Average MAE: 1.3797661066055298, Learning Rate: 0.009007199254741001\n",
      "Epoch 862, Average Loss: 14.189231872558594, Average MAE: 1.3786380290985107, Learning Rate: 0.009007199254741001\n",
      "Epoch 863, Average Loss: 14.203157424926758, Average MAE: 1.383849024772644, Learning Rate: 0.009007199254741001\n",
      "Epoch 864, Average Loss: 14.18090534210205, Average MAE: 1.3771415948867798, Learning Rate: 0.009007199254741001\n",
      "Epoch 865, Average Loss: 14.198193550109863, Average MAE: 1.3770146369934082, Learning Rate: 0.009007199254741001\n",
      "Epoch 866, Average Loss: 14.207931518554688, Average MAE: 1.3805214166641235, Learning Rate: 0.009007199254741001\n",
      "Epoch 867, Average Loss: 14.187313079833984, Average MAE: 1.3770939111709595, Learning Rate: 0.009007199254741001\n",
      "Epoch 868, Average Loss: 14.230426788330078, Average MAE: 1.3822190761566162, Learning Rate: 0.007205759403792801\n",
      "Epoch 869, Average Loss: 14.213446617126465, Average MAE: 1.3783358335494995, Learning Rate: 0.007205759403792801\n",
      "Epoch 870, Average Loss: 14.200766563415527, Average MAE: 1.3788617849349976, Learning Rate: 0.007205759403792801\n",
      "Epoch 871, Average Loss: 14.208205223083496, Average MAE: 1.380300521850586, Learning Rate: 0.007205759403792801\n",
      "Epoch 872, Average Loss: 14.198259353637695, Average MAE: 1.3801484107971191, Learning Rate: 0.007205759403792801\n",
      "Epoch 873, Average Loss: 14.22043514251709, Average MAE: 1.3845287561416626, Learning Rate: 0.007205759403792801\n",
      "Epoch 874, Average Loss: 14.193399429321289, Average MAE: 1.377657413482666, Learning Rate: 0.007205759403792801\n",
      "Epoch 875, Average Loss: 14.225735664367676, Average MAE: 1.3865514993667603, Learning Rate: 0.007205759403792801\n",
      "Epoch 876, Average Loss: 14.195893287658691, Average MAE: 1.3767883777618408, Learning Rate: 0.007205759403792801\n",
      "Epoch 877, Average Loss: 14.186729431152344, Average MAE: 1.3768688440322876, Learning Rate: 0.007205759403792801\n",
      "Epoch 878, Average Loss: 14.209660530090332, Average MAE: 1.3801299333572388, Learning Rate: 0.007205759403792801\n",
      "Epoch 879, Average Loss: 14.217418670654297, Average MAE: 1.3821104764938354, Learning Rate: 0.0057646075230342415\n",
      "Epoch 880, Average Loss: 14.198148727416992, Average MAE: 1.3779557943344116, Learning Rate: 0.0057646075230342415\n",
      "Epoch 881, Average Loss: 14.21017074584961, Average MAE: 1.3805698156356812, Learning Rate: 0.0057646075230342415\n",
      "Epoch 882, Average Loss: 14.220736503601074, Average MAE: 1.3805493116378784, Learning Rate: 0.0057646075230342415\n",
      "Epoch 883, Average Loss: 14.214756965637207, Average MAE: 1.3789536952972412, Learning Rate: 0.0057646075230342415\n",
      "Epoch 884, Average Loss: 14.212265014648438, Average MAE: 1.3817789554595947, Learning Rate: 0.0057646075230342415\n",
      "Epoch 885, Average Loss: 14.1936616897583, Average MAE: 1.379144549369812, Learning Rate: 0.0057646075230342415\n",
      "Epoch 886, Average Loss: 14.195195198059082, Average MAE: 1.3802387714385986, Learning Rate: 0.0057646075230342415\n",
      "Epoch 887, Average Loss: 14.221908569335938, Average MAE: 1.385056734085083, Learning Rate: 0.0057646075230342415\n",
      "Epoch 888, Average Loss: 14.207138061523438, Average MAE: 1.3814102411270142, Learning Rate: 0.0057646075230342415\n",
      "Epoch 889, Average Loss: 14.204821586608887, Average MAE: 1.376260757446289, Learning Rate: 0.0057646075230342415\n",
      "Epoch 890, Average Loss: 14.189684867858887, Average MAE: 1.3777241706848145, Learning Rate: 0.0046116860184273935\n",
      "Epoch 891, Average Loss: 14.179821014404297, Average MAE: 1.37869131565094, Learning Rate: 0.0046116860184273935\n",
      "Epoch 892, Average Loss: 14.197264671325684, Average MAE: 1.3807305097579956, Learning Rate: 0.0046116860184273935\n",
      "Epoch 893, Average Loss: 14.211414337158203, Average MAE: 1.380323886871338, Learning Rate: 0.0046116860184273935\n",
      "Epoch 894, Average Loss: 14.184696197509766, Average MAE: 1.3774219751358032, Learning Rate: 0.0046116860184273935\n",
      "Epoch 895, Average Loss: 14.191060066223145, Average MAE: 1.376961350440979, Learning Rate: 0.0046116860184273935\n",
      "Epoch 896, Average Loss: 14.180436134338379, Average MAE: 1.3769086599349976, Learning Rate: 0.0046116860184273935\n",
      "Epoch 897, Average Loss: 14.198969841003418, Average MAE: 1.3814191818237305, Learning Rate: 0.0046116860184273935\n",
      "Epoch 898, Average Loss: 14.207479476928711, Average MAE: 1.3796097040176392, Learning Rate: 0.0046116860184273935\n",
      "Epoch 899, Average Loss: 14.199383735656738, Average MAE: 1.3795558214187622, Learning Rate: 0.0046116860184273935\n",
      "Epoch 900, Average Loss: 14.184316635131836, Average MAE: 1.3791489601135254, Learning Rate: 0.0046116860184273935\n",
      "Epoch 901, Average Loss: 14.208778381347656, Average MAE: 1.3791999816894531, Learning Rate: 0.003689348814741915\n",
      "Epoch 902, Average Loss: 14.170007705688477, Average MAE: 1.373861312866211, Learning Rate: 0.003689348814741915\n",
      "Epoch 903, Average Loss: 14.202054023742676, Average MAE: 1.3797926902770996, Learning Rate: 0.003689348814741915\n",
      "Epoch 904, Average Loss: 14.202079772949219, Average MAE: 1.3795723915100098, Learning Rate: 0.003689348814741915\n",
      "Epoch 905, Average Loss: 14.190909385681152, Average MAE: 1.3765608072280884, Learning Rate: 0.003689348814741915\n",
      "Epoch 906, Average Loss: 14.225666046142578, Average MAE: 1.3811827898025513, Learning Rate: 0.003689348814741915\n",
      "Epoch 907, Average Loss: 14.210938453674316, Average MAE: 1.3792858123779297, Learning Rate: 0.003689348814741915\n",
      "Epoch 908, Average Loss: 14.197132110595703, Average MAE: 1.3788597583770752, Learning Rate: 0.003689348814741915\n",
      "Epoch 909, Average Loss: 14.202128410339355, Average MAE: 1.3812487125396729, Learning Rate: 0.003689348814741915\n",
      "Epoch 910, Average Loss: 14.223877906799316, Average MAE: 1.3795535564422607, Learning Rate: 0.003689348814741915\n",
      "Epoch 911, Average Loss: 14.190293312072754, Average MAE: 1.3800346851348877, Learning Rate: 0.003689348814741915\n",
      "Epoch 912, Average Loss: 14.194465637207031, Average MAE: 1.3793023824691772, Learning Rate: 0.003689348814741915\n",
      "Epoch 913, Average Loss: 14.21679401397705, Average MAE: 1.3778444528579712, Learning Rate: 0.002951479051793532\n",
      "Epoch 914, Average Loss: 14.195902824401855, Average MAE: 1.3781052827835083, Learning Rate: 0.002951479051793532\n",
      "Epoch 915, Average Loss: 14.20670223236084, Average MAE: 1.3805949687957764, Learning Rate: 0.002951479051793532\n",
      "Epoch 916, Average Loss: 14.211750984191895, Average MAE: 1.3806942701339722, Learning Rate: 0.002951479051793532\n",
      "Epoch 917, Average Loss: 14.213127136230469, Average MAE: 1.3805078268051147, Learning Rate: 0.002951479051793532\n",
      "Epoch 918, Average Loss: 14.19613265991211, Average MAE: 1.377759337425232, Learning Rate: 0.002951479051793532\n",
      "Epoch 919, Average Loss: 14.178033828735352, Average MAE: 1.3740911483764648, Learning Rate: 0.002951479051793532\n",
      "Epoch 920, Average Loss: 14.206525802612305, Average MAE: 1.3782007694244385, Learning Rate: 0.002951479051793532\n",
      "Epoch 921, Average Loss: 14.185220718383789, Average MAE: 1.3783478736877441, Learning Rate: 0.002951479051793532\n",
      "Epoch 922, Average Loss: 14.20556926727295, Average MAE: 1.3796191215515137, Learning Rate: 0.002951479051793532\n",
      "Epoch 923, Average Loss: 14.195517539978027, Average MAE: 1.381028652191162, Learning Rate: 0.002951479051793532\n",
      "Epoch 924, Average Loss: 14.195098876953125, Average MAE: 1.3837369680404663, Learning Rate: 0.002361183241434826\n",
      "Epoch 925, Average Loss: 14.189484596252441, Average MAE: 1.3775897026062012, Learning Rate: 0.002361183241434826\n",
      "Epoch 926, Average Loss: 14.198139190673828, Average MAE: 1.37941312789917, Learning Rate: 0.002361183241434826\n",
      "Epoch 927, Average Loss: 14.190766334533691, Average MAE: 1.3752495050430298, Learning Rate: 0.002361183241434826\n",
      "Epoch 928, Average Loss: 14.169028282165527, Average MAE: 1.3709838390350342, Learning Rate: 0.002361183241434826\n",
      "Epoch 929, Average Loss: 14.192296981811523, Average MAE: 1.3777885437011719, Learning Rate: 0.002361183241434826\n",
      "Epoch 930, Average Loss: 14.177868843078613, Average MAE: 1.3745044469833374, Learning Rate: 0.002361183241434826\n",
      "Epoch 931, Average Loss: 14.2070894241333, Average MAE: 1.3781057596206665, Learning Rate: 0.002361183241434826\n",
      "Epoch 932, Average Loss: 14.201245307922363, Average MAE: 1.3800653219223022, Learning Rate: 0.002361183241434826\n",
      "Epoch 933, Average Loss: 14.185787200927734, Average MAE: 1.3755669593811035, Learning Rate: 0.002361183241434826\n",
      "Epoch 934, Average Loss: 14.193791389465332, Average MAE: 1.377481460571289, Learning Rate: 0.002361183241434826\n",
      "Epoch 935, Average Loss: 14.163243293762207, Average MAE: 1.3764503002166748, Learning Rate: 0.002361183241434826\n",
      "Epoch 936, Average Loss: 14.189654350280762, Average MAE: 1.3818767070770264, Learning Rate: 0.002361183241434826\n",
      "Epoch 937, Average Loss: 14.206740379333496, Average MAE: 1.3751734495162964, Learning Rate: 0.002361183241434826\n",
      "Epoch 938, Average Loss: 14.192999839782715, Average MAE: 1.376880407333374, Learning Rate: 0.002361183241434826\n",
      "Epoch 939, Average Loss: 14.21342945098877, Average MAE: 1.381688117980957, Learning Rate: 0.002361183241434826\n",
      "Epoch 940, Average Loss: 14.188730239868164, Average MAE: 1.3763933181762695, Learning Rate: 0.002361183241434826\n",
      "Epoch 941, Average Loss: 14.188928604125977, Average MAE: 1.3764371871948242, Learning Rate: 0.002361183241434826\n",
      "Epoch 942, Average Loss: 14.187848091125488, Average MAE: 1.3781917095184326, Learning Rate: 0.002361183241434826\n",
      "Epoch 943, Average Loss: 14.187681198120117, Average MAE: 1.3770965337753296, Learning Rate: 0.002361183241434826\n",
      "Epoch 944, Average Loss: 14.203699111938477, Average MAE: 1.3790981769561768, Learning Rate: 0.002361183241434826\n",
      "Epoch 945, Average Loss: 14.195968627929688, Average MAE: 1.3734854459762573, Learning Rate: 0.002361183241434826\n",
      "Epoch 946, Average Loss: 14.201456069946289, Average MAE: 1.379746913909912, Learning Rate: 0.0018889465931478608\n",
      "Epoch 947, Average Loss: 14.230316162109375, Average MAE: 1.3838839530944824, Learning Rate: 0.0018889465931478608\n",
      "Epoch 948, Average Loss: 14.224516868591309, Average MAE: 1.3791323900222778, Learning Rate: 0.0018889465931478608\n",
      "Epoch 949, Average Loss: 14.19756031036377, Average MAE: 1.3838123083114624, Learning Rate: 0.0018889465931478608\n",
      "Epoch 950, Average Loss: 14.198399543762207, Average MAE: 1.3797366619110107, Learning Rate: 0.0018889465931478608\n",
      "Epoch 951, Average Loss: 14.207178115844727, Average MAE: 1.3776310682296753, Learning Rate: 0.0018889465931478608\n",
      "Epoch 952, Average Loss: 14.210304260253906, Average MAE: 1.3761776685714722, Learning Rate: 0.0018889465931478608\n",
      "Epoch 953, Average Loss: 14.207944869995117, Average MAE: 1.3801352977752686, Learning Rate: 0.0018889465931478608\n",
      "Epoch 954, Average Loss: 14.191253662109375, Average MAE: 1.3820205926895142, Learning Rate: 0.0018889465931478608\n",
      "Epoch 955, Average Loss: 14.221763610839844, Average MAE: 1.382765531539917, Learning Rate: 0.0018889465931478608\n",
      "Epoch 956, Average Loss: 14.218291282653809, Average MAE: 1.3794413805007935, Learning Rate: 0.0018889465931478608\n",
      "Epoch 957, Average Loss: 14.201542854309082, Average MAE: 1.377974271774292, Learning Rate: 0.0015111572745182887\n",
      "Epoch 958, Average Loss: 14.215864181518555, Average MAE: 1.3845152854919434, Learning Rate: 0.0015111572745182887\n",
      "Epoch 959, Average Loss: 14.199649810791016, Average MAE: 1.380401849746704, Learning Rate: 0.0015111572745182887\n",
      "Epoch 960, Average Loss: 14.211135864257812, Average MAE: 1.3807610273361206, Learning Rate: 0.0015111572745182887\n",
      "Epoch 961, Average Loss: 14.20039176940918, Average MAE: 1.3788260221481323, Learning Rate: 0.0015111572745182887\n",
      "Epoch 962, Average Loss: 14.178504943847656, Average MAE: 1.3740320205688477, Learning Rate: 0.0015111572745182887\n",
      "Epoch 963, Average Loss: 14.200956344604492, Average MAE: 1.374290943145752, Learning Rate: 0.0015111572745182887\n",
      "Epoch 964, Average Loss: 14.206870079040527, Average MAE: 1.3773118257522583, Learning Rate: 0.0015111572745182887\n",
      "Epoch 965, Average Loss: 14.202701568603516, Average MAE: 1.3796782493591309, Learning Rate: 0.0015111572745182887\n",
      "Epoch 966, Average Loss: 14.232259750366211, Average MAE: 1.3810449838638306, Learning Rate: 0.0015111572745182887\n",
      "Epoch 967, Average Loss: 14.20825481414795, Average MAE: 1.3771899938583374, Learning Rate: 0.0015111572745182887\n",
      "Epoch 968, Average Loss: 14.18403148651123, Average MAE: 1.3771569728851318, Learning Rate: 0.001208925819614631\n",
      "Epoch 969, Average Loss: 14.19329833984375, Average MAE: 1.38009774684906, Learning Rate: 0.001208925819614631\n",
      "Epoch 970, Average Loss: 14.217368125915527, Average MAE: 1.3815158605575562, Learning Rate: 0.001208925819614631\n",
      "Epoch 971, Average Loss: 14.193345069885254, Average MAE: 1.3746181726455688, Learning Rate: 0.001208925819614631\n",
      "Epoch 972, Average Loss: 14.186676025390625, Average MAE: 1.3738120794296265, Learning Rate: 0.001208925819614631\n",
      "Epoch 973, Average Loss: 14.213193893432617, Average MAE: 1.3772789239883423, Learning Rate: 0.001208925819614631\n",
      "Epoch 974, Average Loss: 14.18837833404541, Average MAE: 1.3758612871170044, Learning Rate: 0.001208925819614631\n",
      "Epoch 975, Average Loss: 14.194154739379883, Average MAE: 1.381223201751709, Learning Rate: 0.001208925819614631\n",
      "Epoch 976, Average Loss: 14.200057029724121, Average MAE: 1.377555012702942, Learning Rate: 0.001208925819614631\n",
      "Epoch 977, Average Loss: 14.210386276245117, Average MAE: 1.3806654214859009, Learning Rate: 0.001208925819614631\n",
      "Epoch 978, Average Loss: 14.211488723754883, Average MAE: 1.377866506576538, Learning Rate: 0.001208925819614631\n",
      "Epoch 979, Average Loss: 14.183639526367188, Average MAE: 1.3739458322525024, Learning Rate: 0.0009671406556917048\n",
      "Epoch 980, Average Loss: 14.199389457702637, Average MAE: 1.3772896528244019, Learning Rate: 0.0009671406556917048\n",
      "Epoch 981, Average Loss: 14.200246810913086, Average MAE: 1.37575364112854, Learning Rate: 0.0009671406556917048\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epoch_losses = []\n",
    "epoch_maes = []\n",
    "\n",
    "# Set the model to training mode\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    total_mae = 0\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for b in range(num_batches_train):\n",
    "        # Get the batched node features and desired output\n",
    "        node_features_batch = batched_input_train[b].to(device)\n",
    "        desired_output_batch = batched_output_train[b].to(device)\n",
    "        \n",
    "        # Reshape node_features_batch to match the expected input shape for the model\n",
    "        node_features_batch = node_features_batch.view(-1, node_features_batch.size(2)).to(device)\n",
    "        \n",
    "        model_output_batch = model(node_features_batch, edge_index, edge_attr)\n",
    "        \n",
    "        # Reshape model_output_batch back to the original shape\n",
    "        model_output_batch = model_output_batch.view(batched_input_train.size(1), -1, model_output_batch.size(1))\n",
    "        loss = loss_fn(model_output_batch, desired_output_batch)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the optimizer\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute MAE for debugging\n",
    "        mae = torch.mean(torch.abs(model_output_batch - desired_output_batch))\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_mae += mae.item()\n",
    "\n",
    "    \n",
    "    \n",
    "    average_loss = total_loss / num_batches_train\n",
    "    average_mae = total_mae / num_batches_train\n",
    "    epoch_losses.append(average_loss)\n",
    "    epoch_maes.append(average_mae)\n",
    "\n",
    "    scheduler.step(average_loss)\n",
    "\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Epoch {epoch+1}, Average Loss: {average_loss}, Average MAE: {average_mae}, Learning Rate: {current_lr}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
