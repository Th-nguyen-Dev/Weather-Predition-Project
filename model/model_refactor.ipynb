{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv, GraphConv\n",
    "from torch_geometric.nn.norm import BatchNorm\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">**ZE EPIC DATA**</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([72790024141, 72785524114, 72789094197, 72793024233, 72785794129, 72788594266, 72797624217, 72785024157, 72797094240, 72798594276, 72792424223, 72792894263, 72781024243, 72781524237, 72788324220, 72698824219, 72793894274, 74206024207, 72782724110, 72793724222, 72792594227, 72782594239, 72794504205, 72792394225, 72784524163, 72792024227, 72785694176])\n"
     ]
    }
   ],
   "source": [
    "# Directory containing the CSV files\n",
    "directory = '../processed-final-data-2'\n",
    "\n",
    "# Dictionary to store the dataframes\n",
    "dataframes = {}\n",
    "\n",
    "# Iterate over all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        # Extract the file name without extension and convert it to int\n",
    "        key = int(os.path.splitext(filename)[0])\n",
    "        \n",
    "        # Read the CSV file into a dataframe\n",
    "        df = pd.read_csv(os.path.join(directory, filename))\n",
    "        dataframes[key] = df\n",
    "\n",
    "\n",
    "# Print the dictionary keys to verify\n",
    "print(dataframes.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72790024141: Sequential split verified.\n",
      "72785524114: Sequential split verified.\n",
      "72789094197: Sequential split verified.\n",
      "72793024233: Sequential split verified.\n",
      "72785794129: Sequential split verified.\n",
      "72788594266: Sequential split verified.\n",
      "72797624217: Sequential split verified.\n",
      "72785024157: Sequential split verified.\n",
      "72797094240: Sequential split verified.\n",
      "72798594276: Sequential split verified.\n",
      "72792424223: Sequential split verified.\n",
      "72792894263: Sequential split verified.\n",
      "72781024243: Sequential split verified.\n",
      "72781524237: Sequential split verified.\n",
      "72788324220: Sequential split verified.\n",
      "72698824219: Sequential split verified.\n",
      "72793894274: Sequential split verified.\n",
      "74206024207: Sequential split verified.\n",
      "72782724110: Sequential split verified.\n",
      "72793724222: Sequential split verified.\n",
      "72792594227: Sequential split verified.\n",
      "72782594239: Sequential split verified.\n",
      "72794504205: Sequential split verified.\n",
      "72792394225: Sequential split verified.\n",
      "72784524163: Sequential split verified.\n",
      "72792024227: Sequential split verified.\n",
      "72785694176: Sequential split verified.\n",
      "dict_keys([72790024141, 72785524114, 72789094197, 72793024233, 72785794129, 72788594266, 72797624217, 72785024157, 72797094240, 72798594276, 72792424223, 72792894263, 72781024243, 72781524237, 72788324220, 72698824219, 72793894274, 74206024207, 72782724110, 72793724222, 72792594227, 72782594239, 72794504205, 72792394225, 72784524163, 72792024227, 72785694176])\n",
      "dict_keys([72790024141, 72785524114, 72789094197, 72793024233, 72785794129, 72788594266, 72797624217, 72785024157, 72797094240, 72798594276, 72792424223, 72792894263, 72781024243, 72781524237, 72788324220, 72698824219, 72793894274, 74206024207, 72782724110, 72793724222, 72792594227, 72782594239, 72794504205, 72792394225, 72784524163, 72792024227, 72785694176])\n"
     ]
    }
   ],
   "source": [
    "# Dictionaries to store the training and testing dataframes\n",
    "train_dataframes = {}\n",
    "test_dataframes = {}\n",
    "\n",
    "# Split each dataframe into training and testing sets\n",
    "for key, df in dataframes.items():\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, shuffle=False)\n",
    "    train_dataframes[key] = train_df\n",
    "    test_dataframes[key] = test_df\n",
    "    # Check if the maximum index of the training set is less than the minimum index of the testing set\n",
    "    if train_df.index.max() < test_df.index.min():\n",
    "        print(f\"{key}: Sequential split verified.\")\n",
    "    else:\n",
    "        print(f\"{key}: Sequential split NOT verified.\")\n",
    "\n",
    "# Print the keys of the training and testing dictionaries to verify\n",
    "print(train_dataframes.keys())\n",
    "print(test_dataframes.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1408, 27, 70])\n",
      "torch.Size([352, 27, 70])\n"
     ]
    }
   ],
   "source": [
    "def create_node_features_sequences(dataframes):\n",
    "    # Create a list to store the node features for each time step for input and desired output\n",
    "    node_features_sequence_input = []\n",
    "    node_features_sequence_output = []\n",
    "\n",
    "    # Iterate over the rows of the dataframes (assuming all dataframes have the same number of rows)\n",
    "    for i in range(len(next(iter(dataframes.values())))):\n",
    "        if i == len(next(iter(dataframes.values()))) - 1:\n",
    "            break\n",
    "        # Create a list to store the features of all nodes at the current time step for input\n",
    "        node_features_input = []\n",
    "        # Create a list to store the features of all nodes at the next time step for output\n",
    "        node_features_output = []\n",
    "\n",
    "        # Iterate over each dataframe and extract the features at the current row for input\n",
    "        # and the next row for output\n",
    "        for key, df in dataframes.items():\n",
    "            node_features_input.append((df.iloc[i].values - df.iloc[i].mean()) / df.iloc[i].std())\n",
    "            node_features_output.append(df.iloc[i + 1].values)\n",
    "\n",
    "        # Stack the features of all nodes to create a 2D array (num_nodes, num_features)\n",
    "        node_features_sequence_input.append(np.stack(node_features_input))\n",
    "        node_features_sequence_output.append(np.stack(node_features_output))\n",
    "\n",
    "    # Convert the lists to numpy arrays (time_steps, num_nodes, num_features)\n",
    "    node_features_sequence_input = np.array(node_features_sequence_input)\n",
    "    node_features_sequence_output = np.array(node_features_sequence_output)\n",
    "\n",
    "    # Convert the numpy arrays to torch tensors\n",
    "    node_features_sequence_input = torch.tensor(node_features_sequence_input, dtype=torch.float)\n",
    "    node_features_sequence_output = torch.tensor(node_features_sequence_output, dtype=torch.float)\n",
    "\n",
    "    return node_features_sequence_input, node_features_sequence_output\n",
    "\n",
    "# Call the function and print the shapes of the resulting tensors\n",
    "node_features_sequence_input_train, node_features_sequence_output_train = create_node_features_sequences(train_dataframes)\n",
    "node_features_sequence_input_test, node_features_sequence_output_test = create_node_features_sequences(test_dataframes)\n",
    "print(node_features_sequence_input_train.shape)\n",
    "print(node_features_sequence_output_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: tensor([[[ 2.5068, -0.0083,  0.1616,  1.8645,  1.7890,  4.7494, -0.3193,\n",
      "           2.9336,  1.8637, -0.3198,  3.3552, -0.2026, -0.3196, -0.3129,\n",
      "          -0.3196, -0.3186, -0.3197, -0.3197, -0.3197, -0.3197, -0.3196,\n",
      "          -0.3197, -0.3197, -0.3197, -0.3026, -0.2830, -0.3197, -0.3197,\n",
      "          -0.3197, -0.3197, -0.3193, -0.3129, -0.3196, -0.3197, -0.3195,\n",
      "          -0.3192, -0.3195, -0.3197, -0.3197, -0.3197, -0.3185, -0.3197,\n",
      "          -0.3194, -0.3196, -0.3196, -0.3140, -0.3194, -0.3197, -0.3188,\n",
      "          -0.3197, -0.2848, -0.3197, -0.3191, -0.3197, -0.3196, -0.3193,\n",
      "          -0.3197, -0.3184, -0.2834, -0.3197, -0.3197, -0.3140, -0.3176,\n",
      "          -0.3193, -0.3185, -0.3196, -0.3197, -0.3194, -0.3126, -0.3197]]])\n",
      "Standard Deviation: tensor([[[0.5516, 0.1048, 0.3027, 0.2799, 0.2542, 1.4446, 0.0145, 0.6318,\n",
      "          0.2789, 0.0144, 0.9440, 0.1545, 0.0146, 0.0244, 0.0147, 0.0165,\n",
      "          0.0143, 0.0144, 0.0143, 0.0144, 0.0146, 0.0144, 0.0146, 0.0144,\n",
      "          0.0375, 0.0337, 0.0144, 0.0144, 0.0146, 0.0144, 0.0160, 0.0245,\n",
      "          0.0147, 0.0143, 0.0151, 0.0155, 0.0148, 0.0146, 0.0143, 0.0143,\n",
      "          0.0169, 0.0144, 0.0152, 0.0146, 0.0146, 0.0279, 0.0151, 0.0146,\n",
      "          0.0163, 0.0145, 0.0339, 0.0146, 0.0156, 0.0143, 0.0147, 0.0151,\n",
      "          0.0145, 0.0179, 0.0337, 0.0143, 0.0145, 0.0279, 0.0200, 0.0152,\n",
      "          0.0169, 0.0146, 0.0143, 0.0151, 0.0248, 0.0143]]])\n"
     ]
    }
   ],
   "source": [
    "# Calculate mean and standard deviation of the training data\n",
    "mean = node_features_sequence_input_train.mean(dim=(0, 1), keepdim=True)\n",
    "std = node_features_sequence_input_train.std(dim=(0, 1), keepdim=True)\n",
    "\n",
    "# Normalize the training and testing data\n",
    "# node_features_sequence_input_train = (node_features_sequence_input_train - mean) / std\n",
    "# node_features_sequence_input_test = (node_features_sequence_input_test - mean) / std\n",
    "\n",
    "# Print the mean and standard deviation to verify\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Standard Deviation:\", std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">**ZE EPIC EDGE DATA**</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        STATION  LONGITUDE  LATITUDE  ELEVATION\n",
      "0  7.279002e+10 -119.51551  47.30777      382.1\n",
      "1  7.278552e+10 -117.65000  47.63333      750.1\n",
      "2  7.278909e+10 -119.52091  48.46113      397.4\n",
      "3  7.279302e+10 -122.31442  47.44467      112.5\n",
      "4  7.278579e+10 -117.11581  46.74376      775.7\n"
     ]
    }
   ],
   "source": [
    "# Import the location-datamap.csv file as a dataframe\n",
    "location_datamap_df = pd.read_csv('../location-datamap.csv')\n",
    "\n",
    "# Print the first few rows of the dataframe to verify\n",
    "print(location_datamap_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_distance(lat1, lon1, lat2, lon2, el1=0, el2=0):\n",
    "    # Radius of the Earth in kilometers\n",
    "    R = 6371.0\n",
    "\n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "\n",
    "    # Difference in coordinates\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "\n",
    "    # Haversine formula\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "\n",
    "    # Distance in kilometers\n",
    "    distance = R * c\n",
    "\n",
    "    # Elevation difference\n",
    "    height = el2 - el1\n",
    "\n",
    "    # Calculate the total distance considering elevation\n",
    "    total_distance = sqrt(distance**2 + height**2)\n",
    "\n",
    "    return total_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(72790024141, 72785524114), (72790024141, 72789094197), (72790024141, 72793024233), (72790024141, 72785794129), (72790024141, 72788594266), (72790024141, 72797624217), (72790024141, 72785024157), (72790024141, 72797094240), (72790024141, 72798594276), (72790024141, 72792424223), (72790024141, 72792894263), (72790024141, 72781024243), (72790024141, 72781524237), (72790024141, 72788324220), (72790024141, 72698824219), (72790024141, 72793894274), (72790024141, 74206024207), (72790024141, 72782724110), (72790024141, 72793724222), (72790024141, 72792594227), (72790024141, 72782594239), (72790024141, 72794504205), (72790024141, 72792394225), (72790024141, 72784524163), (72790024141, 72792024227), (72790024141, 72785694176), (72785524114, 72789094197), (72785524114, 72793024233), (72785524114, 72785794129), (72785524114, 72788594266), (72785524114, 72797624217), (72785524114, 72785024157), (72785524114, 72797094240), (72785524114, 72798594276), (72785524114, 72792424223), (72785524114, 72792894263), (72785524114, 72781024243), (72785524114, 72781524237), (72785524114, 72788324220), (72785524114, 72698824219), (72785524114, 72793894274), (72785524114, 74206024207), (72785524114, 72782724110), (72785524114, 72793724222), (72785524114, 72792594227), (72785524114, 72782594239), (72785524114, 72794504205), (72785524114, 72792394225), (72785524114, 72784524163), (72785524114, 72792024227), (72785524114, 72785694176), (72789094197, 72793024233), (72789094197, 72785794129), (72789094197, 72788594266), (72789094197, 72797624217), (72789094197, 72785024157), (72789094197, 72797094240), (72789094197, 72798594276), (72789094197, 72792424223), (72789094197, 72792894263), (72789094197, 72781024243), (72789094197, 72781524237), (72789094197, 72788324220), (72789094197, 72698824219), (72789094197, 72793894274), (72789094197, 74206024207), (72789094197, 72782724110), (72789094197, 72793724222), (72789094197, 72792594227), (72789094197, 72782594239), (72789094197, 72794504205), (72789094197, 72792394225), (72789094197, 72784524163), (72789094197, 72792024227), (72789094197, 72785694176), (72793024233, 72785794129), (72793024233, 72788594266), (72793024233, 72797624217), (72793024233, 72785024157), (72793024233, 72797094240), (72793024233, 72798594276), (72793024233, 72792424223), (72793024233, 72792894263), (72793024233, 72781024243), (72793024233, 72781524237), (72793024233, 72788324220), (72793024233, 72698824219), (72793024233, 72793894274), (72793024233, 74206024207), (72793024233, 72782724110), (72793024233, 72793724222), (72793024233, 72792594227), (72793024233, 72782594239), (72793024233, 72794504205), (72793024233, 72792394225), (72793024233, 72784524163), (72793024233, 72792024227), (72793024233, 72785694176), (72785794129, 72788594266), (72785794129, 72797624217), (72785794129, 72785024157), (72785794129, 72797094240), (72785794129, 72798594276), (72785794129, 72792424223), (72785794129, 72792894263), (72785794129, 72781024243), (72785794129, 72781524237), (72785794129, 72788324220), (72785794129, 72698824219), (72785794129, 72793894274), (72785794129, 74206024207), (72785794129, 72782724110), (72785794129, 72793724222), (72785794129, 72792594227), (72785794129, 72782594239), (72785794129, 72794504205), (72785794129, 72792394225), (72785794129, 72784524163), (72785794129, 72792024227), (72785794129, 72785694176), (72788594266, 72797624217), (72788594266, 72785024157), (72788594266, 72797094240), (72788594266, 72798594276), (72788594266, 72792424223), (72788594266, 72792894263), (72788594266, 72781024243), (72788594266, 72781524237), (72788594266, 72788324220), (72788594266, 72698824219), (72788594266, 72793894274), (72788594266, 74206024207), (72788594266, 72782724110), (72788594266, 72793724222), (72788594266, 72792594227), (72788594266, 72782594239), (72788594266, 72794504205), (72788594266, 72792394225), (72788594266, 72784524163), (72788594266, 72792024227), (72788594266, 72785694176), (72797624217, 72785024157), (72797624217, 72797094240), (72797624217, 72798594276), (72797624217, 72792424223), (72797624217, 72792894263), (72797624217, 72781024243), (72797624217, 72781524237), (72797624217, 72788324220), (72797624217, 72698824219), (72797624217, 72793894274), (72797624217, 74206024207), (72797624217, 72782724110), (72797624217, 72793724222), (72797624217, 72792594227), (72797624217, 72782594239), (72797624217, 72794504205), (72797624217, 72792394225), (72797624217, 72784524163), (72797624217, 72792024227), (72797624217, 72785694176), (72785024157, 72797094240), (72785024157, 72798594276), (72785024157, 72792424223), (72785024157, 72792894263), (72785024157, 72781024243), (72785024157, 72781524237), (72785024157, 72788324220), (72785024157, 72698824219), (72785024157, 72793894274), (72785024157, 74206024207), (72785024157, 72782724110), (72785024157, 72793724222), (72785024157, 72792594227), (72785024157, 72782594239), (72785024157, 72794504205), (72785024157, 72792394225), (72785024157, 72784524163), (72785024157, 72792024227), (72785024157, 72785694176), (72797094240, 72798594276), (72797094240, 72792424223), (72797094240, 72792894263), (72797094240, 72781024243), (72797094240, 72781524237), (72797094240, 72788324220), (72797094240, 72698824219), (72797094240, 72793894274), (72797094240, 74206024207), (72797094240, 72782724110), (72797094240, 72793724222), (72797094240, 72792594227), (72797094240, 72782594239), (72797094240, 72794504205), (72797094240, 72792394225), (72797094240, 72784524163), (72797094240, 72792024227), (72797094240, 72785694176), (72798594276, 72792424223), (72798594276, 72792894263), (72798594276, 72781024243), (72798594276, 72781524237), (72798594276, 72788324220), (72798594276, 72698824219), (72798594276, 72793894274), (72798594276, 74206024207), (72798594276, 72782724110), (72798594276, 72793724222), (72798594276, 72792594227), (72798594276, 72782594239), (72798594276, 72794504205), (72798594276, 72792394225), (72798594276, 72784524163), (72798594276, 72792024227), (72798594276, 72785694176), (72792424223, 72792894263), (72792424223, 72781024243), (72792424223, 72781524237), (72792424223, 72788324220), (72792424223, 72698824219), (72792424223, 72793894274), (72792424223, 74206024207), (72792424223, 72782724110), (72792424223, 72793724222), (72792424223, 72792594227), (72792424223, 72782594239), (72792424223, 72794504205), (72792424223, 72792394225), (72792424223, 72784524163), (72792424223, 72792024227), (72792424223, 72785694176), (72792894263, 72781024243), (72792894263, 72781524237), (72792894263, 72788324220), (72792894263, 72698824219), (72792894263, 72793894274), (72792894263, 74206024207), (72792894263, 72782724110), (72792894263, 72793724222), (72792894263, 72792594227), (72792894263, 72782594239), (72792894263, 72794504205), (72792894263, 72792394225), (72792894263, 72784524163), (72792894263, 72792024227), (72792894263, 72785694176), (72781024243, 72781524237), (72781024243, 72788324220), (72781024243, 72698824219), (72781024243, 72793894274), (72781024243, 74206024207), (72781024243, 72782724110), (72781024243, 72793724222), (72781024243, 72792594227), (72781024243, 72782594239), (72781024243, 72794504205), (72781024243, 72792394225), (72781024243, 72784524163), (72781024243, 72792024227), (72781024243, 72785694176), (72781524237, 72788324220), (72781524237, 72698824219), (72781524237, 72793894274), (72781524237, 74206024207), (72781524237, 72782724110), (72781524237, 72793724222), (72781524237, 72792594227), (72781524237, 72782594239), (72781524237, 72794504205), (72781524237, 72792394225), (72781524237, 72784524163), (72781524237, 72792024227), (72781524237, 72785694176), (72788324220, 72698824219), (72788324220, 72793894274), (72788324220, 74206024207), (72788324220, 72782724110), (72788324220, 72793724222), (72788324220, 72792594227), (72788324220, 72782594239), (72788324220, 72794504205), (72788324220, 72792394225), (72788324220, 72784524163), (72788324220, 72792024227), (72788324220, 72785694176), (72698824219, 72793894274), (72698824219, 74206024207), (72698824219, 72782724110), (72698824219, 72793724222), (72698824219, 72792594227), (72698824219, 72782594239), (72698824219, 72794504205), (72698824219, 72792394225), (72698824219, 72784524163), (72698824219, 72792024227), (72698824219, 72785694176), (72793894274, 74206024207), (72793894274, 72782724110), (72793894274, 72793724222), (72793894274, 72792594227), (72793894274, 72782594239), (72793894274, 72794504205), (72793894274, 72792394225), (72793894274, 72784524163), (72793894274, 72792024227), (72793894274, 72785694176), (74206024207, 72782724110), (74206024207, 72793724222), (74206024207, 72792594227), (74206024207, 72782594239), (74206024207, 72794504205), (74206024207, 72792394225), (74206024207, 72784524163), (74206024207, 72792024227), (74206024207, 72785694176), (72782724110, 72793724222), (72782724110, 72792594227), (72782724110, 72782594239), (72782724110, 72794504205), (72782724110, 72792394225), (72782724110, 72784524163), (72782724110, 72792024227), (72782724110, 72785694176), (72793724222, 72792594227), (72793724222, 72782594239), (72793724222, 72794504205), (72793724222, 72792394225), (72793724222, 72784524163), (72793724222, 72792024227), (72793724222, 72785694176), (72792594227, 72782594239), (72792594227, 72794504205), (72792594227, 72792394225), (72792594227, 72784524163), (72792594227, 72792024227), (72792594227, 72785694176), (72782594239, 72794504205), (72782594239, 72792394225), (72782594239, 72784524163), (72782594239, 72792024227), (72782594239, 72785694176), (72794504205, 72792394225), (72794504205, 72784524163), (72794504205, 72792024227), (72794504205, 72785694176), (72792394225, 72784524163), (72792394225, 72792024227), (72792394225, 72785694176), (72784524163, 72792024227), (72784524163, 72785694176), (72792024227, 72785694176)]\n"
     ]
    }
   ],
   "source": [
    "def create_edge_index(dataframes):\n",
    "    edges = []\n",
    "    keys = list(dataframes.keys())\n",
    "    for i in range(len(keys)):\n",
    "        for j in range(i + 1, len(keys)):\n",
    "            if i != j:\n",
    "                edges.append((keys[i], keys[j]))\n",
    "    return edges\n",
    "\n",
    "edge_index = create_edge_index(dataframes)\n",
    "print(edge_index)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 1): 395.4679457314314, (0, 2): 129.15783117310457, (0, 3): 342.5330553210191, (0, 4): 438.10430559873856, (0, 5): 432.0504921333856, (0, 6): 437.11321302131137, (0, 7): 369.0087247919059, (0, 8): 503.7031148557305, (0, 9): 455.6811328123654, (0, 10): 474.59654599988056, (0, 11): 348.0950737502581, (0, 12): 128.6980383337319, (0, 13): 835.9738346074198, (0, 14): 171.59537540199554, (0, 15): 384.56540064951065, (0, 16): 373.58764051749137, (0, 17): 362.1843030623904, (0, 18): 32.53657446575969, (0, 19): 306.4908073667999, (0, 20): 404.50991220772727, (0, 21): 52.81240627430448, (0, 22): 404.8085057270694, (0, 23): 505.4288197147213, (0, 24): 286.0836462058196, (0, 25): 412.6227567328569, (0, 26): 272.19719043646785, (1, 2): 390.14029117618804, (1, 3): 727.6917541369755, (1, 4): 109.85904921372176, (1, 5): 798.995937782049, (1, 6): 802.4581375032445, (1, 7): 33.690956835057186, (1, 8): 865.1737430163511, (1, 9): 825.6499844120128, (1, 10): 860.6728795779842, (1, 11): 724.9963923066945, (1, 12): 495.92270395996894, (1, 13): 535.6266022869553, (1, 14): 314.54161595295784, (1, 15): 763.6141112163369, (1, 16): 759.3841400612968, (1, 17): 748.5486591666476, (1, 18): 416.12981588895354, (1, 19): 678.7815012654146, (1, 20): 784.7770299622911, (1, 21): 419.79175595055284, (1, 22): 786.18443118649, (1, 23): 886.2658783050533, (1, 24): 655.5895444605051, (1, 25): 798.318691116012, (1, 26): 157.5431429141368, (2, 3): 370.42190175405364, (2, 4): 460.5219705003162, (2, 5): 432.51979230044606, (2, 6): 417.3828379309705, (2, 7): 365.04041085309774, (2, 8): 508.89691489063597, (2, 9): 445.736559884783, (2, 10): 534.5520542530354, (2, 11): 372.6745336577991, (2, 12): 236.9285970548754, (2, 13): 831.0617187024466, (2, 14): 221.69557651346355, (2, 15): 470.9577296531585, (2, 16): 406.24280451088293, (2, 17): 399.71114545506987, (2, 18): 147.68778122386612, (2, 19): 313.8982068438734, (2, 20): 435.3656912848407, (2, 21): 130.20708355028395, (2, 22): 406.9699486305622, (2, 23): 539.0132078425393, (2, 24): 368.91523621650714, (2, 25): 452.3832910640423, (2, 26): 270.2281521689557, (3, 4): 775.0588941479449, (3, 5): 120.26611512741678, (3, 6): 165.52848353942377, (3, 7): 704.0879836514051, (3, 8): 185.28412975619443, (3, 9): 152.43166309928537, (3, 10): 187.27787071696568, (3, 11): 41.159846544112625, (3, 12): 266.9618608270944, (3, 13): 1096.8318771247875, (3, 14): 443.4038293955239, (3, 15): 225.02216925780087, (3, 16): 36.85266364881154, (3, 17): 37.9895765777111, (3, 18): 333.6917238582668, (3, 19): 76.27723241526142, (3, 20): 72.18185980856553, (3, 21): 309.0749255571238, (3, 22): 107.09860018045799, (3, 23): 171.23674671831446, (3, 24): 276.0731537992283, (3, 25): 85.99186556469607, (3, 26): 611.0889227645558, (4, 5): 856.701667766572, (4, 6): 865.4792367557994, (4, 7): 117.74480456011335, (4, 8): 921.4842980890041, (4, 9): 885.9031028312606, (4, 10): 890.5893542480043, (4, 11): 774.366157782777, (4, 12): 524.6337306289196, (4, 13): 540.0809528724242, (4, 14): 357.13279391570444, (4, 15): 780.4774458332262, (4, 16): 804.444912957628, (4, 17): 791.9148221844617, (4, 18): 454.1201925958629, (4, 19): 734.2694292819892, (4, 20): 830.6418814848106, (4, 21): 467.37603414879794, (4, 22): 840.9110561664348, (4, 23): 929.3740741921883, (4, 24): 673.3913688173589, (4, 25): 839.7968409183139, (4, 26): 209.78499150005845, (5, 6): 110.15242913456197, (5, 7): 777.4585937545535, (5, 8): 85.13925057078639, (5, 9): 74.97130998219406, (5, 10): 240.43942234567194, (5, 11): 103.9757703201679, (5, 12): 369.69695959787833, (5, 13): 1139.0521959187856, (5, 14): 516.0490346912795, (5, 15): 330.5216959336131, (5, 16): 117.91278236100351, (5, 17): 133.34207595548997, (5, 18): 428.7153028579556, (5, 19): 125.7282441840991, (5, 20): 101.97987733596389, (5, 21): 392.60154297249403, (5, 22): 108.31645122446824, (5, 23): 153.40108385187867, (5, 24): 392.1898886399527, (5, 25): 137.25092367847324, (5, 26): 690.174750212649, (6, 7): 778.6874797868297, (6, 8): 177.46961023734613, (6, 9): 48.71149879169117, (6, 10): 302.0846527899822, (6, 11): 172.27646695285324, (6, 12): 399.97805818217563, (6, 13): 1176.4852153080844, (6, 14): 545.6427005439592, (6, 15): 369.3977887607593, (6, 16): 175.55537172802033, (6, 17): 190.70446482220075, (6, 18): 431.096161984872, (6, 19): 156.64330089805705, (6, 20): 183.24573042058552, (6, 21): 405.4909815288361, (6, 22): 76.4859126118275, (6, 23): 231.7098185991757, (6, 24): 388.4359336032824, (6, 25): 205.33116784821942, (6, 26): 682.4260680955291, (7, 8): 845.1366146343045, (7, 9): 802.4776134962176, (7, 10): 837.0612376685274, (7, 11): 702.8044715545802, (7, 12): 472.23598917705993, (7, 13): 567.9966156275789, (7, 14): 299.672336109386, (7, 15): 738.0303386253106, (7, 16): 735.9632694864449, (7, 17): 725.1092500944276, (7, 18): 388.53428759844127, (7, 19): 656.1420174916371, (7, 20): 762.4710980022056, (7, 21): 395.81370216605853, (7, 22): 761.3237576215986, (7, 23): 864.2082908552868, (7, 24): 626.0309963149288, (7, 25): 775.1913077729471, (7, 26): 124.27275180830249, (8, 9): 132.5097212349861, (8, 10): 243.7073144688935, (8, 11): 163.47287592761103, (8, 12): 430.6615227573649, (8, 13): 1177.64872286148, (8, 14): 572.9402144031785, (8, 15): 365.1202761234219, (8, 16): 169.20923499165121, (8, 17): 183.39281681071296, (8, 18): 501.4584011725272, (8, 19): 202.41419212127153, (8, 20): 134.47678138943274, (8, 21): 461.61393826175714, (8, 22): 180.6112148305096, (8, 23): 128.08708542091654, (8, 24): 456.1447880245876, (8, 25): 164.101942102309, (8, 26): 763.0321197403028, (9, 10): 268.89465547374556, (9, 11): 154.53742133091865, (9, 12): 405.60664848136616, (9, 13): 1187.3176896736784, (9, 14): 557.1020923327553, (9, 15): 353.92282978101207, (9, 16): 153.1723115228679, (9, 17): 170.09372061722416, (9, 18): 449.65266636740307, (9, 19): 158.4233084920311, (9, 20): 151.2637681156068, (9, 21): 421.5198568724629, (9, 22): 75.9354339763056, (9, 23): 187.20830867381414, (9, 24): 396.00851322484175, (9, 25): 174.23493854899442, (9, 26): 708.5304039928845, (10, 11): 199.69966656538062, (10, 12): 366.6429361022303, (10, 13): 1213.3303112543872, (10, 14): 565.7742092205691, (10, 15): 158.75602862100996, (10, 16): 154.05814985745886, (10, 17): 150.49523188191975, (10, 18): 459.96707604415764, (10, 19): 261.56576121699044, (10, 20): 148.37325199067052, (10, 21): 447.531376407682, (10, 22): 236.6757463314917, (10, 23): 123.91187323342717, (10, 24): 313.3574949442864, (10, 25): 109.93432994721853, (10, 26): 745.579780595948, (11, 12): 271.20003239227077, (11, 13): 1077.0374760399814, (11, 14): 434.420916611982, (11, 15): 249.12813853110475, (11, 16): 54.60340565924095, (11, 17): 56.69074125354086, (11, 18): 342.83747887822733, (11, 19): 68.6532776654429, (11, 20): 64.41876337172579, (11, 21): 309.79708423419913, (11, 22): 128.34321457181807, (11, 23): 167.4998627149666, (11, 24): 308.66359444651886, (11, 25): 94.10375597750865, (11, 26): 614.9675457923169, (12, 13): 891.3237029023409, (12, 14): 217.85555378560997, (12, 15): 275.33106494313824, (12, 16): 290.27386357894756, (12, 17): 275.46064771106325, (12, 18): 121.40670159689395, (12, 19): 252.86361822595595, (12, 20): 317.66456605180196, (12, 21): 111.45482222908821, (12, 22): 352.74223587837065, (12, 23): 411.2440393694844, (12, 24): 228.97116863460516, (12, 25): 319.85603194172165, (12, 26): 386.6621826263452, (13, 14): 677.4867035602812, (13, 15): 1150.4386824494006, (13, 16): 1122.20080799066, (13, 17): 1112.0640517140919, (13, 18): 864.0517998527835, (13, 19): 1044.4930844703567, (13, 20): 1130.7362905229766, (13, 21): 833.7147080759753, (13, 22): 1170.6533446424978, (13, 23): 1218.574885845403, (13, 24): 1103.3829323700534, (13, 25): 1152.414996794818, (13, 26): 683.9489018806388, (14, 15): 489.8134161401565, (14, 16): 470.9676802550459, (14, 17): 459.0274176052015, (14, 18): 199.59384465449898, (14, 19): 400.7840441174383, (14, 20): 490.3741382897129, (14, 21): 162.11769623669784, (14, 22): 520.9373250801362, (14, 23): 587.5949150283049, (14, 24): 432.7632336033511, (14, 25): 504.8208347435879, (14, 26): 259.92185050812566, (15, 16): 213.47199631168857, (15, 17): 199.7906856396986, (15, 18): 363.3873739871944, (15, 19): 286.49766075950384, (15, 20): 235.6189290359573, (15, 21): 371.98475050597, (15, 22): 293.97835633481196, (15, 23): 268.7494270590419, (15, 24): 181.34059793217904, (15, 25): 201.61183866011444, (15, 26): 642.4950004501114, (16, 17): 17.716247962574563, (16, 18): 363.9821600522806, (16, 19): 109.51765254130393, (16, 20): 42.988933127483634, (16, 21): 340.3237239231559, (16, 22): 114.05670021657697, (16, 23): 136.43815241862657, (16, 24): 287.7450730575042, (16, 25): 49.41202473657626, (16, 26): 643.6682517559431, (17, 18): 352.2158347034709, (17, 19): 111.26188855442645, (17, 20): 52.561875466910436, (17, 21): 329.3368670529716, (17, 22): 128.01154758942099, (17, 23): 145.49531716211806, (17, 24): 275.6435250697721, (17, 25): 52.80087090274467, (17, 26): 633.1197418989807, (18, 19): 303.31516235822005, (18, 20): 396.9755284116139, (18, 21): 73.74026824660453, (18, 22): 394.74559894207687, (18, 23): 496.478460291939, (18, 24): 256.1592053257525, (18, 25): 402.1731282608459, (18, 26): 286.80868520838226, (19, 20): 129.67576043022495, (19, 21): 268.31498195846285, (19, 22): 128.38153131965652, (19, 23): 230.11767302669344, (19, 24): 305.30416194483115, (19, 25): 156.88223261226412, (19, 26): 566.3246868369048, (20, 21): 368.04423285388, (20, 22): 133.10933796080906, (20, 23): 103.88553424589651, (20, 24): 327.04970158433304, (20, 25): 41.59043713208587, (20, 26): 674.0342619771884, (21, 22): 375.88513957717953, (21, 23): 469.86938496204897, (21, 24): 296.22180486691445, (21, 25): 379.7335338263519, (21, 26): 308.0303254221726, (22, 23): 191.05105434406818, (22, 24): 321.52952520777865, (22, 25): 144.61575947894278, (22, 26): 662.014427725483, (23, 24): 393.67944122771735, (23, 25): 96.11729883125135, (23, 26): 776.1835969743106, (24, 25): 305.7627599001508, (24, 26): 516.3645983655035, (25, 26): 684.1746652775099}\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary to store the distances between nodes\n",
    "distances = {}\n",
    "\n",
    "# Define the number of nodes\n",
    "num_nodes = len(dataframes)\n",
    "\n",
    "# Iterate over each pair of nodes\n",
    "for i, j in itertools.combinations(range(num_nodes), 2):\n",
    "    # Get the station IDs for the nodes\n",
    "    station_i = list(dataframes.keys())[i]\n",
    "    station_j = list(dataframes.keys())[j]\n",
    "    \n",
    "    # Get the location data for the stations\n",
    "    location_i = location_datamap_df[location_datamap_df['STATION'] == station_i].iloc[0]\n",
    "    location_j = location_datamap_df[location_datamap_df['STATION'] == station_j].iloc[0]\n",
    "    \n",
    "    # Calculate the distance between the stations\n",
    "    distance = haversine_distance(location_i['LATITUDE'], location_i['LONGITUDE'], location_j['LATITUDE'], location_j['LONGITUDE'], location_i['ELEVATION'], location_j['ELEVATION'])\n",
    "    \n",
    "    # Store the distance in the dictionary\n",
    "    distances[(i, j)] = distance\n",
    "\n",
    "# Print the distances to verify\n",
    "print(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 1): 0.314567998140773, (0, 2): 0.09280158354609237, (0, 3): 0.2704871307176606, (0, 4): 0.35007289315695667, (0, 5): 0.34503165576699535, (0, 6): 0.349247573218238, (0, 7): 0.2925344130835223, (0, 8): 0.40469948049003845, (0, 9): 0.3647097760165542, (0, 10): 0.380461349591329, (0, 11): 0.27511883194691195, (0, 12): 0.0924186968141592, (0, 13): 0.6813937634553494, (0, 14): 0.1281409173278857, (0, 15): 0.3054890401868689, (0, 16): 0.2963474478414338, (0, 17): 0.28685146130699407, (0, 18): 0.012341441395062182, (0, 19): 0.2404733998610767, (0, 20): 0.32209758254900717, (0, 21): 0.029225886548649996, (0, 22): 0.3223462325648563, (0, 23): 0.4061365396113629, (0, 24): 0.2234795918330484, (0, 25): 0.32885345228187846, (0, 26): 0.21191581960267647, (1, 2): 0.31013146049414686, (1, 3): 0.5912232162697533, (1, 4): 0.0767307644250279, (1, 5): 0.6506008827124798, (1, 6): 0.6534839861952507, (1, 7): 0.013302738864123758, (1, 8): 0.7057096216985912, (1, 9): 0.672796706424883, (1, 10): 0.7019615840059098, (1, 11): 0.5889786874424193, (1, 12): 0.39822044070107643, (1, 13): 0.4312833650740786, (1, 14): 0.24717760994224985, (1, 15): 0.6211371095009285, (1, 16): 0.6176146539665304, (1, 17): 0.6085915428751594, (1, 18): 0.33177391189757455, (1, 19): 0.5504938153822416, (1, 20): 0.6387602651983096, (1, 21): 0.3348233466487418, (1, 22): 0.6399322609518483, (1, 23): 0.7232738333579158, (1, 24): 0.5311810036379734, (1, 25): 0.6500369140281799, (1, 26): 0.11643909661014204, (2, 3): 0.2937112185105451, (2, 4): 0.3687409230102465, (2, 5): 0.34542245960706086, (2, 6): 0.3328173503194575, (2, 7): 0.2892298493208762, (2, 8): 0.40902455246026, (2, 9): 0.3564285573835974, (2, 10): 0.4303885486485463, (2, 11): 0.295587069533041, (2, 12): 0.18254633990790353, (2, 13): 0.6773032604185943, (2, 14): 0.16986123271804443, (2, 15): 0.37743117082429484, (2, 16): 0.32354062692449737, (2, 17): 0.31810146960009433, (2, 18): 0.108232167518433, (2, 19): 0.24664181905997015, (2, 20): 0.34779234636526585, (2, 21): 0.09367533533009026, (2, 22): 0.3241461470887702, (2, 23): 0.43410351846162026, (2, 24): 0.29245656164252043, (2, 25): 0.36196353957848576, (2, 26): 0.21027612763111883, (3, 4): 0.6306676092371719, (3, 5): 0.08539711830331884, (3, 6): 0.1230887890663378, (3, 7): 0.571567471837432, (3, 8): 0.13954005617934356, (3, 9): 0.11218257577279916, (3, 10): 0.1412003190095184, (3, 11): 0.019522363284050027, (3, 12): 0.20755616439912683, (3, 13): 0.8986200332994612, (3, 14): 0.3544860052665791, (3, 15): 0.17263141118817832, (3, 16): 0.01593561063938, (3, 17): 0.01688236065060862, (3, 18): 0.2631246226056817, (3, 19): 0.0487659268171087, (3, 20): 0.04535555653912475, (3, 21): 0.24262529193963123, (3, 22): 0.07443203504408213, (3, 23): 0.12784227378037094, (3, 24): 0.21514347957903632, (3, 25): 0.05685566597788288, (3, 26): 0.4941236679182537, (4, 5): 0.6986546070761243, (4, 6): 0.7059640177863663, (4, 7): 0.08329752848669511, (4, 8): 0.7526015316172568, (4, 9): 0.7229717366228394, (4, 10): 0.7268741538340823, (4, 11): 0.6300907416997995, (4, 12): 0.4221291887944986, (4, 13): 0.4349926697706931, (4, 14): 0.28264488029293566, (4, 15): 0.6351798403311119, (4, 16): 0.6551384485871659, (4, 17): 0.6447041723302557, (4, 18): 0.36340992258896465, (4, 19): 0.5967006929164721, (4, 20): 0.676953646230553, (4, 21): 0.37444855872377714, (4, 22): 0.6855051729112698, (4, 23): 0.7591716439138168, (4, 24): 0.5460052500523883, (4, 25): 0.6845773241096111, (4, 26): 0.15994284212845428, (5, 6): 0.07697507288198122, (5, 7): 0.6326659290484358, (5, 8): 0.0561456615135665, (5, 9): 0.04767843625671288, (5, 10): 0.18546993572513168, (5, 11): 0.07183153756521501, (5, 12): 0.29310753200381906, (5, 13): 0.9337784753192767, (5, 14): 0.41498039070384635, (5, 15): 0.2604848215294762, (5, 16): 0.08343740989787128, (5, 17): 0.0962859610160021, (5, 18): 0.34225431864319356, (5, 19): 0.08994563790785139, (5, 20): 0.07016948266446262, (5, 21): 0.3121810371209556, (5, 22): 0.07544618525759716, (5, 23): 0.11298984877064547, (5, 24): 0.3118382371280546, (5, 25): 0.09954100503173634, (5, 26): 0.5599814008380295, (6, 7): 0.6336892685103085, (6, 8): 0.1330326128614309, (6, 9): 0.025810907172023778, (6, 10): 0.23680422978741508, (6, 11): 0.1287080877918951, (6, 12): 0.3183237378327452, (6, 13): 0.9649503536806591, (6, 14): 0.43962414552985546, (6, 15): 0.2928584012338173, (6, 16): 0.1314385547025949, (6, 17): 0.14405377236126038, (6, 18): 0.344236949280813, (6, 19): 0.11568976443423645, (6, 20): 0.13784260464649475, (6, 21): 0.322914555746484, (6, 22): 0.048939702638827395, (6, 23): 0.17820046747041107, (6, 24): 0.30871217805811385, (6, 25): 0.15623397622922458, (6, 26): 0.5535287827923442, (7, 8): 0.6890239538356585, (7, 9): 0.6535002045846253, (7, 10): 0.6822992847438707, (7, 11): 0.5704986432040403, (7, 12): 0.378495625443329, (7, 13): 0.45823908852017337, (7, 14): 0.2347954032657112, (7, 15): 0.5998325430982321, (7, 16): 0.5981112171455704, (7, 17): 0.5890726683525555, (7, 18): 0.30879408111652235, (7, 19): 0.531641068639551, (7, 20): 0.6201852795535281, (7, 21): 0.3148559224840052, (7, 22): 0.6192298462124066, (7, 23): 0.7049056534956674, (7, 24): 0.5065664926429994, (7, 25): 0.6307778750260209, (7, 26): 0.08873359484975862, (8, 9): 0.09559282804077418, (8, 10): 0.18819123198776505, (8, 11): 0.12137700755686981, (8, 12): 0.34387500890432254, (8, 13): 0.965919250032562, (8, 14): 0.46235580852338326, (8, 15): 0.289296356125095, (8, 16): 0.12615388876759553, (8, 17): 0.13796508899684823, (8, 18): 0.4028302232665897, (8, 19): 0.15380490120329926, (8, 20): 0.09723087276343585, (8, 21): 0.3696502454958358, (8, 22): 0.13564874476410202, (8, 23): 0.09190993342308056, (8, 24): 0.365095879091134, (8, 25): 0.12190085454006432, (8, 26): 0.6206524633838301, (9, 10): 0.20916567494906027, (9, 11): 0.11393611958320622, (9, 12): 0.3230108759534436, (9, 13): 0.973970961122591, (9, 14): 0.44916681060906877, (9, 15): 0.27997182283768707, (9, 16): 0.11279934147711913, (9, 17): 0.12689043310151676, (9, 18): 0.359689646040564, (9, 19): 0.11717204347843128, (9, 20): 0.11121002584324408, (9, 21): 0.3362624010614717, (9, 22): 0.04848129844523104, (9, 23): 0.14114239208876594, (9, 24): 0.31501814895483, (9, 25): 0.13033898050012765, (9, 26): 0.5752668417726906, (10, 11): 0.15154441402332883, (10, 12): 0.29056433216388416, (10, 13): 0.9956326461537036, (10, 14): 0.45638840740176295, (10, 15): 0.11744911200130542, (10, 16): 0.11353701226254376, (10, 17): 0.11057003691412082, (10, 18): 0.36827884159728613, (10, 19): 0.20306262998976654, (10, 20): 0.108802984719709, (10, 21): 0.3579231683779967, (10, 22): 0.18233578163284334, (10, 23): 0.08843307773351292, (10, 24): 0.24619154799347698, (10, 25): 0.07679345351358663, (10, 26): 0.6061192464057483, (11, 12): 0.21108544872242438, (11, 13): 0.882136493555179, (11, 14): 0.3470055970818329, (11, 15): 0.19270535537515096, (11, 16): 0.030717318869188645, (11, 17): 0.03245552145894557, (11, 18): 0.27074063562457035, (11, 19): 0.0424171739253779, (11, 20): 0.03889093514911131, (11, 21): 0.24322666053897657, (11, 22): 0.09212322176762135, (11, 23): 0.12473043039975776, (11, 24): 0.24228276110575214, (11, 25): 0.0636107411856644, (11, 26): 0.49735354269735277, (12, 13): 0.7274856734844146, (12, 14): 0.16666350185554782, (12, 15): 0.21452551437257505, (12, 16): 0.22696894290314232, (12, 17): 0.21463342280062572, (12, 18): 0.08634692740948313, (12, 19): 0.19581602933544076, (12, 20): 0.24977820754826785, (12, 21): 0.0780596244298823, (12, 22): 0.27898869804231324, (12, 23): 0.32770534265441786, (12, 24): 0.17591989099106883, (12, 25): 0.25160312333834234, (12, 26): 0.3072351091334447, (13, 14): 0.5494155887997888, (13, 15): 0.9432604294572675, (13, 16): 0.919745692944629, (13, 17): 0.9113044360332913, (13, 18): 0.7047753375720718, (13, 19): 0.8550355588215105, (13, 20): 0.9268535091879839, (13, 21): 0.6795125041129281, (13, 22): 0.9600939363792285, (13, 23): 1.0, (13, 24): 0.9040753425578563, (13, 25): 0.9449061804916289, (13, 26): 0.5547969035661553, (14, 15): 0.39313300773679066, (14, 16): 0.3774394570634686, (14, 17): 0.3674963527935975, (14, 18): 0.15145629215157527, (14, 19): 0.3189949125321117, (14, 20): 0.39359994208848503, (14, 21): 0.12024849863153748, (14, 22): 0.41905105334027043, (14, 23): 0.47455932704156895, (14, 24): 0.3456251823049916, (14, 25): 0.4056302477365713, (14, 26): 0.20169368392317283, (15, 16): 0.16301314923648366, (15, 17): 0.15162020901821552, (15, 18): 0.2878533035612373, (15, 19): 0.22382435727056416, (15, 20): 0.1814557302577726, (15, 21): 0.2950126612471121, (15, 22): 0.23005381287784274, (15, 23): 0.20904473780448538, (15, 24): 0.13625612941260268, (15, 25): 0.15313675140127786, (15, 26): 0.5202766860128114, (16, 17): 0.0, (16, 18): 0.2883486042122239, (16, 19): 0.07644647061920598, (16, 20): 0.021045512242361872, (16, 21): 0.268647337649462, (16, 22): 0.0802263057572332, (16, 23): 0.09886417993825188, (16, 24): 0.22486312424833235, (16, 25): 0.026394261384406467, (16, 26): 0.5212536963525964, (17, 18): 0.2785503440526814, (17, 19): 0.07789896132718613, (17, 20): 0.029017260154592706, (17, 21): 0.2594981701091805, (17, 22): 0.09184703023938133, (17, 23): 0.10640642051326218, (17, 24): 0.2147857116320833, (17, 25): 0.02921628061236748, (17, 26): 0.5124695568009504, (18, 19): 0.23782892122853866, (18, 20): 0.3158234187478483, (18, 21): 0.04665330165988813, (18, 22): 0.3139664728932817, (18, 23): 0.39868323983033105, (18, 24): 0.19856038824316935, (18, 25): 0.32015165496588943, (18, 26): 0.22408335898739135, (19, 20): 0.09323288265223323, (19, 21): 0.20868295908476445, (19, 22): 0.09215512955978745, (19, 23): 0.17687462817321511, (19, 24): 0.23948523573872763, (19, 25): 0.11588873182862379, (19, 26): 0.45684681074664474, (20, 21): 0.29173124449431487, (20, 22): 0.09609215136402571, (20, 23): 0.0717563946038166, (20, 24): 0.2575935616927636, (20, 25): 0.019880932206643945, (20, 26): 0.5465406112843839, (21, 22): 0.2982606614264556, (21, 23): 0.3765248654051755, (21, 24): 0.2319220165625477, (21, 25): 0.30146536356854725, (21, 26): 0.24175541425211852, (22, 23): 0.1443423904474645, (22, 24): 0.25299670390916407, (22, 25): 0.10567397986169143, (22, 26): 0.5365312447590309, (23, 24): 0.31307864340134484, (23, 25): 0.06528749379435833, (23, 26): 0.6316041914383448, (24, 25): 0.23986712744592156, (24, 26): 0.4152431724037643, (25, 26): 0.5549849052091039}\n"
     ]
    }
   ],
   "source": [
    "# Extract the distance values from the dictionary\n",
    "distance_values = np.array(list(distances.values()))\n",
    "\n",
    "# Perform Min-Max normalization\n",
    "min_distance = distance_values.min()\n",
    "max_distance = distance_values.max()\n",
    "normalized_distances = (distance_values - min_distance) / (max_distance - min_distance)\n",
    "\n",
    "# Update the distances dictionary with normalized values\n",
    "normalized_distances_dict = {key: normalized_distances[i] for i, key in enumerate(distances.keys())}\n",
    "\n",
    "# Print the normalized distances to verify\n",
    "\n",
    "print(normalized_distances_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 702])\n",
      "torch.Size([702, 1])\n"
     ]
    }
   ],
   "source": [
    "# Create edge index and edge attributes from distances dictionary\n",
    "edge_index = []\n",
    "edge_attr = []\n",
    "\n",
    "for (i, j), distance in distances.items():\n",
    "    edge_index.append([i, j])\n",
    "    edge_index.append([j, i])  # Assuming undirected graph\n",
    "    edge_attr.append([distance])\n",
    "    edge_attr.append([distance])  # Assuming undirected graph\n",
    "\n",
    "edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "print(edge_index.shape)\n",
    "print(edge_attr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">**ZE EPIC GPU IMPORT**</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move data to device\n",
    "edge_index = edge_index.to(device)\n",
    "edge_attr = edge_attr.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">**ZE EPIC BATCHING**</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 704\n",
      "Number of full batches: 1408\n",
      "Batched input train dimensionality: torch.Size([2, 704, 27, 70])\n",
      "Batched output train dimensionality: torch.Size([2, 704, 27, 70])\n",
      "Batched input train dimensionality: torch.Size([704, 27, 70])\n",
      "Batched output train dimensionality: torch.Size([704, 27, 70])\n"
     ]
    }
   ],
   "source": [
    "batch_size = node_features_sequence_input_train.shape[0] //2 # Adjust this value based on your GPU memory capacity\n",
    "# Calculate the number of full batches\n",
    "num_full_batches_train = (node_features_sequence_input_train.size(0) // batch_size) * batch_size\n",
    "\n",
    "# Trim the input and output tensors to have a size divisible by the batch size\n",
    "trimmed_input_train = node_features_sequence_input_train[:num_full_batches_train]\n",
    "trimmed_output_train = node_features_sequence_output_train[:num_full_batches_train]\n",
    "\n",
    "# Create batches of data\n",
    "batched_input_train = trimmed_input_train.view(-1, batch_size, trimmed_input_train.size(1), trimmed_input_train.size(2))\n",
    "batched_output_train = trimmed_output_train.view(-1, batch_size, trimmed_output_train.size(1), trimmed_output_train.size(2))\n",
    "\n",
    "# Adjust the number of batches\n",
    "num_batches_train = batched_input_train.size(0)\n",
    "\n",
    "# Print the batch size and number of full batches to verify\n",
    "print(\"Batch size:\", batch_size)\n",
    "print(\"Number of full batches:\", num_full_batches_train)\n",
    "\n",
    "print(\"Batched input train dimensionality:\", batched_input_train.shape)\n",
    "print(\"Batched output train dimensionality:\", batched_output_train.shape)\n",
    "print(\"Batched input train dimensionality:\", batched_input_train[0].shape)\n",
    "print(\"Batched output train dimensionality:\", batched_output_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 88\n",
      "Number of full batches: 1408\n",
      "Batched input test dimensionality: torch.Size([4, 88, 27, 70])\n",
      "Batched output test dimensionality: torch.Size([4, 88, 27, 70])\n"
     ]
    }
   ],
   "source": [
    "batch_size = node_features_sequence_input_test.shape[0] // 4# Adjust this value based on your GPU memory capacity\n",
    "# Calculate the number of full batches\n",
    "num_full_batches_test = (node_features_sequence_input_test.size(0) // batch_size) * batch_size\n",
    "\n",
    "# Trim the input and output tensors to have a size divisible by the batch size\n",
    "trimmed_input_test = node_features_sequence_input_test[:num_full_batches_test]\n",
    "trimmed_output_test = node_features_sequence_output_test[:num_full_batches_test]\n",
    "\n",
    "# Create batches of data\n",
    "batched_input_test = trimmed_input_test.view(-1, batch_size, trimmed_input_test.size(1), trimmed_input_test.size(2))\n",
    "batched_output_test = trimmed_output_test.view(-1, batch_size, trimmed_output_test.size(1), trimmed_output_test.size(2))\n",
    "\n",
    "# Adjust the number of batches\n",
    "num_batches_test = batched_input_test.size(0)\n",
    "print(\"Batch size:\", batch_size)\n",
    "print(\"Number of full batches:\", num_full_batches_train)\n",
    "\n",
    "print(\"Batched input test dimensionality:\", batched_input_test.shape)\n",
    "print(\"Batched output test dimensionality:\", batched_output_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">**ZE EPIC MODEL**</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer model for the hybrid model\n",
    "# Input Dimension: [batch_length ,num_nodes, num_features]\n",
    "# Output Dimension: [batch_length, num_nodes, hidden_channels]\n",
    "class TransformerModule(nn.Module):\n",
    "    def __init__(self, features_channels, out_channels, transformer_layers):\n",
    "        super(TransformerModule, self).__init__()\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=features_channels, nhead=features_channels, batch_first=True),\n",
    "            num_layers=transformer_layers\n",
    "        )\n",
    "        self.output_linear = nn.Linear(features_channels, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        temporal_outputs = self.transformer_encoder(x)  # Shape: [num_timesteps, num_nodes, hidden_channels]\n",
    "        x = self.output_linear(temporal_outputs)  # Shape: [num_timesteps, num_nodes, out_channels]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNN module for the hybrid model\n",
    "# Input Dimension: [batch_length ,num_nodes, num_features]\n",
    "# Output Dimension: [batch_length, num_nodes, hidden_channels]\n",
    "class GNNModule(torch.nn.Module):\n",
    "    def __init__(self, features_channels, hidden_channels, edge_in_channels):\n",
    "        super(GNNModule, self).__init__()\n",
    "        self.conv1 = GATConv(features_channels, hidden_channels, edge_dim=edge_in_channels)\n",
    "        self.conv2 = GraphConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.batch_norm1 = BatchNorm(hidden_channels)\n",
    "        self.batch_norm2 = BatchNorm(hidden_channels)\n",
    "        self.batch_norm3 = BatchNorm(hidden_channels)\n",
    "        self.dropout = torch.nn.Dropout(p=0.5)\n",
    "\n",
    "    def gnn_forward(self, x, edge_index, edge_attr):\n",
    "        edge_weight = 0.1 / edge_attr + 1e-6\n",
    "        x1 = self.conv1(x, edge_index, edge_weight)\n",
    "        x1 = F.relu(x1)\n",
    "        x1 = self.dropout(x1)\n",
    "\n",
    "        x2 = self.conv2(x1, edge_index)\n",
    "        x2 = F.relu(x2)\n",
    "        x2 = self.dropout(x2)\n",
    "\n",
    "        x3 = self.conv3(x2, edge_index, edge_weight)\n",
    "        x3 = F.relu(x3)\n",
    "        x3 = self.dropout(x3)\n",
    "\n",
    "        x = x1 + x2 + x3  # Residual connection\n",
    "        return x  # Shape: [num_nodes, hidden_channels]\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        spatial_outputs = []\n",
    "        for t in range(x.size(0)):\n",
    "            x_t = self.gnn_forward(x[t], edge_index, edge_attr)\n",
    "            spatial_outputs.append(x_t)\n",
    "\n",
    "        x = torch.stack(spatial_outputs, dim=0)  # Shape: [num_timesteps, num_nodes, hidden_channels]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid model combining the Transformer and GNN modules\n",
    "# Input Dimension: [batch_length ,num_nodes, num_features]\n",
    "# Output Dimension: [batch_length, num_nodes, out_channels]\n",
    "class HybridModel_Transformer(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, features_channels, edge_in_channels, hidden_channels, transformer_layers):\n",
    "        super(HybridModel_Transformer, self).__init__()\n",
    "        self.transformer = TransformerModule(features_channels, hidden_channels, transformer_layers)\n",
    "        self.gnn = GNNModule(hidden_channels, hidden_channels, edge_in_channels)\n",
    "        self.output_layer = torch.nn.Linear(hidden_channels, features_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # Project the input features to the hidden dimension\n",
    "        # Transform the input features using the transformer\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # Pass the transformed features to the GNN\n",
    "        x = self.gnn(x, edge_index, edge_attr)\n",
    "\n",
    "        # Pass the GNN outputs through a linear layer\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridModel_GNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, features_channels, edge_in_channels, hidden_channels, transformer_layers):\n",
    "        super(HybridModel_GNN, self).__init__()\n",
    "        self.gnn = GNNModule(features_channels, hidden_channels, edge_in_channels)\n",
    "        self.transformer = TransformerModule(hidden_channels, features_channels, transformer_layers)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # Apply GNN first\n",
    "        x = self.gnn(x, edge_index, edge_attr)\n",
    "\n",
    "        # Apply transformer\n",
    "        x = self.transformer(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleModel_Transformer(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, features_channels, edge_in_channels, hidden_channels, transformer_layers):\n",
    "        super(SingleModel_Transformer, self).__init__()\n",
    "        self.transformer = TransformerModule(features_channels, hidden_channels, transformer_layers)\n",
    "        self.output_layer = torch.nn.Linear(hidden_channels, features_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # Project the input features to the hidden dimension\n",
    "        # Transform the input features using the transformer\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # Pass the GNN outputs through a linear layer\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleModel_GNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, features_channels, edge_in_channels, hidden_channels, transformer_layers):\n",
    "        super(SingleModel_GNN, self).__init__()\n",
    "        self.gnn = GNNModule(features_channels, hidden_channels, edge_in_channels)\n",
    "        self.output_layer = torch.nn.Linear(hidden_channels, features_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # Apply GNN first\n",
    "        x = self.gnn(x, edge_index, edge_attr)\n",
    "\n",
    "        # Apply transformer\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">**ZE EPIC MODEL SETUP**</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of epochs: 200\n",
      "Learning rate: 0.01\n",
      "Scheduler mode: min\n",
      "Scheduler factor: 0.5\n",
      "Scheduler patience: 10\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters\n",
    "num_epochs = 200  # Adjust the number of epochs as needed\n",
    "learning_rate = 0.01\n",
    "scheduler_mode = 'min'\n",
    "scheduler_factor = 0.5\n",
    "scheduler_patience = 10\n",
    "\n",
    "#Print all parameter \n",
    "print(\"Number of epochs:\", num_epochs)\n",
    "print(\"Learning rate:\", learning_rate)\n",
    "print(\"Scheduler mode:\", scheduler_mode)\n",
    "print(\"Scheduler factor:\", scheduler_factor)\n",
    "print(\"Scheduler patience:\", scheduler_patience)\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">**ZE EPIC MODEL TRAIN**</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, scheduler, loss_fn, num_epochs, num_batches_train, batched_input_train, batched_output_train, device):\n",
    "    epoch_losses = []\n",
    "    epoch_maes = []\n",
    "\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        total_mae = 0\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for b in range(num_batches_train):\n",
    "            # Get the batched node features and desired output\n",
    "            node_features_batch = batched_input_train[b].to(device)\n",
    "            desired_output_batch = batched_output_train[b].to(device)\n",
    "            # Forward pass with batch_size parameter\n",
    "            model_output_batch = model(node_features_batch, edge_index, edge_attr)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn(model_output_batch, desired_output_batch)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the optimizer\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Compute MAE for debugging\n",
    "            mae = torch.mean(torch.abs(model_output_batch - desired_output_batch))\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_mae += mae.item()\n",
    "\n",
    "        average_loss = total_loss / num_batches_train\n",
    "        average_mae = total_mae / num_batches_train\n",
    "        epoch_losses.append(average_loss)\n",
    "        epoch_maes.append(average_mae)\n",
    "\n",
    "        scheduler.step(average_loss)\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        current_patience = scheduler.num_bad_epochs\n",
    "        print(f\"Epoch {epoch+1}, Average Loss: {average_loss}, Average MAE: {average_mae}, Learning Rate: {current_lr}, Current Patience: {current_patience}\")\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return epoch_losses, epoch_maes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Channels: 27\n",
      "Features Channels: 70\n",
      "Output Channels: 27\n",
      "Edge Input Channels: 1\n",
      "Hidden Channels: 256\n",
      "Transformer Layers: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/HDD/RyanFolder/projects/Weather-Predition-Project/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 317.75184631347656, Average MAE: 13.082350254058838, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 2, Average Loss: 7502.842514038086, Average MAE: 41.76781725883484, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 3, Average Loss: 134.92889404296875, Average MAE: 7.622934341430664, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 4, Average Loss: 165.15779876708984, Average MAE: 8.178812026977539, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 5, Average Loss: 159.20421600341797, Average MAE: 7.361576557159424, Learning Rate: 0.01, Current Patience: 2\n",
      "Epoch 6, Average Loss: 129.49358367919922, Average MAE: 7.157864809036255, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 7, Average Loss: 93.8619384765625, Average MAE: 5.907185792922974, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 8, Average Loss: 68.80436325073242, Average MAE: 5.184405565261841, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 9, Average Loss: 62.39634132385254, Average MAE: 4.474942207336426, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 10, Average Loss: 68.88326263427734, Average MAE: 4.765790581703186, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 11, Average Loss: 58.158111572265625, Average MAE: 4.350295186042786, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 12, Average Loss: 45.87002182006836, Average MAE: 3.3970346450805664, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 13, Average Loss: 42.958757400512695, Average MAE: 3.767038583755493, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 14, Average Loss: 35.79836654663086, Average MAE: 3.244797706604004, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 15, Average Loss: 35.85734748840332, Average MAE: 3.2309616804122925, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 16, Average Loss: 31.607417106628418, Average MAE: 2.903023362159729, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 17, Average Loss: 29.307333946228027, Average MAE: 2.550489902496338, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 18, Average Loss: 27.926166534423828, Average MAE: 2.605210065841675, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 19, Average Loss: 26.383182525634766, Average MAE: 2.5145808458328247, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 20, Average Loss: 25.488279342651367, Average MAE: 2.4168978929519653, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 21, Average Loss: 23.827492713928223, Average MAE: 2.213710904121399, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 22, Average Loss: 23.126795768737793, Average MAE: 2.1166282892227173, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 23, Average Loss: 22.089240074157715, Average MAE: 2.057984232902527, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 24, Average Loss: 21.44784927368164, Average MAE: 2.0136791467666626, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 25, Average Loss: 20.829319953918457, Average MAE: 1.9208025932312012, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 26, Average Loss: 20.166126251220703, Average MAE: 1.8116483092308044, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 27, Average Loss: 19.782334327697754, Average MAE: 1.771082580089569, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 28, Average Loss: 19.345335006713867, Average MAE: 1.7211580276489258, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 29, Average Loss: 19.043910026550293, Average MAE: 1.6875123381614685, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 30, Average Loss: 18.791940689086914, Average MAE: 1.6472323536872864, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 31, Average Loss: 18.416309356689453, Average MAE: 1.600277602672577, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 32, Average Loss: 18.393936157226562, Average MAE: 1.5597902536392212, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 33, Average Loss: 17.881030082702637, Average MAE: 1.5315306186676025, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 34, Average Loss: 17.805316925048828, Average MAE: 1.5074077248573303, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 35, Average Loss: 17.842090606689453, Average MAE: 1.481078028678894, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 36, Average Loss: 17.46299171447754, Average MAE: 1.4431844353675842, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 37, Average Loss: 17.108001708984375, Average MAE: 1.411088228225708, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 38, Average Loss: 17.53545379638672, Average MAE: 1.4149793982505798, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 39, Average Loss: 17.34385871887207, Average MAE: 1.3984225988388062, Learning Rate: 0.01, Current Patience: 2\n",
      "Epoch 40, Average Loss: 17.24648952484131, Average MAE: 1.374749481678009, Learning Rate: 0.01, Current Patience: 3\n",
      "Epoch 41, Average Loss: 17.906764030456543, Average MAE: 1.3804101347923279, Learning Rate: 0.01, Current Patience: 4\n",
      "Epoch 42, Average Loss: 17.666017532348633, Average MAE: 1.3739580512046814, Learning Rate: 0.01, Current Patience: 5\n",
      "Epoch 43, Average Loss: 19.404635429382324, Average MAE: 1.4167749285697937, Learning Rate: 0.01, Current Patience: 6\n",
      "Epoch 44, Average Loss: 19.351932525634766, Average MAE: 1.4304922819137573, Learning Rate: 0.01, Current Patience: 7\n",
      "Epoch 45, Average Loss: 18.090869903564453, Average MAE: 1.3649234175682068, Learning Rate: 0.01, Current Patience: 8\n",
      "Epoch 46, Average Loss: 18.268339157104492, Average MAE: 1.3845144510269165, Learning Rate: 0.01, Current Patience: 9\n",
      "Epoch 47, Average Loss: 17.91403102874756, Average MAE: 1.3495935797691345, Learning Rate: 0.01, Current Patience: 10\n",
      "Epoch 48, Average Loss: 17.597936630249023, Average MAE: 1.3326144814491272, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 49, Average Loss: 17.645344734191895, Average MAE: 1.3121920228004456, Learning Rate: 0.005, Current Patience: 1\n",
      "Epoch 50, Average Loss: 17.278459548950195, Average MAE: 1.2997509241104126, Learning Rate: 0.005, Current Patience: 2\n",
      "Epoch 51, Average Loss: 17.416739463806152, Average MAE: 1.2890841960906982, Learning Rate: 0.005, Current Patience: 3\n",
      "Epoch 52, Average Loss: 17.204792976379395, Average MAE: 1.2732974886894226, Learning Rate: 0.005, Current Patience: 4\n",
      "Epoch 53, Average Loss: 17.231624603271484, Average MAE: 1.2724250555038452, Learning Rate: 0.005, Current Patience: 5\n",
      "Epoch 54, Average Loss: 17.071969985961914, Average MAE: 1.2610327005386353, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 55, Average Loss: 16.985661506652832, Average MAE: 1.2627061009407043, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 56, Average Loss: 16.940763473510742, Average MAE: 1.2502442598342896, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 57, Average Loss: 16.924599647521973, Average MAE: 1.2556710243225098, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 58, Average Loss: 16.88857936859131, Average MAE: 1.2483947277069092, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 59, Average Loss: 16.72114086151123, Average MAE: 1.2426121830940247, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 60, Average Loss: 16.726746559143066, Average MAE: 1.240431785583496, Learning Rate: 0.005, Current Patience: 1\n",
      "Epoch 61, Average Loss: 16.797030448913574, Average MAE: 1.234558343887329, Learning Rate: 0.005, Current Patience: 2\n",
      "Epoch 62, Average Loss: 16.692435264587402, Average MAE: 1.2329055666923523, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 63, Average Loss: 16.608702659606934, Average MAE: 1.2245745062828064, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 64, Average Loss: 16.555115699768066, Average MAE: 1.2253539562225342, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 65, Average Loss: 16.48524570465088, Average MAE: 1.2169296145439148, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 66, Average Loss: 16.423065185546875, Average MAE: 1.2144068479537964, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 67, Average Loss: 16.403099536895752, Average MAE: 1.2116975784301758, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 68, Average Loss: 16.43362331390381, Average MAE: 1.2099542617797852, Learning Rate: 0.005, Current Patience: 1\n",
      "Epoch 69, Average Loss: 16.475120067596436, Average MAE: 1.2092692255973816, Learning Rate: 0.005, Current Patience: 2\n",
      "Epoch 70, Average Loss: 16.26646852493286, Average MAE: 1.2033909559249878, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 71, Average Loss: 16.399279594421387, Average MAE: 1.2045817971229553, Learning Rate: 0.005, Current Patience: 1\n",
      "Epoch 72, Average Loss: 16.384679794311523, Average MAE: 1.1998682022094727, Learning Rate: 0.005, Current Patience: 2\n",
      "Epoch 73, Average Loss: 16.310243606567383, Average MAE: 1.19809091091156, Learning Rate: 0.005, Current Patience: 3\n",
      "Epoch 74, Average Loss: 16.315608978271484, Average MAE: 1.1951743960380554, Learning Rate: 0.005, Current Patience: 4\n",
      "Epoch 75, Average Loss: 16.29957103729248, Average MAE: 1.194106936454773, Learning Rate: 0.005, Current Patience: 5\n",
      "Epoch 76, Average Loss: 16.165278434753418, Average MAE: 1.1871908903121948, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 77, Average Loss: 16.23261070251465, Average MAE: 1.1858654022216797, Learning Rate: 0.005, Current Patience: 1\n",
      "Epoch 78, Average Loss: 16.243617057800293, Average MAE: 1.1853402256965637, Learning Rate: 0.005, Current Patience: 2\n",
      "Epoch 79, Average Loss: 16.182534217834473, Average MAE: 1.1823071837425232, Learning Rate: 0.005, Current Patience: 3\n",
      "Epoch 80, Average Loss: 16.114296436309814, Average MAE: 1.177798867225647, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 81, Average Loss: 16.150527954101562, Average MAE: 1.17518150806427, Learning Rate: 0.005, Current Patience: 1\n",
      "Epoch 82, Average Loss: 16.00498056411743, Average MAE: 1.1716071367263794, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 83, Average Loss: 16.093066692352295, Average MAE: 1.1713612079620361, Learning Rate: 0.005, Current Patience: 1\n",
      "Epoch 84, Average Loss: 16.03507137298584, Average MAE: 1.1715636253356934, Learning Rate: 0.005, Current Patience: 2\n",
      "Epoch 85, Average Loss: 16.104356288909912, Average MAE: 1.1681857109069824, Learning Rate: 0.005, Current Patience: 3\n",
      "Epoch 86, Average Loss: 16.06912136077881, Average MAE: 1.1658380627632141, Learning Rate: 0.005, Current Patience: 4\n",
      "Epoch 87, Average Loss: 15.969582080841064, Average MAE: 1.1633979678153992, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 88, Average Loss: 16.038729190826416, Average MAE: 1.1602901816368103, Learning Rate: 0.005, Current Patience: 1\n",
      "Epoch 89, Average Loss: 15.973479747772217, Average MAE: 1.1584660410881042, Learning Rate: 0.005, Current Patience: 2\n",
      "Epoch 90, Average Loss: 15.955416202545166, Average MAE: 1.1535082459449768, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 91, Average Loss: 16.016808032989502, Average MAE: 1.1587032675743103, Learning Rate: 0.005, Current Patience: 1\n",
      "Epoch 92, Average Loss: 15.881936073303223, Average MAE: 1.146754801273346, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 93, Average Loss: 15.961977481842041, Average MAE: 1.155379593372345, Learning Rate: 0.005, Current Patience: 1\n",
      "Epoch 94, Average Loss: 15.965359210968018, Average MAE: 1.1476720571517944, Learning Rate: 0.005, Current Patience: 2\n",
      "Epoch 95, Average Loss: 15.95577335357666, Average MAE: 1.1522266864776611, Learning Rate: 0.005, Current Patience: 3\n",
      "Epoch 96, Average Loss: 15.8650484085083, Average MAE: 1.1443111896514893, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 97, Average Loss: 15.811630249023438, Average MAE: 1.140564739704132, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 98, Average Loss: 15.89852523803711, Average MAE: 1.147399127483368, Learning Rate: 0.005, Current Patience: 1\n",
      "Epoch 99, Average Loss: 15.849210739135742, Average MAE: 1.1366538405418396, Learning Rate: 0.005, Current Patience: 2\n",
      "Epoch 100, Average Loss: 15.82958173751831, Average MAE: 1.138272225856781, Learning Rate: 0.005, Current Patience: 3\n",
      "Epoch 101, Average Loss: 15.91248369216919, Average MAE: 1.137398362159729, Learning Rate: 0.005, Current Patience: 4\n",
      "Epoch 102, Average Loss: 15.878410816192627, Average MAE: 1.1378656029701233, Learning Rate: 0.005, Current Patience: 5\n",
      "Epoch 103, Average Loss: 15.839937210083008, Average MAE: 1.134639024734497, Learning Rate: 0.005, Current Patience: 6\n",
      "Epoch 104, Average Loss: 15.818358421325684, Average MAE: 1.1311444640159607, Learning Rate: 0.005, Current Patience: 7\n",
      "Epoch 105, Average Loss: 15.715156078338623, Average MAE: 1.1287806630134583, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 106, Average Loss: 15.75043535232544, Average MAE: 1.1263114213943481, Learning Rate: 0.005, Current Patience: 1\n",
      "Epoch 107, Average Loss: 15.873657703399658, Average MAE: 1.1314714550971985, Learning Rate: 0.005, Current Patience: 2\n",
      "Epoch 108, Average Loss: 15.705963611602783, Average MAE: 1.1250423192977905, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 109, Average Loss: 15.68330430984497, Average MAE: 1.1204949617385864, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 110, Average Loss: 15.730886936187744, Average MAE: 1.1215259432792664, Learning Rate: 0.005, Current Patience: 1\n",
      "Epoch 111, Average Loss: 15.72925329208374, Average MAE: 1.11781507730484, Learning Rate: 0.005, Current Patience: 2\n",
      "Epoch 112, Average Loss: 15.661872386932373, Average MAE: 1.11713707447052, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 113, Average Loss: 15.62786054611206, Average MAE: 1.113396942615509, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 114, Average Loss: 15.641603946685791, Average MAE: 1.113829255104065, Learning Rate: 0.005, Current Patience: 1\n",
      "Epoch 115, Average Loss: 15.754655838012695, Average MAE: 1.11846262216568, Learning Rate: 0.005, Current Patience: 2\n",
      "Epoch 116, Average Loss: 15.606856346130371, Average MAE: 1.1087785959243774, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 117, Average Loss: 15.704848766326904, Average MAE: 1.1134859919548035, Learning Rate: 0.005, Current Patience: 1\n",
      "Epoch 118, Average Loss: 15.592456340789795, Average MAE: 1.1092984676361084, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 119, Average Loss: 15.604902267456055, Average MAE: 1.1092341542243958, Learning Rate: 0.005, Current Patience: 1\n",
      "Epoch 120, Average Loss: 15.683711051940918, Average MAE: 1.1123738288879395, Learning Rate: 0.005, Current Patience: 2\n",
      "Epoch 121, Average Loss: 15.637931823730469, Average MAE: 1.1102136373519897, Learning Rate: 0.005, Current Patience: 3\n",
      "Epoch 122, Average Loss: 15.601090908050537, Average MAE: 1.1061608791351318, Learning Rate: 0.005, Current Patience: 4\n",
      "Epoch 123, Average Loss: 15.587013721466064, Average MAE: 1.1076580286026, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 124, Average Loss: 15.567977905273438, Average MAE: 1.1023520231246948, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 125, Average Loss: 15.621167659759521, Average MAE: 1.1041505932807922, Learning Rate: 0.005, Current Patience: 1\n",
      "Epoch 126, Average Loss: 15.580236911773682, Average MAE: 1.1029205322265625, Learning Rate: 0.005, Current Patience: 2\n",
      "Epoch 127, Average Loss: 15.568370342254639, Average MAE: 1.0991917848587036, Learning Rate: 0.005, Current Patience: 3\n",
      "Epoch 128, Average Loss: 15.570298194885254, Average MAE: 1.0984961986541748, Learning Rate: 0.005, Current Patience: 4\n",
      "Epoch 129, Average Loss: 15.637369155883789, Average MAE: 1.1028488874435425, Learning Rate: 0.005, Current Patience: 5\n",
      "Epoch 130, Average Loss: 15.55646562576294, Average MAE: 1.0952650308609009, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 131, Average Loss: 15.546449661254883, Average MAE: 1.0974518656730652, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 132, Average Loss: 15.535906791687012, Average MAE: 1.0958253145217896, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 133, Average Loss: 15.58087158203125, Average MAE: 1.0972183346748352, Learning Rate: 0.005, Current Patience: 1\n",
      "Epoch 134, Average Loss: 15.595223903656006, Average MAE: 1.0956810116767883, Learning Rate: 0.005, Current Patience: 2\n",
      "Epoch 135, Average Loss: 15.477932929992676, Average MAE: 1.093515932559967, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 136, Average Loss: 15.49968433380127, Average MAE: 1.0935375690460205, Learning Rate: 0.005, Current Patience: 1\n",
      "Epoch 137, Average Loss: 15.546838760375977, Average MAE: 1.0946518182754517, Learning Rate: 0.005, Current Patience: 2\n",
      "Epoch 138, Average Loss: 15.528106212615967, Average MAE: 1.0902372598648071, Learning Rate: 0.005, Current Patience: 3\n",
      "Epoch 139, Average Loss: 15.542675018310547, Average MAE: 1.092653512954712, Learning Rate: 0.005, Current Patience: 4\n",
      "Epoch 140, Average Loss: 15.549695491790771, Average MAE: 1.0906598567962646, Learning Rate: 0.005, Current Patience: 5\n",
      "Epoch 141, Average Loss: 15.538114547729492, Average MAE: 1.0895599126815796, Learning Rate: 0.005, Current Patience: 6\n",
      "Epoch 142, Average Loss: 15.543497085571289, Average MAE: 1.0889130234718323, Learning Rate: 0.005, Current Patience: 7\n",
      "Epoch 143, Average Loss: 15.568188667297363, Average MAE: 1.095006287097931, Learning Rate: 0.005, Current Patience: 8\n",
      "Epoch 144, Average Loss: 16.01771593093872, Average MAE: 1.1079355478286743, Learning Rate: 0.005, Current Patience: 9\n",
      "Epoch 145, Average Loss: 16.298931121826172, Average MAE: 1.1064093708992004, Learning Rate: 0.005, Current Patience: 10\n",
      "Epoch 146, Average Loss: 15.840035915374756, Average MAE: 1.0974037647247314, Learning Rate: 0.0025, Current Patience: 0\n",
      "Epoch 147, Average Loss: 16.49320697784424, Average MAE: 1.1247973442077637, Learning Rate: 0.0025, Current Patience: 1\n",
      "Epoch 148, Average Loss: 16.282934188842773, Average MAE: 1.1043806672096252, Learning Rate: 0.0025, Current Patience: 2\n",
      "Epoch 149, Average Loss: 16.293031692504883, Average MAE: 1.1067193150520325, Learning Rate: 0.0025, Current Patience: 3\n",
      "Epoch 150, Average Loss: 16.309541702270508, Average MAE: 1.0879446268081665, Learning Rate: 0.0025, Current Patience: 4\n",
      "Epoch 151, Average Loss: 16.189687252044678, Average MAE: 1.1054723262786865, Learning Rate: 0.0025, Current Patience: 5\n",
      "Epoch 152, Average Loss: 15.925497055053711, Average MAE: 1.0802459120750427, Learning Rate: 0.0025, Current Patience: 6\n",
      "Epoch 153, Average Loss: 15.877525806427002, Average MAE: 1.0958028435707092, Learning Rate: 0.0025, Current Patience: 7\n",
      "Epoch 154, Average Loss: 15.516182899475098, Average MAE: 1.0762264132499695, Learning Rate: 0.0025, Current Patience: 8\n",
      "Epoch 155, Average Loss: 15.786815643310547, Average MAE: 1.0824511647224426, Learning Rate: 0.0025, Current Patience: 9\n",
      "Epoch 156, Average Loss: 15.779009342193604, Average MAE: 1.0855135321617126, Learning Rate: 0.0025, Current Patience: 10\n",
      "Epoch 157, Average Loss: 15.63146162033081, Average MAE: 1.0825903415679932, Learning Rate: 0.00125, Current Patience: 0\n",
      "Epoch 158, Average Loss: 15.624006748199463, Average MAE: 1.0976061820983887, Learning Rate: 0.00125, Current Patience: 1\n",
      "Epoch 159, Average Loss: 15.519208908081055, Average MAE: 1.081213653087616, Learning Rate: 0.00125, Current Patience: 2\n",
      "Epoch 160, Average Loss: 15.556729793548584, Average MAE: 1.0801568031311035, Learning Rate: 0.00125, Current Patience: 3\n",
      "Epoch 161, Average Loss: 15.59779405593872, Average MAE: 1.0879302620887756, Learning Rate: 0.00125, Current Patience: 4\n",
      "Epoch 162, Average Loss: 15.490261554718018, Average MAE: 1.0817936062812805, Learning Rate: 0.00125, Current Patience: 5\n",
      "Epoch 163, Average Loss: 15.542627811431885, Average MAE: 1.085826814174652, Learning Rate: 0.00125, Current Patience: 6\n",
      "Epoch 164, Average Loss: 15.451880931854248, Average MAE: 1.083864152431488, Learning Rate: 0.00125, Current Patience: 0\n",
      "Epoch 165, Average Loss: 15.479270458221436, Average MAE: 1.0811210870742798, Learning Rate: 0.00125, Current Patience: 1\n",
      "Epoch 166, Average Loss: 15.50935697555542, Average MAE: 1.0824581384658813, Learning Rate: 0.00125, Current Patience: 2\n",
      "Epoch 167, Average Loss: 15.499972820281982, Average MAE: 1.079245388507843, Learning Rate: 0.00125, Current Patience: 3\n",
      "Epoch 168, Average Loss: 15.421246528625488, Average MAE: 1.0783510208129883, Learning Rate: 0.00125, Current Patience: 0\n",
      "Epoch 169, Average Loss: 15.426460266113281, Average MAE: 1.0831413269042969, Learning Rate: 0.00125, Current Patience: 1\n",
      "Epoch 170, Average Loss: 15.436041831970215, Average MAE: 1.080706000328064, Learning Rate: 0.00125, Current Patience: 2\n",
      "Epoch 171, Average Loss: 15.452431201934814, Average MAE: 1.0779422521591187, Learning Rate: 0.00125, Current Patience: 3\n",
      "Epoch 172, Average Loss: 15.419085502624512, Average MAE: 1.07908034324646, Learning Rate: 0.00125, Current Patience: 0\n",
      "Epoch 173, Average Loss: 15.51457405090332, Average MAE: 1.0815043449401855, Learning Rate: 0.00125, Current Patience: 1\n",
      "Epoch 174, Average Loss: 15.424468517303467, Average MAE: 1.0803304314613342, Learning Rate: 0.00125, Current Patience: 2\n",
      "Epoch 175, Average Loss: 15.456174850463867, Average MAE: 1.0804762840270996, Learning Rate: 0.00125, Current Patience: 3\n",
      "Epoch 176, Average Loss: 15.382806301116943, Average MAE: 1.0780361890792847, Learning Rate: 0.00125, Current Patience: 0\n",
      "Epoch 177, Average Loss: 15.439492225646973, Average MAE: 1.0798529386520386, Learning Rate: 0.00125, Current Patience: 1\n",
      "Epoch 178, Average Loss: 15.423058986663818, Average MAE: 1.079079270362854, Learning Rate: 0.00125, Current Patience: 2\n",
      "Epoch 179, Average Loss: 15.365446090698242, Average MAE: 1.0777223706245422, Learning Rate: 0.00125, Current Patience: 0\n",
      "Epoch 180, Average Loss: 15.514472484588623, Average MAE: 1.0843409895896912, Learning Rate: 0.00125, Current Patience: 1\n",
      "Epoch 181, Average Loss: 15.476112842559814, Average MAE: 1.0823022723197937, Learning Rate: 0.00125, Current Patience: 2\n",
      "Epoch 182, Average Loss: 15.349712371826172, Average MAE: 1.0769224762916565, Learning Rate: 0.00125, Current Patience: 0\n",
      "Epoch 183, Average Loss: 15.366859436035156, Average MAE: 1.0772032737731934, Learning Rate: 0.00125, Current Patience: 1\n",
      "Epoch 184, Average Loss: 15.438783168792725, Average MAE: 1.0810562372207642, Learning Rate: 0.00125, Current Patience: 2\n",
      "Epoch 185, Average Loss: 15.494542598724365, Average MAE: 1.0804599523544312, Learning Rate: 0.00125, Current Patience: 3\n",
      "Epoch 186, Average Loss: 15.465850353240967, Average MAE: 1.0798815488815308, Learning Rate: 0.00125, Current Patience: 4\n",
      "Epoch 187, Average Loss: 15.408586978912354, Average MAE: 1.0791501998901367, Learning Rate: 0.00125, Current Patience: 5\n",
      "Epoch 188, Average Loss: 15.390640258789062, Average MAE: 1.0781834721565247, Learning Rate: 0.00125, Current Patience: 6\n",
      "Epoch 189, Average Loss: 15.421297550201416, Average MAE: 1.0767391324043274, Learning Rate: 0.00125, Current Patience: 7\n",
      "Epoch 190, Average Loss: 15.461796283721924, Average MAE: 1.0803945660591125, Learning Rate: 0.00125, Current Patience: 8\n",
      "Epoch 191, Average Loss: 15.43834924697876, Average MAE: 1.0798598527908325, Learning Rate: 0.00125, Current Patience: 9\n",
      "Epoch 192, Average Loss: 15.342501640319824, Average MAE: 1.0733096599578857, Learning Rate: 0.00125, Current Patience: 0\n",
      "Epoch 193, Average Loss: 15.43989896774292, Average MAE: 1.0778617858886719, Learning Rate: 0.00125, Current Patience: 1\n",
      "Epoch 194, Average Loss: 15.419536113739014, Average MAE: 1.0769652724266052, Learning Rate: 0.00125, Current Patience: 2\n",
      "Epoch 195, Average Loss: 15.443233489990234, Average MAE: 1.0810039043426514, Learning Rate: 0.00125, Current Patience: 3\n",
      "Epoch 196, Average Loss: 15.418599605560303, Average MAE: 1.0760480761528015, Learning Rate: 0.00125, Current Patience: 4\n",
      "Epoch 197, Average Loss: 15.391342639923096, Average MAE: 1.0761738419532776, Learning Rate: 0.00125, Current Patience: 5\n",
      "Epoch 198, Average Loss: 15.425062656402588, Average MAE: 1.079477071762085, Learning Rate: 0.00125, Current Patience: 6\n",
      "Epoch 199, Average Loss: 15.350679874420166, Average MAE: 1.0756141543388367, Learning Rate: 0.00125, Current Patience: 7\n",
      "Epoch 200, Average Loss: 15.467848300933838, Average MAE: 1.0790302753448486, Learning Rate: 0.00125, Current Patience: 8\n"
     ]
    }
   ],
   "source": [
    "# Call the function\n",
    "# Define the parameters\n",
    "in_channels = batched_input_train.shape[2]\n",
    "features_channels = batched_input_train.shape[3]\n",
    "out_channels = in_channels  \n",
    "edge_in_channels = 1\n",
    "hidden_channels = 256\n",
    "transformer_layers = 3\n",
    "\n",
    "# Print the parameters to verify\n",
    "print(\"Input Channels:\", in_channels)\n",
    "print(\"Features Channels:\", features_channels)\n",
    "print(\"Output Channels:\", out_channels)\n",
    "print(\"Edge Input Channels:\", edge_in_channels)\n",
    "print(\"Hidden Channels:\", hidden_channels)\n",
    "print(\"Transformer Layers:\", transformer_layers)\n",
    "\n",
    "model_transformer_gnn = HybridModel_Transformer(in_channels, out_channels, features_channels, edge_in_channels, hidden_channels, transformer_layers).to(device)\n",
    "\n",
    "# Define the optimizer and loss function for model_transformer_gnn\n",
    "optimizer_transformer_gnn = torch.optim.Adam(model_transformer_gnn.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "scheduler_transformer_gnn = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_transformer_gnn, mode=scheduler_mode, factor=scheduler_factor, patience=scheduler_patience, verbose=True, min_lr=1e-5)\n",
    "\n",
    "transformer_gnn_epoch_losses, transformer_gnn_epoch_maes = train_model(model_transformer_gnn, optimizer_transformer_gnn, scheduler_transformer_gnn, loss_fn, num_epochs, num_batches_train, batched_input_train, batched_output_train, device)\n",
    "\n",
    "torch.save(model_transformer_gnn.state_dict(), 'model_transformer_gnn{hidden_channels}_layers{transformer_layers}.pth')\n",
    "del model_transformer_gnn\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Channels: 27\n",
      "Features Channels: 70\n",
      "Output Channels: 27\n",
      "Edge Input Channels: 1\n",
      "Hidden Channels: 256\n",
      "Transformer Layers: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/HDD/RyanFolder/projects/Weather-Predition-Project/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 203.810546875, Average MAE: 4.701146602630615, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 2, Average Loss: 164.6363754272461, Average MAE: 4.626003265380859, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 3, Average Loss: 133.3765983581543, Average MAE: 3.9108327627182007, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 4, Average Loss: 106.04197311401367, Average MAE: 3.4535073041915894, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 5, Average Loss: 80.67039108276367, Average MAE: 2.6524375677108765, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 6, Average Loss: 59.59385108947754, Average MAE: 2.2307552099227905, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 7, Average Loss: 43.11870574951172, Average MAE: 1.6981468200683594, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 8, Average Loss: 31.873056411743164, Average MAE: 1.5451809763908386, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 9, Average Loss: 25.29386043548584, Average MAE: 1.5154548287391663, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 10, Average Loss: 22.003767013549805, Average MAE: 1.5012832283973694, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 11, Average Loss: 20.09159755706787, Average MAE: 1.476105511188507, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 12, Average Loss: 17.74412441253662, Average MAE: 1.3454866409301758, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 13, Average Loss: 15.196524620056152, Average MAE: 1.1329212188720703, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 14, Average Loss: 15.020723342895508, Average MAE: 1.166783332824707, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 15, Average Loss: 14.875586986541748, Average MAE: 1.2029134035110474, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 16, Average Loss: 14.48814868927002, Average MAE: 1.1049041748046875, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 17, Average Loss: 14.818785190582275, Average MAE: 1.0509595274925232, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 18, Average Loss: 14.483880996704102, Average MAE: 1.041497528553009, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 19, Average Loss: 14.197686195373535, Average MAE: 1.0664082169532776, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 20, Average Loss: 14.25255298614502, Average MAE: 1.0672715306282043, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 21, Average Loss: 14.097824096679688, Average MAE: 1.0122892260551453, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 22, Average Loss: 14.012129306793213, Average MAE: 0.9835106730461121, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 23, Average Loss: 14.043869972229004, Average MAE: 1.0122818350791931, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 24, Average Loss: 13.973833084106445, Average MAE: 1.0131852328777313, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 25, Average Loss: 13.850443363189697, Average MAE: 0.9906782507896423, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 26, Average Loss: 13.812739372253418, Average MAE: 0.9709506928920746, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 27, Average Loss: 13.74523401260376, Average MAE: 0.9650700390338898, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 28, Average Loss: 13.681882858276367, Average MAE: 0.9600320756435394, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 29, Average Loss: 13.687243461608887, Average MAE: 0.9540175199508667, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 30, Average Loss: 13.694551944732666, Average MAE: 0.9502741098403931, Learning Rate: 0.01, Current Patience: 2\n",
      "Epoch 31, Average Loss: 13.69584035873413, Average MAE: 0.9540722370147705, Learning Rate: 0.01, Current Patience: 3\n",
      "Epoch 32, Average Loss: 13.696930408477783, Average MAE: 0.9611513912677765, Learning Rate: 0.01, Current Patience: 4\n",
      "Epoch 33, Average Loss: 13.69352912902832, Average MAE: 0.959310919046402, Learning Rate: 0.01, Current Patience: 5\n",
      "Epoch 34, Average Loss: 13.673839092254639, Average MAE: 0.9491955637931824, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 35, Average Loss: 13.660539150238037, Average MAE: 0.9410344958305359, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 36, Average Loss: 13.64459753036499, Average MAE: 0.937098354101181, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 37, Average Loss: 13.63958215713501, Average MAE: 0.9353512227535248, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 38, Average Loss: 13.635226249694824, Average MAE: 0.9348386228084564, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 39, Average Loss: 13.638993740081787, Average MAE: 0.9326904118061066, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 40, Average Loss: 13.631500720977783, Average MAE: 0.9312838912010193, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 41, Average Loss: 13.62583875656128, Average MAE: 0.9277590215206146, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 42, Average Loss: 13.632081508636475, Average MAE: 0.9248811304569244, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 43, Average Loss: 13.625317573547363, Average MAE: 0.9225601255893707, Learning Rate: 0.01, Current Patience: 2\n",
      "Epoch 44, Average Loss: 13.62786054611206, Average MAE: 0.9219160377979279, Learning Rate: 0.01, Current Patience: 3\n",
      "Epoch 45, Average Loss: 13.62852144241333, Average MAE: 0.9214773774147034, Learning Rate: 0.01, Current Patience: 4\n",
      "Epoch 46, Average Loss: 13.632081031799316, Average MAE: 0.9198404550552368, Learning Rate: 0.01, Current Patience: 5\n",
      "Epoch 47, Average Loss: 13.625406265258789, Average MAE: 0.9171176552772522, Learning Rate: 0.01, Current Patience: 6\n",
      "Epoch 48, Average Loss: 13.622602939605713, Average MAE: 0.9154670536518097, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 49, Average Loss: 13.62435007095337, Average MAE: 0.9146896600723267, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 50, Average Loss: 13.6234450340271, Average MAE: 0.9140746891498566, Learning Rate: 0.01, Current Patience: 2\n",
      "Epoch 51, Average Loss: 13.61634874343872, Average MAE: 0.9126458168029785, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 52, Average Loss: 13.62199354171753, Average MAE: 0.9116232991218567, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 53, Average Loss: 13.623774528503418, Average MAE: 0.9101262092590332, Learning Rate: 0.01, Current Patience: 2\n",
      "Epoch 54, Average Loss: 13.61205005645752, Average MAE: 0.9084352850914001, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 55, Average Loss: 13.618518829345703, Average MAE: 0.9075631201267242, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 56, Average Loss: 13.612650871276855, Average MAE: 0.9063342213630676, Learning Rate: 0.01, Current Patience: 2\n",
      "Epoch 57, Average Loss: 13.610647678375244, Average MAE: 0.9051770567893982, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 58, Average Loss: 13.620927810668945, Average MAE: 0.903992086648941, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 59, Average Loss: 13.610564231872559, Average MAE: 0.9023554623126984, Learning Rate: 0.01, Current Patience: 2\n",
      "Epoch 60, Average Loss: 13.612983703613281, Average MAE: 0.9011237323284149, Learning Rate: 0.01, Current Patience: 3\n",
      "Epoch 61, Average Loss: 13.612829685211182, Average MAE: 0.9003337025642395, Learning Rate: 0.01, Current Patience: 4\n",
      "Epoch 62, Average Loss: 13.607824802398682, Average MAE: 0.8994404375553131, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 63, Average Loss: 13.609420776367188, Average MAE: 0.8978836834430695, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 64, Average Loss: 13.611222267150879, Average MAE: 0.8971327841281891, Learning Rate: 0.01, Current Patience: 2\n",
      "Epoch 65, Average Loss: 13.613084316253662, Average MAE: 0.8959596455097198, Learning Rate: 0.01, Current Patience: 3\n",
      "Epoch 66, Average Loss: 13.608638763427734, Average MAE: 0.8952706754207611, Learning Rate: 0.01, Current Patience: 4\n",
      "Epoch 67, Average Loss: 13.607315063476562, Average MAE: 0.8941532969474792, Learning Rate: 0.01, Current Patience: 5\n",
      "Epoch 68, Average Loss: 13.604538440704346, Average MAE: 0.8937419950962067, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 69, Average Loss: 13.617788791656494, Average MAE: 0.8930044770240784, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 70, Average Loss: 13.609540939331055, Average MAE: 0.8914813995361328, Learning Rate: 0.01, Current Patience: 2\n",
      "Epoch 71, Average Loss: 13.615168571472168, Average MAE: 0.8908421695232391, Learning Rate: 0.01, Current Patience: 3\n",
      "Epoch 72, Average Loss: 13.599533557891846, Average MAE: 0.8898558616638184, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 73, Average Loss: 13.605855941772461, Average MAE: 0.8891446888446808, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 74, Average Loss: 13.611207008361816, Average MAE: 0.8887726068496704, Learning Rate: 0.01, Current Patience: 2\n",
      "Epoch 75, Average Loss: 13.605017185211182, Average MAE: 0.8874900043010712, Learning Rate: 0.01, Current Patience: 3\n",
      "Epoch 76, Average Loss: 13.603106498718262, Average MAE: 0.8868426978588104, Learning Rate: 0.01, Current Patience: 4\n",
      "Epoch 77, Average Loss: 13.603384971618652, Average MAE: 0.8865262866020203, Learning Rate: 0.01, Current Patience: 5\n",
      "Epoch 78, Average Loss: 13.604135990142822, Average MAE: 0.8868140876293182, Learning Rate: 0.01, Current Patience: 6\n",
      "Epoch 79, Average Loss: 13.605154514312744, Average MAE: 0.8873757421970367, Learning Rate: 0.01, Current Patience: 7\n",
      "Epoch 80, Average Loss: 13.60750675201416, Average MAE: 0.8872921168804169, Learning Rate: 0.01, Current Patience: 8\n",
      "Epoch 81, Average Loss: 13.60417652130127, Average MAE: 0.884958952665329, Learning Rate: 0.01, Current Patience: 9\n",
      "Epoch 82, Average Loss: 13.602688312530518, Average MAE: 0.8850470185279846, Learning Rate: 0.01, Current Patience: 10\n",
      "Epoch 83, Average Loss: 13.60424518585205, Average MAE: 0.8827228546142578, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 84, Average Loss: 13.607714176177979, Average MAE: 0.8817369043827057, Learning Rate: 0.005, Current Patience: 1\n",
      "Epoch 85, Average Loss: 13.599154472351074, Average MAE: 0.8811020255088806, Learning Rate: 0.005, Current Patience: 2\n",
      "Epoch 86, Average Loss: 13.599014282226562, Average MAE: 0.8804305493831635, Learning Rate: 0.005, Current Patience: 3\n",
      "Epoch 87, Average Loss: 13.606492519378662, Average MAE: 0.880716472864151, Learning Rate: 0.005, Current Patience: 4\n",
      "Epoch 88, Average Loss: 13.603872776031494, Average MAE: 0.8797123432159424, Learning Rate: 0.005, Current Patience: 5\n",
      "Epoch 89, Average Loss: 13.596939086914062, Average MAE: 0.8793100714683533, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 90, Average Loss: 13.602587223052979, Average MAE: 0.8791371285915375, Learning Rate: 0.005, Current Patience: 1\n",
      "Epoch 91, Average Loss: 13.601224899291992, Average MAE: 0.8784647583961487, Learning Rate: 0.005, Current Patience: 2\n",
      "Epoch 92, Average Loss: 13.59882640838623, Average MAE: 0.8779781758785248, Learning Rate: 0.005, Current Patience: 3\n",
      "Epoch 93, Average Loss: 13.599009990692139, Average MAE: 0.8777786791324615, Learning Rate: 0.005, Current Patience: 4\n",
      "Epoch 94, Average Loss: 13.599876880645752, Average MAE: 0.8774746954441071, Learning Rate: 0.005, Current Patience: 5\n",
      "Epoch 95, Average Loss: 13.603739261627197, Average MAE: 0.8772335648536682, Learning Rate: 0.005, Current Patience: 6\n",
      "Epoch 96, Average Loss: 13.598613739013672, Average MAE: 0.8769104182720184, Learning Rate: 0.005, Current Patience: 7\n",
      "Epoch 97, Average Loss: 13.597709655761719, Average MAE: 0.87638920545578, Learning Rate: 0.005, Current Patience: 8\n",
      "Epoch 98, Average Loss: 13.600168704986572, Average MAE: 0.8761742413043976, Learning Rate: 0.005, Current Patience: 9\n",
      "Epoch 99, Average Loss: 13.597527027130127, Average MAE: 0.8759072124958038, Learning Rate: 0.005, Current Patience: 10\n",
      "Epoch 100, Average Loss: 13.599061965942383, Average MAE: 0.875629186630249, Learning Rate: 0.0025, Current Patience: 0\n",
      "Epoch 101, Average Loss: 13.592893600463867, Average MAE: 0.8751397132873535, Learning Rate: 0.0025, Current Patience: 0\n",
      "Epoch 102, Average Loss: 13.598005294799805, Average MAE: 0.8750636279582977, Learning Rate: 0.0025, Current Patience: 1\n",
      "Epoch 103, Average Loss: 13.599080562591553, Average MAE: 0.8751260042190552, Learning Rate: 0.0025, Current Patience: 2\n",
      "Epoch 104, Average Loss: 13.59817886352539, Average MAE: 0.8749130070209503, Learning Rate: 0.0025, Current Patience: 3\n",
      "Epoch 105, Average Loss: 13.596325874328613, Average MAE: 0.8744833469390869, Learning Rate: 0.0025, Current Patience: 4\n",
      "Epoch 106, Average Loss: 13.598920345306396, Average MAE: 0.8743772208690643, Learning Rate: 0.0025, Current Patience: 5\n",
      "Epoch 107, Average Loss: 13.603487014770508, Average MAE: 0.8747494220733643, Learning Rate: 0.0025, Current Patience: 6\n",
      "Epoch 108, Average Loss: 13.601779460906982, Average MAE: 0.8743678033351898, Learning Rate: 0.0025, Current Patience: 7\n",
      "Epoch 109, Average Loss: 13.597305297851562, Average MAE: 0.8741670548915863, Learning Rate: 0.0025, Current Patience: 8\n",
      "Epoch 110, Average Loss: 13.591803073883057, Average MAE: 0.8735297620296478, Learning Rate: 0.0025, Current Patience: 9\n",
      "Epoch 111, Average Loss: 13.594764232635498, Average MAE: 0.8737210631370544, Learning Rate: 0.0025, Current Patience: 10\n",
      "Epoch 112, Average Loss: 13.597194194793701, Average MAE: 0.8737616837024689, Learning Rate: 0.00125, Current Patience: 0\n",
      "Epoch 113, Average Loss: 13.599152565002441, Average MAE: 0.8734524250030518, Learning Rate: 0.00125, Current Patience: 1\n",
      "Epoch 114, Average Loss: 13.599863052368164, Average MAE: 0.8734802305698395, Learning Rate: 0.00125, Current Patience: 2\n",
      "Epoch 115, Average Loss: 13.599860668182373, Average MAE: 0.8732974529266357, Learning Rate: 0.00125, Current Patience: 3\n",
      "Epoch 116, Average Loss: 13.600048065185547, Average MAE: 0.8732898533344269, Learning Rate: 0.00125, Current Patience: 4\n",
      "Epoch 117, Average Loss: 13.596202850341797, Average MAE: 0.8732734620571136, Learning Rate: 0.00125, Current Patience: 5\n",
      "Epoch 118, Average Loss: 13.594858646392822, Average MAE: 0.8730548620223999, Learning Rate: 0.00125, Current Patience: 6\n",
      "Epoch 119, Average Loss: 13.59556770324707, Average MAE: 0.87327840924263, Learning Rate: 0.00125, Current Patience: 7\n",
      "Epoch 120, Average Loss: 13.595715999603271, Average MAE: 0.87293541431427, Learning Rate: 0.00125, Current Patience: 8\n",
      "Epoch 121, Average Loss: 13.599432468414307, Average MAE: 0.8729919493198395, Learning Rate: 0.00125, Current Patience: 9\n",
      "Epoch 122, Average Loss: 13.589794158935547, Average MAE: 0.8726727366447449, Learning Rate: 0.00125, Current Patience: 0\n",
      "Epoch 123, Average Loss: 13.595439910888672, Average MAE: 0.8726962506771088, Learning Rate: 0.00125, Current Patience: 1\n",
      "Epoch 124, Average Loss: 13.599446773529053, Average MAE: 0.872800886631012, Learning Rate: 0.00125, Current Patience: 2\n",
      "Epoch 125, Average Loss: 13.599254131317139, Average MAE: 0.8727474808692932, Learning Rate: 0.00125, Current Patience: 3\n",
      "Epoch 126, Average Loss: 13.597934246063232, Average MAE: 0.8727681040763855, Learning Rate: 0.00125, Current Patience: 4\n",
      "Epoch 127, Average Loss: 13.598388195037842, Average MAE: 0.8725678324699402, Learning Rate: 0.00125, Current Patience: 5\n",
      "Epoch 128, Average Loss: 13.602134704589844, Average MAE: 0.8724625706672668, Learning Rate: 0.00125, Current Patience: 6\n",
      "Epoch 129, Average Loss: 13.599956512451172, Average MAE: 0.8727171421051025, Learning Rate: 0.00125, Current Patience: 7\n",
      "Epoch 130, Average Loss: 13.604981899261475, Average MAE: 0.8725058734416962, Learning Rate: 0.00125, Current Patience: 8\n",
      "Epoch 131, Average Loss: 13.6049165725708, Average MAE: 0.872405081987381, Learning Rate: 0.00125, Current Patience: 9\n",
      "Epoch 132, Average Loss: 13.594873428344727, Average MAE: 0.8719823658466339, Learning Rate: 0.00125, Current Patience: 10\n",
      "Epoch 133, Average Loss: 13.596349239349365, Average MAE: 0.8721169829368591, Learning Rate: 0.000625, Current Patience: 0\n",
      "Epoch 134, Average Loss: 13.60165786743164, Average MAE: 0.8722577095031738, Learning Rate: 0.000625, Current Patience: 1\n",
      "Epoch 135, Average Loss: 13.601710319519043, Average MAE: 0.8719388544559479, Learning Rate: 0.000625, Current Patience: 2\n",
      "Epoch 136, Average Loss: 13.60084056854248, Average MAE: 0.8720897436141968, Learning Rate: 0.000625, Current Patience: 3\n",
      "Epoch 137, Average Loss: 13.598828315734863, Average MAE: 0.8718730211257935, Learning Rate: 0.000625, Current Patience: 4\n",
      "Epoch 138, Average Loss: 13.60318660736084, Average MAE: 0.8720289170742035, Learning Rate: 0.000625, Current Patience: 5\n",
      "Epoch 139, Average Loss: 13.597841739654541, Average MAE: 0.8716820180416107, Learning Rate: 0.000625, Current Patience: 6\n",
      "Epoch 140, Average Loss: 13.597443580627441, Average MAE: 0.8717215061187744, Learning Rate: 0.000625, Current Patience: 7\n",
      "Epoch 141, Average Loss: 13.596944332122803, Average MAE: 0.8717918694019318, Learning Rate: 0.000625, Current Patience: 8\n",
      "Epoch 142, Average Loss: 13.598891735076904, Average MAE: 0.8716508448123932, Learning Rate: 0.000625, Current Patience: 9\n",
      "Epoch 143, Average Loss: 13.591852188110352, Average MAE: 0.8713511526584625, Learning Rate: 0.000625, Current Patience: 10\n",
      "Epoch 144, Average Loss: 13.598019123077393, Average MAE: 0.871653139591217, Learning Rate: 0.0003125, Current Patience: 0\n",
      "Epoch 145, Average Loss: 13.59148645401001, Average MAE: 0.871494472026825, Learning Rate: 0.0003125, Current Patience: 1\n",
      "Epoch 146, Average Loss: 13.601271152496338, Average MAE: 0.8716083765029907, Learning Rate: 0.0003125, Current Patience: 2\n",
      "Epoch 147, Average Loss: 13.59712266921997, Average MAE: 0.871391773223877, Learning Rate: 0.0003125, Current Patience: 3\n",
      "Epoch 148, Average Loss: 13.600519180297852, Average MAE: 0.8715372085571289, Learning Rate: 0.0003125, Current Patience: 4\n",
      "Epoch 149, Average Loss: 13.599451065063477, Average MAE: 0.8715435862541199, Learning Rate: 0.0003125, Current Patience: 5\n",
      "Epoch 150, Average Loss: 13.590945720672607, Average MAE: 0.8713787496089935, Learning Rate: 0.0003125, Current Patience: 6\n",
      "Epoch 151, Average Loss: 13.595855712890625, Average MAE: 0.8715522885322571, Learning Rate: 0.0003125, Current Patience: 7\n",
      "Epoch 152, Average Loss: 13.600012302398682, Average MAE: 0.8715401589870453, Learning Rate: 0.0003125, Current Patience: 8\n",
      "Epoch 153, Average Loss: 13.59660816192627, Average MAE: 0.871388703584671, Learning Rate: 0.0003125, Current Patience: 9\n",
      "Epoch 154, Average Loss: 13.596231460571289, Average MAE: 0.8716004192829132, Learning Rate: 0.0003125, Current Patience: 10\n",
      "Epoch 155, Average Loss: 13.596608638763428, Average MAE: 0.8712463974952698, Learning Rate: 0.00015625, Current Patience: 0\n",
      "Epoch 156, Average Loss: 13.597066879272461, Average MAE: 0.871436208486557, Learning Rate: 0.00015625, Current Patience: 1\n",
      "Epoch 157, Average Loss: 13.596732139587402, Average MAE: 0.8715029954910278, Learning Rate: 0.00015625, Current Patience: 2\n",
      "Epoch 158, Average Loss: 13.590751647949219, Average MAE: 0.8714360892772675, Learning Rate: 0.00015625, Current Patience: 3\n",
      "Epoch 159, Average Loss: 13.599402904510498, Average MAE: 0.8714348077774048, Learning Rate: 0.00015625, Current Patience: 4\n",
      "Epoch 160, Average Loss: 13.598339080810547, Average MAE: 0.8714971840381622, Learning Rate: 0.00015625, Current Patience: 5\n",
      "Epoch 161, Average Loss: 13.599814891815186, Average MAE: 0.8712378442287445, Learning Rate: 0.00015625, Current Patience: 6\n",
      "Epoch 162, Average Loss: 13.602977752685547, Average MAE: 0.8713477551937103, Learning Rate: 0.00015625, Current Patience: 7\n",
      "Epoch 163, Average Loss: 13.598090171813965, Average MAE: 0.8714610934257507, Learning Rate: 0.00015625, Current Patience: 8\n",
      "Epoch 164, Average Loss: 13.598419189453125, Average MAE: 0.8711844086647034, Learning Rate: 0.00015625, Current Patience: 9\n",
      "Epoch 165, Average Loss: 13.600241661071777, Average MAE: 0.8714417517185211, Learning Rate: 0.00015625, Current Patience: 10\n",
      "Epoch 166, Average Loss: 13.59726095199585, Average MAE: 0.8712843358516693, Learning Rate: 7.8125e-05, Current Patience: 0\n",
      "Epoch 167, Average Loss: 13.596724510192871, Average MAE: 0.8711855709552765, Learning Rate: 7.8125e-05, Current Patience: 1\n",
      "Epoch 168, Average Loss: 13.598081588745117, Average MAE: 0.8713789880275726, Learning Rate: 7.8125e-05, Current Patience: 2\n",
      "Epoch 169, Average Loss: 13.59463357925415, Average MAE: 0.8712334930896759, Learning Rate: 7.8125e-05, Current Patience: 3\n",
      "Epoch 170, Average Loss: 13.58938980102539, Average MAE: 0.8709596693515778, Learning Rate: 7.8125e-05, Current Patience: 4\n",
      "Epoch 171, Average Loss: 13.591592788696289, Average MAE: 0.8710389137268066, Learning Rate: 7.8125e-05, Current Patience: 5\n",
      "Epoch 172, Average Loss: 13.595955848693848, Average MAE: 0.8712175488471985, Learning Rate: 7.8125e-05, Current Patience: 6\n",
      "Epoch 173, Average Loss: 13.593715190887451, Average MAE: 0.8712033629417419, Learning Rate: 7.8125e-05, Current Patience: 7\n",
      "Epoch 174, Average Loss: 13.59355354309082, Average MAE: 0.8712634444236755, Learning Rate: 7.8125e-05, Current Patience: 8\n",
      "Epoch 175, Average Loss: 13.59928560256958, Average MAE: 0.8712648451328278, Learning Rate: 7.8125e-05, Current Patience: 9\n",
      "Epoch 176, Average Loss: 13.598445892333984, Average MAE: 0.871152251958847, Learning Rate: 7.8125e-05, Current Patience: 10\n",
      "Epoch 177, Average Loss: 13.594661235809326, Average MAE: 0.871179848909378, Learning Rate: 3.90625e-05, Current Patience: 0\n",
      "Epoch 178, Average Loss: 13.59684133529663, Average MAE: 0.8712478578090668, Learning Rate: 3.90625e-05, Current Patience: 1\n",
      "Epoch 179, Average Loss: 13.593847751617432, Average MAE: 0.8713575601577759, Learning Rate: 3.90625e-05, Current Patience: 2\n",
      "Epoch 180, Average Loss: 13.604987621307373, Average MAE: 0.871352344751358, Learning Rate: 3.90625e-05, Current Patience: 3\n",
      "Epoch 181, Average Loss: 13.598274230957031, Average MAE: 0.8713928759098053, Learning Rate: 3.90625e-05, Current Patience: 4\n",
      "Epoch 182, Average Loss: 13.59704065322876, Average MAE: 0.8712504208087921, Learning Rate: 3.90625e-05, Current Patience: 5\n",
      "Epoch 183, Average Loss: 13.599862098693848, Average MAE: 0.8712637722492218, Learning Rate: 3.90625e-05, Current Patience: 6\n",
      "Epoch 184, Average Loss: 13.597059726715088, Average MAE: 0.8712899684906006, Learning Rate: 3.90625e-05, Current Patience: 7\n",
      "Epoch 185, Average Loss: 13.596739768981934, Average MAE: 0.871211588382721, Learning Rate: 3.90625e-05, Current Patience: 8\n",
      "Epoch 186, Average Loss: 13.59601354598999, Average MAE: 0.871058464050293, Learning Rate: 3.90625e-05, Current Patience: 9\n",
      "Epoch 187, Average Loss: 13.600621223449707, Average MAE: 0.871267557144165, Learning Rate: 3.90625e-05, Current Patience: 10\n",
      "Epoch 188, Average Loss: 13.599864959716797, Average MAE: 0.8713476955890656, Learning Rate: 1.953125e-05, Current Patience: 0\n",
      "Epoch 189, Average Loss: 13.600995063781738, Average MAE: 0.8713590204715729, Learning Rate: 1.953125e-05, Current Patience: 1\n",
      "Epoch 190, Average Loss: 13.598482131958008, Average MAE: 0.8712287545204163, Learning Rate: 1.953125e-05, Current Patience: 2\n",
      "Epoch 191, Average Loss: 13.593854904174805, Average MAE: 0.8710546493530273, Learning Rate: 1.953125e-05, Current Patience: 3\n",
      "Epoch 192, Average Loss: 13.602349758148193, Average MAE: 0.8712868988513947, Learning Rate: 1.953125e-05, Current Patience: 4\n",
      "Epoch 193, Average Loss: 13.600692749023438, Average MAE: 0.8712480366230011, Learning Rate: 1.953125e-05, Current Patience: 5\n",
      "Epoch 194, Average Loss: 13.600136280059814, Average MAE: 0.8714627623558044, Learning Rate: 1.953125e-05, Current Patience: 6\n",
      "Epoch 195, Average Loss: 13.596781730651855, Average MAE: 0.8710216283798218, Learning Rate: 1.953125e-05, Current Patience: 7\n",
      "Epoch 196, Average Loss: 13.595977306365967, Average MAE: 0.8712487816810608, Learning Rate: 1.953125e-05, Current Patience: 8\n",
      "Epoch 197, Average Loss: 13.59159231185913, Average MAE: 0.870840847492218, Learning Rate: 1.953125e-05, Current Patience: 9\n",
      "Epoch 198, Average Loss: 13.597149848937988, Average MAE: 0.8712389767169952, Learning Rate: 1.953125e-05, Current Patience: 10\n",
      "Epoch 199, Average Loss: 13.595386028289795, Average MAE: 0.8711425065994263, Learning Rate: 1e-05, Current Patience: 0\n",
      "Epoch 200, Average Loss: 13.59525203704834, Average MAE: 0.8711820542812347, Learning Rate: 1e-05, Current Patience: 1\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters\n",
    "in_channels = batched_input_train.shape[2]\n",
    "features_channels = batched_input_train.shape[3]\n",
    "out_channels = in_channels  \n",
    "edge_in_channels = 1\n",
    "hidden_channels = 256\n",
    "transformer_layers = 3\n",
    "\n",
    "# Print the parameters to verify\n",
    "print(\"Input Channels:\", in_channels)\n",
    "print(\"Features Channels:\", features_channels)\n",
    "print(\"Output Channels:\", out_channels)\n",
    "print(\"Edge Input Channels:\", edge_in_channels)\n",
    "print(\"Hidden Channels:\", hidden_channels)\n",
    "print(\"Transformer Layers:\", transformer_layers)\n",
    "\n",
    "model_gnn_transformer = HybridModel_GNN(in_channels, out_channels, features_channels, edge_in_channels, hidden_channels, transformer_layers).to(device)\n",
    "\n",
    "# Define the optimizer and loss function for model_gnn_transformer\n",
    "optimizer_gnn_transformer = torch.optim.Adam(model_gnn_transformer.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "scheduler_gnn_transformer = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_gnn_transformer, mode=scheduler_mode, factor=scheduler_factor, patience=scheduler_patience, verbose=True, min_lr=1e-5)\n",
    "\n",
    "gnn_transformer_epoch_losses, gnn_transformer_epoch_maes = train_model(model_gnn_transformer, optimizer_gnn_transformer, scheduler_gnn_transformer, loss_fn, num_epochs, num_batches_train, batched_input_train, batched_output_train, device)\n",
    "torch.save(model_gnn_transformer.state_dict(), 'model_gnn_transformer_hidden{hidden_channels}_layers{transformer_layers}.pth')\n",
    "del model_gnn_transformer\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Channels: 27\n",
      "Features Channels: 70\n",
      "Output Channels: 27\n",
      "Edge Input Channels: 1\n",
      "Hidden Channels: 512\n",
      "Transformer Layers: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/HDD/RyanFolder/projects/Weather-Predition-Project/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 210.01707458496094, Average MAE: 4.652186393737793, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 2, Average Loss: 126.38285064697266, Average MAE: 4.692593216896057, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 3, Average Loss: 42.02210807800293, Average MAE: 3.4078190326690674, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 4, Average Loss: 49.750125885009766, Average MAE: 3.6813308000564575, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 5, Average Loss: 26.21917724609375, Average MAE: 2.711572527885437, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 6, Average Loss: 25.732948303222656, Average MAE: 2.342315673828125, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 7, Average Loss: 31.62534999847412, Average MAE: 2.8621429204940796, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 8, Average Loss: 20.73493480682373, Average MAE: 1.9904300570487976, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 9, Average Loss: 18.2442626953125, Average MAE: 1.8694208860397339, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 10, Average Loss: 18.369132041931152, Average MAE: 1.7203845381736755, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 11, Average Loss: 16.797572135925293, Average MAE: 1.723396897315979, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 12, Average Loss: 17.079442977905273, Average MAE: 1.5533044934272766, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 13, Average Loss: 17.05391502380371, Average MAE: 1.4111069440841675, Learning Rate: 0.01, Current Patience: 2\n",
      "Epoch 14, Average Loss: 15.333655834197998, Average MAE: 1.4003469347953796, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 15, Average Loss: 14.752794742584229, Average MAE: 1.2875943183898926, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 16, Average Loss: 15.240684509277344, Average MAE: 1.2675216794013977, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 17, Average Loss: 14.842236042022705, Average MAE: 1.1850755214691162, Learning Rate: 0.01, Current Patience: 2\n",
      "Epoch 18, Average Loss: 14.49064588546753, Average MAE: 1.1888746619224548, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 19, Average Loss: 14.582069873809814, Average MAE: 1.1573967337608337, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 20, Average Loss: 14.2392258644104, Average MAE: 1.101482331752777, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 21, Average Loss: 14.118988513946533, Average MAE: 1.0986913442611694, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 22, Average Loss: 14.301676273345947, Average MAE: 1.1245483756065369, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 23, Average Loss: 14.069011688232422, Average MAE: 1.0701214671134949, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 24, Average Loss: 13.873263835906982, Average MAE: 1.003926306962967, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 25, Average Loss: 13.997082233428955, Average MAE: 1.0263448357582092, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 26, Average Loss: 13.966709613800049, Average MAE: 1.0033197402954102, Learning Rate: 0.01, Current Patience: 2\n",
      "Epoch 27, Average Loss: 13.860111236572266, Average MAE: 0.980415403842926, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 28, Average Loss: 13.87912654876709, Average MAE: 0.9611080884933472, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 29, Average Loss: 13.838569164276123, Average MAE: 0.9529707431793213, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 30, Average Loss: 13.786693096160889, Average MAE: 0.9400421679019928, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 31, Average Loss: 13.838876247406006, Average MAE: 0.9370331168174744, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 32, Average Loss: 13.789449691772461, Average MAE: 0.9328859150409698, Learning Rate: 0.01, Current Patience: 2\n",
      "Epoch 33, Average Loss: 13.79488754272461, Average MAE: 0.9238794147968292, Learning Rate: 0.01, Current Patience: 3\n",
      "Epoch 34, Average Loss: 13.781116485595703, Average MAE: 0.92343470454216, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 35, Average Loss: 13.753824234008789, Average MAE: 0.9134151637554169, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 36, Average Loss: 13.772141456604004, Average MAE: 0.9212279915809631, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 37, Average Loss: 13.75984001159668, Average MAE: 0.9102666676044464, Learning Rate: 0.01, Current Patience: 2\n",
      "Epoch 38, Average Loss: 13.760000705718994, Average MAE: 0.9041638970375061, Learning Rate: 0.01, Current Patience: 3\n",
      "Epoch 39, Average Loss: 13.765762329101562, Average MAE: 0.9014894366264343, Learning Rate: 0.01, Current Patience: 4\n",
      "Epoch 40, Average Loss: 13.760165691375732, Average MAE: 0.9025485813617706, Learning Rate: 0.01, Current Patience: 5\n",
      "Epoch 41, Average Loss: 13.751144409179688, Average MAE: 0.8978919982910156, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 42, Average Loss: 13.749831199645996, Average MAE: 0.8981057107448578, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 43, Average Loss: 13.743546485900879, Average MAE: 0.8961318135261536, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 44, Average Loss: 13.739833354949951, Average MAE: 0.891591489315033, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 45, Average Loss: 13.738454341888428, Average MAE: 0.8921951949596405, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 46, Average Loss: 13.729011058807373, Average MAE: 0.8907724618911743, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 47, Average Loss: 13.73757553100586, Average MAE: 0.8902922570705414, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 48, Average Loss: 13.730175971984863, Average MAE: 0.8889262676239014, Learning Rate: 0.01, Current Patience: 2\n",
      "Epoch 49, Average Loss: 13.746602535247803, Average MAE: 0.8884603381156921, Learning Rate: 0.01, Current Patience: 3\n",
      "Epoch 50, Average Loss: 13.729892253875732, Average MAE: 0.8876981437206268, Learning Rate: 0.01, Current Patience: 4\n",
      "Epoch 51, Average Loss: 13.731437683105469, Average MAE: 0.885614663362503, Learning Rate: 0.01, Current Patience: 5\n",
      "Epoch 52, Average Loss: 13.735527038574219, Average MAE: 0.8851504325866699, Learning Rate: 0.01, Current Patience: 6\n",
      "Epoch 53, Average Loss: 13.717555522918701, Average MAE: 0.8856942653656006, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 54, Average Loss: 13.73023509979248, Average MAE: 0.8843080997467041, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 55, Average Loss: 13.71890640258789, Average MAE: 0.8844171166419983, Learning Rate: 0.01, Current Patience: 2\n",
      "Epoch 56, Average Loss: 13.719565391540527, Average MAE: 0.8836327791213989, Learning Rate: 0.01, Current Patience: 3\n",
      "Epoch 57, Average Loss: 13.732369422912598, Average MAE: 0.8835387825965881, Learning Rate: 0.01, Current Patience: 4\n",
      "Epoch 58, Average Loss: 13.72988748550415, Average MAE: 0.8830648362636566, Learning Rate: 0.01, Current Patience: 5\n",
      "Epoch 59, Average Loss: 13.724653244018555, Average MAE: 0.8825005888938904, Learning Rate: 0.01, Current Patience: 6\n",
      "Epoch 60, Average Loss: 13.71356201171875, Average MAE: 0.8825689852237701, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 61, Average Loss: 13.701604843139648, Average MAE: 0.8820059597492218, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 62, Average Loss: 13.714041709899902, Average MAE: 0.8824540376663208, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 63, Average Loss: 13.719976902008057, Average MAE: 0.8825370073318481, Learning Rate: 0.01, Current Patience: 2\n",
      "Epoch 64, Average Loss: 13.700644493103027, Average MAE: 0.8824843764305115, Learning Rate: 0.01, Current Patience: 3\n",
      "Epoch 65, Average Loss: 13.711435317993164, Average MAE: 0.8826964795589447, Learning Rate: 0.01, Current Patience: 4\n",
      "Epoch 66, Average Loss: 13.707733154296875, Average MAE: 0.8832218945026398, Learning Rate: 0.01, Current Patience: 5\n",
      "Epoch 67, Average Loss: 13.711163520812988, Average MAE: 0.883419930934906, Learning Rate: 0.01, Current Patience: 6\n",
      "Epoch 68, Average Loss: 13.702162742614746, Average MAE: 0.8829981088638306, Learning Rate: 0.01, Current Patience: 7\n",
      "Epoch 69, Average Loss: 13.717097759246826, Average MAE: 0.8837698996067047, Learning Rate: 0.01, Current Patience: 8\n",
      "Epoch 70, Average Loss: 13.708165645599365, Average MAE: 0.8837300539016724, Learning Rate: 0.01, Current Patience: 9\n",
      "Epoch 71, Average Loss: 13.708175659179688, Average MAE: 0.8836862146854401, Learning Rate: 0.01, Current Patience: 10\n",
      "Epoch 72, Average Loss: 13.698583602905273, Average MAE: 0.8832334578037262, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 73, Average Loss: 13.698028564453125, Average MAE: 0.8831780850887299, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 74, Average Loss: 13.70027494430542, Average MAE: 0.8826196491718292, Learning Rate: 0.01, Current Patience: 2\n",
      "Epoch 75, Average Loss: 13.68781042098999, Average MAE: 0.8819263279438019, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 76, Average Loss: 13.708659648895264, Average MAE: 0.8826352953910828, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 77, Average Loss: 13.687161445617676, Average MAE: 0.8815372586250305, Learning Rate: 0.01, Current Patience: 2\n",
      "Epoch 78, Average Loss: 13.69023084640503, Average MAE: 0.8814258873462677, Learning Rate: 0.01, Current Patience: 3\n",
      "Epoch 79, Average Loss: 13.689786911010742, Average MAE: 0.8816344738006592, Learning Rate: 0.01, Current Patience: 4\n",
      "Epoch 80, Average Loss: 13.686479091644287, Average MAE: 0.8807297646999359, Learning Rate: 0.01, Current Patience: 5\n",
      "Epoch 81, Average Loss: 13.688766956329346, Average MAE: 0.8811082243919373, Learning Rate: 0.01, Current Patience: 6\n",
      "Epoch 82, Average Loss: 13.683099269866943, Average MAE: 0.8804969787597656, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 83, Average Loss: 13.693740367889404, Average MAE: 0.8807230889797211, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 84, Average Loss: 13.683623313903809, Average MAE: 0.8798963129520416, Learning Rate: 0.01, Current Patience: 2\n",
      "Epoch 85, Average Loss: 13.695934772491455, Average MAE: 0.8805141448974609, Learning Rate: 0.01, Current Patience: 3\n",
      "Epoch 86, Average Loss: 13.6974196434021, Average MAE: 0.8804567754268646, Learning Rate: 0.01, Current Patience: 4\n",
      "Epoch 87, Average Loss: 13.691193580627441, Average MAE: 0.8806353807449341, Learning Rate: 0.01, Current Patience: 5\n",
      "Epoch 88, Average Loss: 13.680964469909668, Average MAE: 0.8801637291908264, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 89, Average Loss: 13.688525676727295, Average MAE: 0.880926102399826, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 90, Average Loss: 13.681207656860352, Average MAE: 0.8804460167884827, Learning Rate: 0.01, Current Patience: 2\n",
      "Epoch 91, Average Loss: 13.691463470458984, Average MAE: 0.8807137310504913, Learning Rate: 0.01, Current Patience: 3\n",
      "Epoch 92, Average Loss: 13.674581050872803, Average MAE: 0.8798808753490448, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 93, Average Loss: 13.696238040924072, Average MAE: 0.8799992501735687, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 94, Average Loss: 13.675156593322754, Average MAE: 0.8798402845859528, Learning Rate: 0.01, Current Patience: 2\n",
      "Epoch 95, Average Loss: 13.669227600097656, Average MAE: 0.879107266664505, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 96, Average Loss: 13.668394088745117, Average MAE: 0.879444032907486, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 97, Average Loss: 13.675898551940918, Average MAE: 0.8790903687477112, Learning Rate: 0.01, Current Patience: 2\n",
      "Epoch 98, Average Loss: 13.675749778747559, Average MAE: 0.8790690898895264, Learning Rate: 0.01, Current Patience: 3\n",
      "Epoch 99, Average Loss: 13.673099994659424, Average MAE: 0.8784278035163879, Learning Rate: 0.01, Current Patience: 4\n",
      "Epoch 100, Average Loss: 13.684751987457275, Average MAE: 0.8784447908401489, Learning Rate: 0.01, Current Patience: 5\n",
      "Epoch 101, Average Loss: 13.679166316986084, Average MAE: 0.8782554566860199, Learning Rate: 0.01, Current Patience: 6\n",
      "Epoch 102, Average Loss: 13.678658962249756, Average MAE: 0.8781013488769531, Learning Rate: 0.01, Current Patience: 7\n",
      "Epoch 103, Average Loss: 13.679254531860352, Average MAE: 0.8775653839111328, Learning Rate: 0.01, Current Patience: 8\n",
      "Epoch 104, Average Loss: 13.677148342132568, Average MAE: 0.8774228096008301, Learning Rate: 0.01, Current Patience: 9\n",
      "Epoch 105, Average Loss: 13.663409233093262, Average MAE: 0.8773116171360016, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 106, Average Loss: 13.666276454925537, Average MAE: 0.8771760165691376, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 107, Average Loss: 13.66827392578125, Average MAE: 0.8772302865982056, Learning Rate: 0.01, Current Patience: 2\n",
      "Epoch 108, Average Loss: 13.669784545898438, Average MAE: 0.8769845068454742, Learning Rate: 0.01, Current Patience: 3\n",
      "Epoch 109, Average Loss: 13.659855842590332, Average MAE: 0.8762374818325043, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 110, Average Loss: 13.675548553466797, Average MAE: 0.8763951659202576, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 111, Average Loss: 13.666481971740723, Average MAE: 0.8761819005012512, Learning Rate: 0.01, Current Patience: 2\n",
      "Epoch 112, Average Loss: 13.668796062469482, Average MAE: 0.8760745823383331, Learning Rate: 0.01, Current Patience: 3\n",
      "Epoch 113, Average Loss: 13.664859294891357, Average MAE: 0.8752250373363495, Learning Rate: 0.01, Current Patience: 4\n",
      "Epoch 114, Average Loss: 13.648824214935303, Average MAE: 0.8748652637004852, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 115, Average Loss: 13.659862518310547, Average MAE: 0.8743210434913635, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 116, Average Loss: 13.661803245544434, Average MAE: 0.8747450709342957, Learning Rate: 0.01, Current Patience: 2\n",
      "Epoch 117, Average Loss: 13.657936573028564, Average MAE: 0.8743074536323547, Learning Rate: 0.01, Current Patience: 3\n",
      "Epoch 118, Average Loss: 13.657274723052979, Average MAE: 0.874004989862442, Learning Rate: 0.01, Current Patience: 4\n",
      "Epoch 119, Average Loss: 13.664089679718018, Average MAE: 0.8744407296180725, Learning Rate: 0.01, Current Patience: 5\n",
      "Epoch 120, Average Loss: 13.646602153778076, Average MAE: 0.8732052743434906, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 121, Average Loss: 13.658167362213135, Average MAE: 0.8732643723487854, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 122, Average Loss: 13.661182403564453, Average MAE: 0.8728218376636505, Learning Rate: 0.01, Current Patience: 2\n",
      "Epoch 123, Average Loss: 13.654484272003174, Average MAE: 0.8724009990692139, Learning Rate: 0.01, Current Patience: 3\n",
      "Epoch 124, Average Loss: 13.656271934509277, Average MAE: 0.8719522655010223, Learning Rate: 0.01, Current Patience: 4\n",
      "Epoch 125, Average Loss: 13.64328145980835, Average MAE: 0.870945930480957, Learning Rate: 0.01, Current Patience: 0\n",
      "Epoch 126, Average Loss: 13.661479949951172, Average MAE: 0.8714644312858582, Learning Rate: 0.01, Current Patience: 1\n",
      "Epoch 127, Average Loss: 13.654721736907959, Average MAE: 0.8703562021255493, Learning Rate: 0.01, Current Patience: 2\n",
      "Epoch 128, Average Loss: 13.655089378356934, Average MAE: 0.8700186014175415, Learning Rate: 0.01, Current Patience: 3\n",
      "Epoch 129, Average Loss: 13.651780128479004, Average MAE: 0.8696668446063995, Learning Rate: 0.01, Current Patience: 4\n",
      "Epoch 130, Average Loss: 13.655635833740234, Average MAE: 0.8694197237491608, Learning Rate: 0.01, Current Patience: 5\n",
      "Epoch 131, Average Loss: 13.657262802124023, Average MAE: 0.869055986404419, Learning Rate: 0.01, Current Patience: 6\n",
      "Epoch 132, Average Loss: 13.650852680206299, Average MAE: 0.8690958023071289, Learning Rate: 0.01, Current Patience: 7\n",
      "Epoch 133, Average Loss: 13.661334991455078, Average MAE: 0.8689090311527252, Learning Rate: 0.01, Current Patience: 8\n",
      "Epoch 134, Average Loss: 13.648587703704834, Average MAE: 0.8678677380084991, Learning Rate: 0.01, Current Patience: 9\n",
      "Epoch 135, Average Loss: 13.648635387420654, Average MAE: 0.8674086332321167, Learning Rate: 0.01, Current Patience: 10\n",
      "Epoch 136, Average Loss: 13.65005111694336, Average MAE: 0.866779625415802, Learning Rate: 0.005, Current Patience: 0\n",
      "Epoch 137, Average Loss: 13.648012638092041, Average MAE: 0.8668524026870728, Learning Rate: 0.005, Current Patience: 1\n",
      "Epoch 138, Average Loss: 13.651641845703125, Average MAE: 0.8662139177322388, Learning Rate: 0.005, Current Patience: 2\n",
      "Epoch 139, Average Loss: 13.645648002624512, Average MAE: 0.8658528625965118, Learning Rate: 0.005, Current Patience: 3\n",
      "Epoch 140, Average Loss: 13.644560813903809, Average MAE: 0.865838885307312, Learning Rate: 0.005, Current Patience: 4\n",
      "Epoch 141, Average Loss: 13.650609493255615, Average MAE: 0.8658887147903442, Learning Rate: 0.005, Current Patience: 5\n",
      "Epoch 142, Average Loss: 13.650096416473389, Average MAE: 0.8653822839260101, Learning Rate: 0.005, Current Patience: 6\n",
      "Epoch 143, Average Loss: 13.64769172668457, Average MAE: 0.8652602732181549, Learning Rate: 0.005, Current Patience: 7\n",
      "Epoch 144, Average Loss: 13.650639057159424, Average MAE: 0.865268737077713, Learning Rate: 0.005, Current Patience: 8\n",
      "Epoch 145, Average Loss: 13.6459641456604, Average MAE: 0.8648812770843506, Learning Rate: 0.005, Current Patience: 9\n",
      "Epoch 146, Average Loss: 13.643902778625488, Average MAE: 0.8646726608276367, Learning Rate: 0.005, Current Patience: 10\n",
      "Epoch 147, Average Loss: 13.647885799407959, Average MAE: 0.8646067380905151, Learning Rate: 0.0025, Current Patience: 0\n",
      "Epoch 148, Average Loss: 13.651089191436768, Average MAE: 0.864640474319458, Learning Rate: 0.0025, Current Patience: 1\n",
      "Epoch 149, Average Loss: 13.65208387374878, Average MAE: 0.8646703064441681, Learning Rate: 0.0025, Current Patience: 2\n",
      "Epoch 150, Average Loss: 13.64174747467041, Average MAE: 0.864070475101471, Learning Rate: 0.0025, Current Patience: 0\n",
      "Epoch 151, Average Loss: 13.648088455200195, Average MAE: 0.8642933964729309, Learning Rate: 0.0025, Current Patience: 1\n",
      "Epoch 152, Average Loss: 13.635985374450684, Average MAE: 0.8640466630458832, Learning Rate: 0.0025, Current Patience: 0\n",
      "Epoch 153, Average Loss: 13.632784366607666, Average MAE: 0.8641928732395172, Learning Rate: 0.0025, Current Patience: 0\n",
      "Epoch 154, Average Loss: 13.648526668548584, Average MAE: 0.864119827747345, Learning Rate: 0.0025, Current Patience: 1\n",
      "Epoch 155, Average Loss: 13.650415420532227, Average MAE: 0.8644247353076935, Learning Rate: 0.0025, Current Patience: 2\n",
      "Epoch 156, Average Loss: 13.646005153656006, Average MAE: 0.8641690611839294, Learning Rate: 0.0025, Current Patience: 3\n",
      "Epoch 157, Average Loss: 13.641839981079102, Average MAE: 0.8640526831150055, Learning Rate: 0.0025, Current Patience: 4\n",
      "Epoch 158, Average Loss: 13.632994651794434, Average MAE: 0.863851010799408, Learning Rate: 0.0025, Current Patience: 5\n",
      "Epoch 159, Average Loss: 13.646754264831543, Average MAE: 0.8640189468860626, Learning Rate: 0.0025, Current Patience: 6\n",
      "Epoch 160, Average Loss: 13.63673734664917, Average MAE: 0.8641148209571838, Learning Rate: 0.0025, Current Patience: 7\n",
      "Epoch 161, Average Loss: 13.635316371917725, Average MAE: 0.8635083436965942, Learning Rate: 0.0025, Current Patience: 8\n",
      "Epoch 162, Average Loss: 13.631304740905762, Average MAE: 0.8637937903404236, Learning Rate: 0.0025, Current Patience: 0\n",
      "Epoch 163, Average Loss: 13.641999244689941, Average MAE: 0.8639267683029175, Learning Rate: 0.0025, Current Patience: 1\n",
      "Epoch 164, Average Loss: 13.640207767486572, Average MAE: 0.8634729385375977, Learning Rate: 0.0025, Current Patience: 2\n",
      "Epoch 165, Average Loss: 13.637235164642334, Average MAE: 0.8636647164821625, Learning Rate: 0.0025, Current Patience: 3\n",
      "Epoch 166, Average Loss: 13.640840530395508, Average MAE: 0.8634114563465118, Learning Rate: 0.0025, Current Patience: 4\n",
      "Epoch 167, Average Loss: 13.640070915222168, Average MAE: 0.86360964179039, Learning Rate: 0.0025, Current Patience: 5\n",
      "Epoch 168, Average Loss: 13.64664602279663, Average MAE: 0.8636585772037506, Learning Rate: 0.0025, Current Patience: 6\n",
      "Epoch 169, Average Loss: 13.638233184814453, Average MAE: 0.8634896576404572, Learning Rate: 0.0025, Current Patience: 7\n",
      "Epoch 170, Average Loss: 13.637364864349365, Average MAE: 0.8632517457008362, Learning Rate: 0.0025, Current Patience: 8\n",
      "Epoch 171, Average Loss: 13.638909816741943, Average MAE: 0.8629668653011322, Learning Rate: 0.0025, Current Patience: 9\n",
      "Epoch 172, Average Loss: 13.653620719909668, Average MAE: 0.8634622395038605, Learning Rate: 0.0025, Current Patience: 10\n",
      "Epoch 173, Average Loss: 13.644680976867676, Average MAE: 0.8635239005088806, Learning Rate: 0.00125, Current Patience: 0\n",
      "Epoch 174, Average Loss: 13.648897647857666, Average MAE: 0.863403856754303, Learning Rate: 0.00125, Current Patience: 1\n",
      "Epoch 175, Average Loss: 13.638562679290771, Average MAE: 0.8630517721176147, Learning Rate: 0.00125, Current Patience: 2\n",
      "Epoch 176, Average Loss: 13.649415493011475, Average MAE: 0.8633716404438019, Learning Rate: 0.00125, Current Patience: 3\n",
      "Epoch 177, Average Loss: 13.634873390197754, Average MAE: 0.8629278540611267, Learning Rate: 0.00125, Current Patience: 4\n",
      "Epoch 178, Average Loss: 13.649177551269531, Average MAE: 0.8632164597511292, Learning Rate: 0.00125, Current Patience: 5\n",
      "Epoch 179, Average Loss: 13.637110233306885, Average MAE: 0.8631203770637512, Learning Rate: 0.00125, Current Patience: 6\n",
      "Epoch 180, Average Loss: 13.642655849456787, Average MAE: 0.8633185029029846, Learning Rate: 0.00125, Current Patience: 7\n",
      "Epoch 181, Average Loss: 13.639673233032227, Average MAE: 0.8629809021949768, Learning Rate: 0.00125, Current Patience: 8\n",
      "Epoch 182, Average Loss: 13.644519329071045, Average MAE: 0.863082766532898, Learning Rate: 0.00125, Current Patience: 9\n",
      "Epoch 183, Average Loss: 13.642972946166992, Average MAE: 0.8630300164222717, Learning Rate: 0.00125, Current Patience: 10\n",
      "Epoch 184, Average Loss: 13.650129795074463, Average MAE: 0.8629370629787445, Learning Rate: 0.000625, Current Patience: 0\n",
      "Epoch 185, Average Loss: 13.644434452056885, Average MAE: 0.8631020784378052, Learning Rate: 0.000625, Current Patience: 1\n",
      "Epoch 186, Average Loss: 13.629594326019287, Average MAE: 0.8624298870563507, Learning Rate: 0.000625, Current Patience: 0\n",
      "Epoch 187, Average Loss: 13.637366771697998, Average MAE: 0.8625775575637817, Learning Rate: 0.000625, Current Patience: 1\n",
      "Epoch 188, Average Loss: 13.645082950592041, Average MAE: 0.8628534972667694, Learning Rate: 0.000625, Current Patience: 2\n",
      "Epoch 189, Average Loss: 13.637426376342773, Average MAE: 0.86247518658638, Learning Rate: 0.000625, Current Patience: 3\n",
      "Epoch 190, Average Loss: 13.64487361907959, Average MAE: 0.8627902269363403, Learning Rate: 0.000625, Current Patience: 4\n",
      "Epoch 191, Average Loss: 13.633540153503418, Average MAE: 0.862452894449234, Learning Rate: 0.000625, Current Patience: 5\n",
      "Epoch 192, Average Loss: 13.641241550445557, Average MAE: 0.8627185523509979, Learning Rate: 0.000625, Current Patience: 6\n",
      "Epoch 193, Average Loss: 13.645452499389648, Average MAE: 0.8628451526165009, Learning Rate: 0.000625, Current Patience: 7\n",
      "Epoch 194, Average Loss: 13.642191886901855, Average MAE: 0.8625618517398834, Learning Rate: 0.000625, Current Patience: 8\n",
      "Epoch 195, Average Loss: 13.63137674331665, Average MAE: 0.8623030185699463, Learning Rate: 0.000625, Current Patience: 9\n",
      "Epoch 196, Average Loss: 13.642028331756592, Average MAE: 0.8626714646816254, Learning Rate: 0.000625, Current Patience: 10\n",
      "Epoch 197, Average Loss: 13.630916118621826, Average MAE: 0.8623896837234497, Learning Rate: 0.0003125, Current Patience: 0\n",
      "Epoch 198, Average Loss: 13.638434410095215, Average MAE: 0.8625627756118774, Learning Rate: 0.0003125, Current Patience: 1\n",
      "Epoch 199, Average Loss: 13.634913921356201, Average MAE: 0.8623160719871521, Learning Rate: 0.0003125, Current Patience: 2\n",
      "Epoch 200, Average Loss: 13.648590564727783, Average MAE: 0.8625969886779785, Learning Rate: 0.0003125, Current Patience: 3\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters\n",
    "in_channels = batched_input_train.shape[2]\n",
    "features_channels = batched_input_train.shape[3]\n",
    "out_channels = in_channels  \n",
    "edge_in_channels = 1\n",
    "hidden_channels = 512\n",
    "transformer_layers = 8\n",
    "\n",
    "# Print the parameters to verify\n",
    "print(\"Input Channels:\", in_channels)\n",
    "print(\"Features Channels:\", features_channels)\n",
    "print(\"Output Channels:\", out_channels)\n",
    "print(\"Edge Input Channels:\", edge_in_channels)\n",
    "print(\"Hidden Channels:\", hidden_channels)\n",
    "print(\"Transformer Layers:\", transformer_layers)\n",
    "\n",
    "model_single_transformer = SingleModel_Transformer(in_channels, out_channels, features_channels, edge_in_channels, hidden_channels, transformer_layers).to(device)\n",
    "\n",
    "# Define the optimizer and loss function for model_transformer\n",
    "optimizer_single_transformer = torch.optim.Adam(model_single_transformer.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "scheduler_single_transformer = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_single_transformer, mode=scheduler_mode, factor=scheduler_factor, patience=scheduler_patience, verbose=True, min_lr=1e-5)\n",
    "\n",
    "single_transformer_epoch_losses, single_transformer_epoch_maes = train_model(model_single_transformer, optimizer_single_transformer, scheduler_single_transformer, loss_fn, num_epochs, num_batches_train, batched_input_train, batched_output_train, device)\n",
    "torch.save(model_single_transformer.state_dict(), 'single_transformer_hidden{hidden_channels}_layers{transformer_layers}.pth')\n",
    "del model_single_transformer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Channels: 27\n",
      "Features Channels: 70\n",
      "Output Channels: 27\n",
      "Edge Input Channels: 1\n",
      "Hidden Channels: 512\n",
      "Transformer Layers: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/HDD/RyanFolder/projects/Weather-Predition-Project/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 241.76262664794922, Average MAE: 7.910881757736206, Learning Rate: 0.0003125, Current Patience: 4\n",
      "Epoch 2, Average Loss: 241.86612701416016, Average MAE: 7.9086596965789795, Learning Rate: 0.0003125, Current Patience: 5\n",
      "Epoch 3, Average Loss: 241.89105224609375, Average MAE: 7.918067216873169, Learning Rate: 0.0003125, Current Patience: 6\n",
      "Epoch 4, Average Loss: 241.92764282226562, Average MAE: 7.913221597671509, Learning Rate: 0.0003125, Current Patience: 7\n",
      "Epoch 5, Average Loss: 241.90284729003906, Average MAE: 7.911094665527344, Learning Rate: 0.0003125, Current Patience: 8\n",
      "Epoch 6, Average Loss: 241.78329467773438, Average MAE: 7.916306495666504, Learning Rate: 0.0003125, Current Patience: 9\n",
      "Epoch 7, Average Loss: 241.931396484375, Average MAE: 7.916272163391113, Learning Rate: 0.0003125, Current Patience: 10\n",
      "Epoch 8, Average Loss: 241.97325897216797, Average MAE: 7.9192304611206055, Learning Rate: 0.00015625, Current Patience: 0\n",
      "Epoch 9, Average Loss: 241.63397216796875, Average MAE: 7.907394170761108, Learning Rate: 0.00015625, Current Patience: 1\n",
      "Epoch 10, Average Loss: 241.90816497802734, Average MAE: 7.913738489151001, Learning Rate: 0.00015625, Current Patience: 2\n",
      "Epoch 11, Average Loss: 241.91252899169922, Average MAE: 7.912614345550537, Learning Rate: 0.00015625, Current Patience: 3\n",
      "Epoch 12, Average Loss: 241.86711883544922, Average MAE: 7.91105842590332, Learning Rate: 0.00015625, Current Patience: 4\n",
      "Epoch 13, Average Loss: 241.81864166259766, Average MAE: 7.911466360092163, Learning Rate: 0.00015625, Current Patience: 5\n",
      "Epoch 14, Average Loss: 241.82748413085938, Average MAE: 7.912234306335449, Learning Rate: 0.00015625, Current Patience: 6\n",
      "Epoch 15, Average Loss: 241.6715316772461, Average MAE: 7.90685772895813, Learning Rate: 0.00015625, Current Patience: 7\n",
      "Epoch 16, Average Loss: 241.96331024169922, Average MAE: 7.917913913726807, Learning Rate: 0.00015625, Current Patience: 8\n",
      "Epoch 17, Average Loss: 241.8690948486328, Average MAE: 7.913537502288818, Learning Rate: 0.00015625, Current Patience: 9\n",
      "Epoch 18, Average Loss: 241.83753967285156, Average MAE: 7.912267446517944, Learning Rate: 0.00015625, Current Patience: 10\n",
      "Epoch 19, Average Loss: 241.81224822998047, Average MAE: 7.909873962402344, Learning Rate: 7.8125e-05, Current Patience: 0\n",
      "Epoch 20, Average Loss: 241.80742645263672, Average MAE: 7.910917043685913, Learning Rate: 7.8125e-05, Current Patience: 1\n",
      "Epoch 21, Average Loss: 241.82799530029297, Average MAE: 7.915129661560059, Learning Rate: 7.8125e-05, Current Patience: 2\n",
      "Epoch 22, Average Loss: 241.80966186523438, Average MAE: 7.910994529724121, Learning Rate: 7.8125e-05, Current Patience: 3\n",
      "Epoch 23, Average Loss: 241.78216552734375, Average MAE: 7.912108898162842, Learning Rate: 7.8125e-05, Current Patience: 4\n",
      "Epoch 24, Average Loss: 241.72201538085938, Average MAE: 7.907284736633301, Learning Rate: 7.8125e-05, Current Patience: 5\n",
      "Epoch 25, Average Loss: 241.81468963623047, Average MAE: 7.9116785526275635, Learning Rate: 7.8125e-05, Current Patience: 6\n",
      "Epoch 26, Average Loss: 241.8297348022461, Average MAE: 7.912953853607178, Learning Rate: 7.8125e-05, Current Patience: 7\n",
      "Epoch 27, Average Loss: 241.80599975585938, Average MAE: 7.910006761550903, Learning Rate: 7.8125e-05, Current Patience: 8\n",
      "Epoch 28, Average Loss: 241.77599334716797, Average MAE: 7.912891149520874, Learning Rate: 7.8125e-05, Current Patience: 9\n",
      "Epoch 29, Average Loss: 241.76197052001953, Average MAE: 7.9113993644714355, Learning Rate: 7.8125e-05, Current Patience: 10\n",
      "Epoch 30, Average Loss: 242.06048583984375, Average MAE: 7.922440052032471, Learning Rate: 3.90625e-05, Current Patience: 0\n",
      "Epoch 31, Average Loss: 242.00829315185547, Average MAE: 7.916631698608398, Learning Rate: 3.90625e-05, Current Patience: 1\n",
      "Epoch 32, Average Loss: 241.8916778564453, Average MAE: 7.915633916854858, Learning Rate: 3.90625e-05, Current Patience: 2\n",
      "Epoch 33, Average Loss: 241.9524917602539, Average MAE: 7.919646739959717, Learning Rate: 3.90625e-05, Current Patience: 3\n",
      "Epoch 34, Average Loss: 241.84547424316406, Average MAE: 7.911377429962158, Learning Rate: 3.90625e-05, Current Patience: 4\n",
      "Epoch 35, Average Loss: 241.85653686523438, Average MAE: 7.917783737182617, Learning Rate: 3.90625e-05, Current Patience: 5\n",
      "Epoch 36, Average Loss: 241.92664337158203, Average MAE: 7.91592812538147, Learning Rate: 3.90625e-05, Current Patience: 6\n",
      "Epoch 37, Average Loss: 241.7923355102539, Average MAE: 7.909736394882202, Learning Rate: 3.90625e-05, Current Patience: 7\n",
      "Epoch 38, Average Loss: 241.820068359375, Average MAE: 7.910809278488159, Learning Rate: 3.90625e-05, Current Patience: 8\n",
      "Epoch 39, Average Loss: 241.79932403564453, Average MAE: 7.910809278488159, Learning Rate: 3.90625e-05, Current Patience: 9\n",
      "Epoch 40, Average Loss: 241.77154541015625, Average MAE: 7.91340970993042, Learning Rate: 3.90625e-05, Current Patience: 10\n",
      "Epoch 41, Average Loss: 241.93502807617188, Average MAE: 7.9170544147491455, Learning Rate: 1.953125e-05, Current Patience: 0\n",
      "Epoch 42, Average Loss: 241.76121520996094, Average MAE: 7.916164875030518, Learning Rate: 1.953125e-05, Current Patience: 1\n",
      "Epoch 43, Average Loss: 241.8017349243164, Average MAE: 7.916367053985596, Learning Rate: 1.953125e-05, Current Patience: 2\n",
      "Epoch 44, Average Loss: 241.93187713623047, Average MAE: 7.91725492477417, Learning Rate: 1.953125e-05, Current Patience: 3\n",
      "Epoch 45, Average Loss: 241.6924819946289, Average MAE: 7.907535552978516, Learning Rate: 1.953125e-05, Current Patience: 4\n",
      "Epoch 46, Average Loss: 241.76841735839844, Average MAE: 7.912598609924316, Learning Rate: 1.953125e-05, Current Patience: 5\n",
      "Epoch 47, Average Loss: 241.9569549560547, Average MAE: 7.919536590576172, Learning Rate: 1.953125e-05, Current Patience: 6\n",
      "Epoch 48, Average Loss: 241.8631820678711, Average MAE: 7.914030313491821, Learning Rate: 1.953125e-05, Current Patience: 7\n",
      "Epoch 49, Average Loss: 241.8053436279297, Average MAE: 7.913074254989624, Learning Rate: 1.953125e-05, Current Patience: 8\n",
      "Epoch 50, Average Loss: 242.00179290771484, Average MAE: 7.919131278991699, Learning Rate: 1.953125e-05, Current Patience: 9\n",
      "Epoch 51, Average Loss: 241.93689727783203, Average MAE: 7.917957782745361, Learning Rate: 1.953125e-05, Current Patience: 10\n",
      "Epoch 52, Average Loss: 242.03673553466797, Average MAE: 7.916484117507935, Learning Rate: 1e-05, Current Patience: 0\n",
      "Epoch 53, Average Loss: 241.8785400390625, Average MAE: 7.916396141052246, Learning Rate: 1e-05, Current Patience: 1\n",
      "Epoch 54, Average Loss: 241.67530059814453, Average MAE: 7.908197641372681, Learning Rate: 1e-05, Current Patience: 2\n",
      "Epoch 55, Average Loss: 242.03211212158203, Average MAE: 7.915902137756348, Learning Rate: 1e-05, Current Patience: 3\n",
      "Epoch 56, Average Loss: 241.83235931396484, Average MAE: 7.913206338882446, Learning Rate: 1e-05, Current Patience: 4\n",
      "Epoch 57, Average Loss: 241.74028778076172, Average MAE: 7.914124965667725, Learning Rate: 1e-05, Current Patience: 5\n",
      "Epoch 58, Average Loss: 241.73590087890625, Average MAE: 7.910403490066528, Learning Rate: 1e-05, Current Patience: 6\n",
      "Epoch 59, Average Loss: 241.8472442626953, Average MAE: 7.915639877319336, Learning Rate: 1e-05, Current Patience: 7\n",
      "Epoch 60, Average Loss: 241.80960845947266, Average MAE: 7.914656162261963, Learning Rate: 1e-05, Current Patience: 8\n",
      "Epoch 61, Average Loss: 241.80825805664062, Average MAE: 7.914087295532227, Learning Rate: 1e-05, Current Patience: 9\n",
      "Epoch 62, Average Loss: 241.8398208618164, Average MAE: 7.912499189376831, Learning Rate: 1e-05, Current Patience: 10\n",
      "Epoch 63, Average Loss: 241.9781951904297, Average MAE: 7.913244962692261, Learning Rate: 1e-05, Current Patience: 0\n",
      "Epoch 64, Average Loss: 241.8540802001953, Average MAE: 7.9157373905181885, Learning Rate: 1e-05, Current Patience: 1\n",
      "Epoch 65, Average Loss: 241.94037628173828, Average MAE: 7.916630744934082, Learning Rate: 1e-05, Current Patience: 2\n",
      "Epoch 66, Average Loss: 241.93153381347656, Average MAE: 7.913635969161987, Learning Rate: 1e-05, Current Patience: 3\n",
      "Epoch 67, Average Loss: 241.93772888183594, Average MAE: 7.914411306381226, Learning Rate: 1e-05, Current Patience: 4\n",
      "Epoch 68, Average Loss: 241.89126586914062, Average MAE: 7.914956331253052, Learning Rate: 1e-05, Current Patience: 5\n",
      "Epoch 69, Average Loss: 241.92183685302734, Average MAE: 7.9169416427612305, Learning Rate: 1e-05, Current Patience: 6\n",
      "Epoch 70, Average Loss: 241.88131713867188, Average MAE: 7.91241717338562, Learning Rate: 1e-05, Current Patience: 7\n",
      "Epoch 71, Average Loss: 241.82465362548828, Average MAE: 7.91521692276001, Learning Rate: 1e-05, Current Patience: 8\n",
      "Epoch 72, Average Loss: 241.84274291992188, Average MAE: 7.912562370300293, Learning Rate: 1e-05, Current Patience: 9\n",
      "Epoch 73, Average Loss: 241.9031524658203, Average MAE: 7.914637327194214, Learning Rate: 1e-05, Current Patience: 10\n",
      "Epoch 74, Average Loss: 241.97734832763672, Average MAE: 7.914267539978027, Learning Rate: 1e-05, Current Patience: 0\n",
      "Epoch 75, Average Loss: 241.85574340820312, Average MAE: 7.915090799331665, Learning Rate: 1e-05, Current Patience: 1\n",
      "Epoch 76, Average Loss: 241.8592071533203, Average MAE: 7.915489912033081, Learning Rate: 1e-05, Current Patience: 2\n",
      "Epoch 77, Average Loss: 241.92274475097656, Average MAE: 7.919614553451538, Learning Rate: 1e-05, Current Patience: 3\n",
      "Epoch 78, Average Loss: 241.98096466064453, Average MAE: 7.918254137039185, Learning Rate: 1e-05, Current Patience: 4\n",
      "Epoch 79, Average Loss: 241.8539276123047, Average MAE: 7.914979934692383, Learning Rate: 1e-05, Current Patience: 5\n",
      "Epoch 80, Average Loss: 241.9408416748047, Average MAE: 7.9165544509887695, Learning Rate: 1e-05, Current Patience: 6\n",
      "Epoch 81, Average Loss: 241.84912109375, Average MAE: 7.911481618881226, Learning Rate: 1e-05, Current Patience: 7\n",
      "Epoch 82, Average Loss: 241.81716918945312, Average MAE: 7.918806552886963, Learning Rate: 1e-05, Current Patience: 8\n",
      "Epoch 83, Average Loss: 241.94692993164062, Average MAE: 7.917402505874634, Learning Rate: 1e-05, Current Patience: 9\n",
      "Epoch 84, Average Loss: 241.99610900878906, Average MAE: 7.915165424346924, Learning Rate: 1e-05, Current Patience: 10\n",
      "Epoch 85, Average Loss: 241.75199127197266, Average MAE: 7.910812616348267, Learning Rate: 1e-05, Current Patience: 0\n",
      "Epoch 86, Average Loss: 241.99842071533203, Average MAE: 7.9141576290130615, Learning Rate: 1e-05, Current Patience: 1\n",
      "Epoch 87, Average Loss: 241.80147552490234, Average MAE: 7.9124720096588135, Learning Rate: 1e-05, Current Patience: 2\n",
      "Epoch 88, Average Loss: 241.90330505371094, Average MAE: 7.914175271987915, Learning Rate: 1e-05, Current Patience: 3\n",
      "Epoch 89, Average Loss: 241.7797622680664, Average MAE: 7.914996147155762, Learning Rate: 1e-05, Current Patience: 4\n",
      "Epoch 90, Average Loss: 241.82522583007812, Average MAE: 7.907876253128052, Learning Rate: 1e-05, Current Patience: 5\n",
      "Epoch 91, Average Loss: 241.83995819091797, Average MAE: 7.913330793380737, Learning Rate: 1e-05, Current Patience: 6\n",
      "Epoch 92, Average Loss: 241.92507934570312, Average MAE: 7.91787314414978, Learning Rate: 1e-05, Current Patience: 7\n",
      "Epoch 93, Average Loss: 241.79528045654297, Average MAE: 7.91274094581604, Learning Rate: 1e-05, Current Patience: 8\n",
      "Epoch 94, Average Loss: 241.78252410888672, Average MAE: 7.916154623031616, Learning Rate: 1e-05, Current Patience: 9\n",
      "Epoch 95, Average Loss: 241.87334442138672, Average MAE: 7.913499355316162, Learning Rate: 1e-05, Current Patience: 10\n",
      "Epoch 96, Average Loss: 241.88391876220703, Average MAE: 7.918086051940918, Learning Rate: 1e-05, Current Patience: 0\n",
      "Epoch 97, Average Loss: 241.6606674194336, Average MAE: 7.909761667251587, Learning Rate: 1e-05, Current Patience: 1\n",
      "Epoch 98, Average Loss: 241.80203247070312, Average MAE: 7.9108195304870605, Learning Rate: 1e-05, Current Patience: 2\n",
      "Epoch 99, Average Loss: 241.88888549804688, Average MAE: 7.915365219116211, Learning Rate: 1e-05, Current Patience: 3\n",
      "Epoch 100, Average Loss: 241.6376953125, Average MAE: 7.910620927810669, Learning Rate: 1e-05, Current Patience: 4\n",
      "Epoch 101, Average Loss: 241.81434631347656, Average MAE: 7.912766218185425, Learning Rate: 1e-05, Current Patience: 5\n",
      "Epoch 102, Average Loss: 241.73500061035156, Average MAE: 7.908029556274414, Learning Rate: 1e-05, Current Patience: 6\n",
      "Epoch 103, Average Loss: 241.9423370361328, Average MAE: 7.915190935134888, Learning Rate: 1e-05, Current Patience: 7\n",
      "Epoch 104, Average Loss: 241.85272216796875, Average MAE: 7.916785001754761, Learning Rate: 1e-05, Current Patience: 8\n",
      "Epoch 105, Average Loss: 241.8286590576172, Average MAE: 7.912627220153809, Learning Rate: 1e-05, Current Patience: 9\n",
      "Epoch 106, Average Loss: 241.83935546875, Average MAE: 7.914108991622925, Learning Rate: 1e-05, Current Patience: 10\n",
      "Epoch 107, Average Loss: 241.88230895996094, Average MAE: 7.9145495891571045, Learning Rate: 1e-05, Current Patience: 0\n",
      "Epoch 108, Average Loss: 241.7512969970703, Average MAE: 7.914226293563843, Learning Rate: 1e-05, Current Patience: 1\n",
      "Epoch 109, Average Loss: 241.78892517089844, Average MAE: 7.910801649093628, Learning Rate: 1e-05, Current Patience: 2\n",
      "Epoch 110, Average Loss: 241.93561553955078, Average MAE: 7.917134046554565, Learning Rate: 1e-05, Current Patience: 3\n",
      "Epoch 111, Average Loss: 241.9093780517578, Average MAE: 7.914846658706665, Learning Rate: 1e-05, Current Patience: 4\n",
      "Epoch 112, Average Loss: 241.8663101196289, Average MAE: 7.918895721435547, Learning Rate: 1e-05, Current Patience: 5\n",
      "Epoch 113, Average Loss: 241.77271270751953, Average MAE: 7.909784555435181, Learning Rate: 1e-05, Current Patience: 6\n",
      "Epoch 114, Average Loss: 241.8689956665039, Average MAE: 7.91633939743042, Learning Rate: 1e-05, Current Patience: 7\n",
      "Epoch 115, Average Loss: 241.7486801147461, Average MAE: 7.9134814739227295, Learning Rate: 1e-05, Current Patience: 8\n",
      "Epoch 116, Average Loss: 241.84270477294922, Average MAE: 7.911387920379639, Learning Rate: 1e-05, Current Patience: 9\n",
      "Epoch 117, Average Loss: 241.8440704345703, Average MAE: 7.919391393661499, Learning Rate: 1e-05, Current Patience: 10\n",
      "Epoch 118, Average Loss: 242.0064926147461, Average MAE: 7.9178266525268555, Learning Rate: 1e-05, Current Patience: 0\n",
      "Epoch 119, Average Loss: 241.8851318359375, Average MAE: 7.9134416580200195, Learning Rate: 1e-05, Current Patience: 1\n",
      "Epoch 120, Average Loss: 241.9055938720703, Average MAE: 7.915140390396118, Learning Rate: 1e-05, Current Patience: 2\n",
      "Epoch 121, Average Loss: 241.96250915527344, Average MAE: 7.913116693496704, Learning Rate: 1e-05, Current Patience: 3\n",
      "Epoch 122, Average Loss: 241.6873550415039, Average MAE: 7.907141208648682, Learning Rate: 1e-05, Current Patience: 4\n",
      "Epoch 123, Average Loss: 241.81366729736328, Average MAE: 7.913391590118408, Learning Rate: 1e-05, Current Patience: 5\n",
      "Epoch 124, Average Loss: 242.00550079345703, Average MAE: 7.92081356048584, Learning Rate: 1e-05, Current Patience: 6\n",
      "Epoch 125, Average Loss: 241.9246368408203, Average MAE: 7.917438268661499, Learning Rate: 1e-05, Current Patience: 7\n",
      "Epoch 126, Average Loss: 241.76892852783203, Average MAE: 7.913083791732788, Learning Rate: 1e-05, Current Patience: 8\n",
      "Epoch 127, Average Loss: 241.83226013183594, Average MAE: 7.915875673294067, Learning Rate: 1e-05, Current Patience: 9\n",
      "Epoch 128, Average Loss: 241.75920867919922, Average MAE: 7.913656949996948, Learning Rate: 1e-05, Current Patience: 10\n",
      "Epoch 129, Average Loss: 241.85970306396484, Average MAE: 7.914764642715454, Learning Rate: 1e-05, Current Patience: 0\n",
      "Epoch 130, Average Loss: 241.8077850341797, Average MAE: 7.909856557846069, Learning Rate: 1e-05, Current Patience: 1\n",
      "Epoch 131, Average Loss: 241.86122131347656, Average MAE: 7.917724370956421, Learning Rate: 1e-05, Current Patience: 2\n",
      "Epoch 132, Average Loss: 241.85517120361328, Average MAE: 7.91234278678894, Learning Rate: 1e-05, Current Patience: 3\n",
      "Epoch 133, Average Loss: 241.78992462158203, Average MAE: 7.914059638977051, Learning Rate: 1e-05, Current Patience: 4\n",
      "Epoch 134, Average Loss: 241.75685119628906, Average MAE: 7.909504175186157, Learning Rate: 1e-05, Current Patience: 5\n",
      "Epoch 135, Average Loss: 241.7435531616211, Average MAE: 7.91078782081604, Learning Rate: 1e-05, Current Patience: 6\n",
      "Epoch 136, Average Loss: 241.84268188476562, Average MAE: 7.914411783218384, Learning Rate: 1e-05, Current Patience: 7\n",
      "Epoch 137, Average Loss: 241.88399505615234, Average MAE: 7.908666372299194, Learning Rate: 1e-05, Current Patience: 8\n",
      "Epoch 138, Average Loss: 241.84568786621094, Average MAE: 7.91231369972229, Learning Rate: 1e-05, Current Patience: 9\n",
      "Epoch 139, Average Loss: 241.8924560546875, Average MAE: 7.914960145950317, Learning Rate: 1e-05, Current Patience: 10\n",
      "Epoch 140, Average Loss: 241.82293701171875, Average MAE: 7.914782285690308, Learning Rate: 1e-05, Current Patience: 0\n",
      "Epoch 141, Average Loss: 241.8703155517578, Average MAE: 7.911717176437378, Learning Rate: 1e-05, Current Patience: 1\n",
      "Epoch 142, Average Loss: 241.77962493896484, Average MAE: 7.913156747817993, Learning Rate: 1e-05, Current Patience: 2\n",
      "Epoch 143, Average Loss: 241.8434295654297, Average MAE: 7.914140462875366, Learning Rate: 1e-05, Current Patience: 3\n",
      "Epoch 144, Average Loss: 241.7532501220703, Average MAE: 7.907217264175415, Learning Rate: 1e-05, Current Patience: 4\n",
      "Epoch 145, Average Loss: 241.89772033691406, Average MAE: 7.912456274032593, Learning Rate: 1e-05, Current Patience: 5\n",
      "Epoch 146, Average Loss: 241.96451568603516, Average MAE: 7.917623519897461, Learning Rate: 1e-05, Current Patience: 6\n",
      "Epoch 147, Average Loss: 241.81987762451172, Average MAE: 7.915489912033081, Learning Rate: 1e-05, Current Patience: 7\n",
      "Epoch 148, Average Loss: 241.79442596435547, Average MAE: 7.90907883644104, Learning Rate: 1e-05, Current Patience: 8\n",
      "Epoch 149, Average Loss: 241.8256072998047, Average MAE: 7.913564682006836, Learning Rate: 1e-05, Current Patience: 9\n",
      "Epoch 150, Average Loss: 241.78805541992188, Average MAE: 7.9109814167022705, Learning Rate: 1e-05, Current Patience: 10\n",
      "Epoch 151, Average Loss: 241.85578155517578, Average MAE: 7.911318063735962, Learning Rate: 1e-05, Current Patience: 0\n",
      "Epoch 152, Average Loss: 241.76951599121094, Average MAE: 7.911502122879028, Learning Rate: 1e-05, Current Patience: 1\n",
      "Epoch 153, Average Loss: 241.85550689697266, Average MAE: 7.913114786148071, Learning Rate: 1e-05, Current Patience: 2\n",
      "Epoch 154, Average Loss: 241.80809020996094, Average MAE: 7.912660360336304, Learning Rate: 1e-05, Current Patience: 3\n",
      "Epoch 155, Average Loss: 241.8392562866211, Average MAE: 7.914606809616089, Learning Rate: 1e-05, Current Patience: 4\n",
      "Epoch 156, Average Loss: 241.97805786132812, Average MAE: 7.918923616409302, Learning Rate: 1e-05, Current Patience: 5\n",
      "Epoch 157, Average Loss: 241.98876953125, Average MAE: 7.919845819473267, Learning Rate: 1e-05, Current Patience: 6\n",
      "Epoch 158, Average Loss: 241.9188461303711, Average MAE: 7.913510084152222, Learning Rate: 1e-05, Current Patience: 7\n",
      "Epoch 159, Average Loss: 241.82708740234375, Average MAE: 7.912569522857666, Learning Rate: 1e-05, Current Patience: 8\n",
      "Epoch 160, Average Loss: 241.89480590820312, Average MAE: 7.915810585021973, Learning Rate: 1e-05, Current Patience: 9\n",
      "Epoch 161, Average Loss: 241.82343292236328, Average MAE: 7.916813611984253, Learning Rate: 1e-05, Current Patience: 10\n",
      "Epoch 162, Average Loss: 241.78070068359375, Average MAE: 7.914959907531738, Learning Rate: 1e-05, Current Patience: 0\n",
      "Epoch 163, Average Loss: 241.75499725341797, Average MAE: 7.909656286239624, Learning Rate: 1e-05, Current Patience: 1\n",
      "Epoch 164, Average Loss: 241.7949676513672, Average MAE: 7.9100117683410645, Learning Rate: 1e-05, Current Patience: 2\n",
      "Epoch 165, Average Loss: 241.88381958007812, Average MAE: 7.914364814758301, Learning Rate: 1e-05, Current Patience: 3\n",
      "Epoch 166, Average Loss: 241.79790496826172, Average MAE: 7.912060260772705, Learning Rate: 1e-05, Current Patience: 4\n",
      "Epoch 167, Average Loss: 241.6766357421875, Average MAE: 7.908874273300171, Learning Rate: 1e-05, Current Patience: 5\n",
      "Epoch 168, Average Loss: 241.69234466552734, Average MAE: 7.9081549644470215, Learning Rate: 1e-05, Current Patience: 6\n",
      "Epoch 169, Average Loss: 241.82848358154297, Average MAE: 7.91480827331543, Learning Rate: 1e-05, Current Patience: 7\n",
      "Epoch 170, Average Loss: 241.88131713867188, Average MAE: 7.909980297088623, Learning Rate: 1e-05, Current Patience: 8\n",
      "Epoch 171, Average Loss: 241.98804473876953, Average MAE: 7.918887138366699, Learning Rate: 1e-05, Current Patience: 9\n",
      "Epoch 172, Average Loss: 241.91514587402344, Average MAE: 7.915732145309448, Learning Rate: 1e-05, Current Patience: 10\n",
      "Epoch 173, Average Loss: 241.8363265991211, Average MAE: 7.9130237102508545, Learning Rate: 1e-05, Current Patience: 0\n",
      "Epoch 174, Average Loss: 241.81466674804688, Average MAE: 7.916824102401733, Learning Rate: 1e-05, Current Patience: 1\n",
      "Epoch 175, Average Loss: 241.77031707763672, Average MAE: 7.910840749740601, Learning Rate: 1e-05, Current Patience: 2\n",
      "Epoch 176, Average Loss: 241.87303161621094, Average MAE: 7.914809942245483, Learning Rate: 1e-05, Current Patience: 3\n",
      "Epoch 177, Average Loss: 241.8116455078125, Average MAE: 7.912371873855591, Learning Rate: 1e-05, Current Patience: 4\n",
      "Epoch 178, Average Loss: 241.79649353027344, Average MAE: 7.910903692245483, Learning Rate: 1e-05, Current Patience: 5\n",
      "Epoch 179, Average Loss: 241.91802215576172, Average MAE: 7.915587663650513, Learning Rate: 1e-05, Current Patience: 6\n",
      "Epoch 180, Average Loss: 241.9404296875, Average MAE: 7.920830249786377, Learning Rate: 1e-05, Current Patience: 7\n",
      "Epoch 181, Average Loss: 241.90573120117188, Average MAE: 7.91527533531189, Learning Rate: 1e-05, Current Patience: 8\n",
      "Epoch 182, Average Loss: 241.96947479248047, Average MAE: 7.915515899658203, Learning Rate: 1e-05, Current Patience: 9\n",
      "Epoch 183, Average Loss: 241.9385757446289, Average MAE: 7.91277289390564, Learning Rate: 1e-05, Current Patience: 10\n",
      "Epoch 184, Average Loss: 241.8707504272461, Average MAE: 7.916369438171387, Learning Rate: 1e-05, Current Patience: 0\n",
      "Epoch 185, Average Loss: 241.95377349853516, Average MAE: 7.915899753570557, Learning Rate: 1e-05, Current Patience: 1\n",
      "Epoch 186, Average Loss: 241.8184814453125, Average MAE: 7.9109947681427, Learning Rate: 1e-05, Current Patience: 2\n",
      "Epoch 187, Average Loss: 241.74005889892578, Average MAE: 7.9109320640563965, Learning Rate: 1e-05, Current Patience: 3\n",
      "Epoch 188, Average Loss: 241.8330307006836, Average MAE: 7.910484552383423, Learning Rate: 1e-05, Current Patience: 4\n",
      "Epoch 189, Average Loss: 241.86912536621094, Average MAE: 7.914483070373535, Learning Rate: 1e-05, Current Patience: 5\n",
      "Epoch 190, Average Loss: 241.9425811767578, Average MAE: 7.913847208023071, Learning Rate: 1e-05, Current Patience: 6\n",
      "Epoch 191, Average Loss: 241.94599151611328, Average MAE: 7.915465831756592, Learning Rate: 1e-05, Current Patience: 7\n",
      "Epoch 192, Average Loss: 241.85267639160156, Average MAE: 7.913451671600342, Learning Rate: 1e-05, Current Patience: 8\n",
      "Epoch 193, Average Loss: 241.8642807006836, Average MAE: 7.917291879653931, Learning Rate: 1e-05, Current Patience: 9\n",
      "Epoch 194, Average Loss: 241.90654754638672, Average MAE: 7.917604446411133, Learning Rate: 1e-05, Current Patience: 10\n",
      "Epoch 195, Average Loss: 241.8510284423828, Average MAE: 7.91727614402771, Learning Rate: 1e-05, Current Patience: 0\n",
      "Epoch 196, Average Loss: 241.83939361572266, Average MAE: 7.908388137817383, Learning Rate: 1e-05, Current Patience: 1\n",
      "Epoch 197, Average Loss: 241.89510345458984, Average MAE: 7.915422677993774, Learning Rate: 1e-05, Current Patience: 2\n",
      "Epoch 198, Average Loss: 242.01487731933594, Average MAE: 7.918405055999756, Learning Rate: 1e-05, Current Patience: 3\n",
      "Epoch 199, Average Loss: 241.77802276611328, Average MAE: 7.908575534820557, Learning Rate: 1e-05, Current Patience: 4\n",
      "Epoch 200, Average Loss: 241.80809020996094, Average MAE: 7.9160919189453125, Learning Rate: 1e-05, Current Patience: 5\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "# Define the parameters\n",
    "in_channels = batched_input_train.shape[2]\n",
    "features_channels = batched_input_train.shape[3]\n",
    "out_channels = in_channels  \n",
    "edge_in_channels = 1\n",
    "hidden_channels = 512\n",
    "transformer_layers = 3\n",
    "\n",
    "# Print the parameters to verify\n",
    "print(\"Input Channels:\", in_channels)\n",
    "print(\"Features Channels:\", features_channels)\n",
    "print(\"Output Channels:\", out_channels)\n",
    "print(\"Edge Input Channels:\", edge_in_channels)\n",
    "print(\"Hidden Channels:\", hidden_channels)\n",
    "print(\"Transformer Layers:\", transformer_layers)\n",
    "\n",
    "model_single_gnn = SingleModel_GNN(in_channels, out_channels, features_channels, edge_in_channels, hidden_channels, transformer_layers).to(device)\n",
    "# Define the optimizer and loss function for model_gnn\n",
    "optimizer_single_gnn = torch.optim.Adam(model_single_gnn.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "scheduler_single_gnn = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_single_gnn, mode=scheduler_mode, factor=scheduler_factor, patience=scheduler_patience, verbose=True, min_lr=1e-5)\n",
    "\n",
    "single_gnn_epoch_losses, single_gnn_epoch_maes = train_model(model_single_gnn, optimizer_single_transformer, scheduler_single_transformer, loss_fn, num_epochs, num_batches_train, batched_input_train, batched_output_train, device)\n",
    "torch.save(model_single_gnn.state_dict(), 'single_gnn_hidden{hidden_channels}.pth')\n",
    "del model_single_gnn\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">**ZE EPIC GRAPHS**</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of gnn_transformer_epoch_losses: (200,)\n",
      "Shape of gnn_transformer_epoch_maes: (200,)\n",
      "Shape of transformer_gnn_epoch_losses: (200,)\n",
      "Shape of transformer_gnn_epoch_maes: (200,)\n",
      "Shape of single_gnn_epoch_losses: (200,)\n",
      "Shape of single_gnn_epoch_maes: (200,)\n",
      "Shape of single_transformer_epoch_losses: (200,)\n",
      "Shape of single_transformer_epoch_maes: (200,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print the shape of each variable\n",
    "print(\"Shape of gnn_transformer_epoch_losses:\", np.shape(gnn_transformer_epoch_losses))\n",
    "print(\"Shape of gnn_transformer_epoch_maes:\", np.shape(gnn_transformer_epoch_maes))\n",
    "print(\"Shape of transformer_gnn_epoch_losses:\", np.shape(transformer_gnn_epoch_losses))\n",
    "print(\"Shape of transformer_gnn_epoch_maes:\", np.shape(transformer_gnn_epoch_maes))\n",
    "print(\"Shape of single_gnn_epoch_losses:\", np.shape(single_gnn_epoch_losses))\n",
    "print(\"Shape of single_gnn_epoch_maes:\", np.shape(single_gnn_epoch_maes))\n",
    "print(\"Shape of single_transformer_epoch_losses:\", np.shape(single_transformer_epoch_losses))\n",
    "print(\"Shape of single_transformer_epoch_maes:\", np.shape(single_transformer_epoch_maes))\n",
    "\n",
    "# Reshape transformer_gnn_epoch_losses and transformer_gnn_epoch_maes\n",
    "transformer_gnn_epoch_losses = np.array(transformer_gnn_epoch_losses).flatten()\n",
    "transformer_gnn_epoch_maes = np.array(transformer_gnn_epoch_maes).flatten()\n",
    "\n",
    "# Plotting the epoch losses\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(gnn_transformer_epoch_losses, label='GNN Transformer Loss')\n",
    "plt.plot(transformer_gnn_epoch_losses, label='Transformer GNN Loss')\n",
    "plt.plot(single_gnn_epoch_losses, label='GNN Loss')\n",
    "plt.plot(single_transformer_epoch_losses, label='Transformer Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Epoch Losses')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting the epoch MAE\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(gnn_transformer_epoch_maes, label='GNN Transformer MAE')\n",
    "plt.plot(transformer_gnn_epoch_maes, label='Transformer GNN MAE')\n",
    "plt.plot(single_gnn_epoch_maes, label='GNN MAE')\n",
    "plt.plot(single_transformer_epoch_maes, label='Transformer MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.title('Epoch MAE')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# # Initialize lists to store losses and MAEs\n",
    "# test_losses = []\n",
    "# test_maes = []\n",
    "\n",
    "# test_losses_per_matrix = []\n",
    "# test_maes_per_matrix = []\n",
    "\n",
    "# # Disable gradient calculation for evaluation\n",
    "# with torch.no_grad():\n",
    "#     for b in range(num_batches_test):\n",
    "#         # Get the batched node features and desired output\n",
    "#         node_features_batch = batched_input_test[b].to(device)\n",
    "#         desired_output_batch = batched_output_test[b].to(device)\n",
    "        \n",
    "#         # Reshape node_features_batch to match the expected input shape for the model\n",
    "#         node_features_batch = node_features_batch.view(-1, node_features_batch.size(2))\n",
    "        \n",
    "#         # Get the model output\n",
    "#         model_output_batch = model(node_features_batch, edge_index, edge_attr)\n",
    "        \n",
    "#         # Reshape model_output_batch back to the original shape\n",
    "#         model_output_batch = model_output_batch.view(batched_input_test.size(1), -1, model_output_batch.size(1))\n",
    "        \n",
    "#         # Compute loss and MAE\n",
    "#         loss = loss_fn(model_output_batch, desired_output_batch)\n",
    "#         mae = torch.mean(torch.abs(model_output_batch - desired_output_batch))\n",
    "        \n",
    "#         # Store the loss and MAE\n",
    "#         test_losses.append(loss.item())\n",
    "#         test_maes.append(mae.item())\n",
    "\n",
    "#         print(f\"Batch {b+1} - Model Output and Desired Output Matrices:\")\n",
    "#         model_output_np = model_output_batch.cpu().numpy()\n",
    "#         desired_output_np = desired_output_batch.cpu().numpy()\n",
    "\n",
    "#         for i in range(model_output_np.shape[0]):\n",
    "#             print()\n",
    "#             print(f\"Model Output Matrix {i+1}:\")\n",
    "#             np.set_printoptions(formatter={'float_kind':lambda x: \"%.4f\" % x})\n",
    "#             print(model_output_np[i])\n",
    "#             print(f\"Desired Output Matrix {i+1}:\")\n",
    "#             print(desired_output_np[i])\n",
    "            \n",
    "#             # Calculate loss and MAE for each matrix\n",
    "#             matrix_loss = loss_fn(torch.tensor(model_output_np[i], device=device), torch.tensor(desired_output_np[i], device=device))\n",
    "#             matrix_mae = torch.mean(torch.abs(torch.tensor(model_output_np[i], device=device) - torch.tensor(desired_output_np[i], device=device)))\n",
    "#             print(f\"Loss for Matrix {i+1}: {matrix_loss.item()}\")\n",
    "#             print(f\"MAE for Matrix {i+1}: {matrix_mae.item()}\")\n",
    "\n",
    "#             test_losses_per_matrix.append(matrix_loss.item())\n",
    "#             test_maes_per_matrix.append(matrix_mae.item())\n",
    "\n",
    "# # Print the average loss and MAE for the test set\n",
    "# average_test_loss = sum(test_losses) / len(test_losses)\n",
    "# average_test_mae = sum(test_maes) / len(test_maes)\n",
    "# print(f\"Average Test Loss: {average_test_loss}, Average Test MAE: {average_test_mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12, 6))\n",
    "\n",
    "# plt.subplot(2, 1, 1)\n",
    "# plt.plot(test_losses, label='Test Loss per Batch')\n",
    "# plt.axhline(y=average_test_loss, color='r', linestyle='--', label='Average Test Loss')\n",
    "# plt.xlabel('Batch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('Test Loss per Batch and Average Test Loss')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "\n",
    "# plt.subplot(2, 1, 2)\n",
    "# plt.plot(test_maes, label='Test MAE per Batch')\n",
    "# plt.axhline(y=average_test_mae, color='r', linestyle='--', label='Average Test MAE')\n",
    "# plt.xlabel('Batch')\n",
    "# plt.ylabel('MAE')\n",
    "# plt.title('Test MAE per Batch and Average Test MAE')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # Plot test loss and MAE per matrix\n",
    "# plt.figure(figsize=(12, 6))\n",
    "\n",
    "# plt.subplot(2, 1, 1)\n",
    "# plt.plot(test_losses_per_matrix, label='Test Loss per Matrix')\n",
    "# plt.xlabel('Matrix')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('Test Loss per Matrix')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "\n",
    "# plt.subplot(2, 1, 2)\n",
    "# plt.plot(test_maes_per_matrix, label='Test MAE per Matrix')\n",
    "# plt.xlabel('Matrix')\n",
    "# plt.ylabel('MAE')\n",
    "# plt.title('Test MAE per Matrix')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
